{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_tts_tts-python-basics/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How do I use Riva TTS APIs with out-of-the-box models?\n",
    "\n",
    "This tutorial walks you through the basics of Riva’s TTS services, specifically covering how to use Riva TTS APIs with OOTB (out-of-the-box) models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVIDIA Riva Overview\n",
    "\n",
    "NVIDIA Riva is a GPU-accelerated SDK for building Speech AI applications that are customized for your use case and deliver real-time performance. <br/>\n",
    "Riva offers a rich set of speech and natural language understanding services such as:\n",
    "\n",
    "- Automated speech recognition (ASR)\n",
    "- Text-to-Speech synthesis (TTS)\n",
    "- A collection of natural language processing (NLP) services, such as named entity recognition (NER), punctuation, and intent classification.\n",
    "\n",
    "In this tutorial, we will interact with the text-to-speech synthesis (TTS) APIs and customize Riva TTS audio output with SSML.\n",
    "\n",
    "For more information about Riva, please refer to the [Riva developer website](https://developer.nvidia.com/riva)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics: Generating Speech with Riva TTS APIs\n",
    "\n",
    "The Riva TTS service is based on a two-stage pipeline: Riva generates a mel spectrogram using the first model, then uses the mel spectrogram to generate speech using the second model. This pipeline forms a text-to-speech system that enables you to synthesize natural sounding speech from raw transcripts without any additional information such as patterns or rhythms of speech.\n",
    "\n",
    "Riva provides two state-of-the-art voices (one male and one female) for English, that can easily be deployed with the Riva Quick Start scripts. Riva also supports easy customization of TTS in various ways, to meet your specific needs.  \n",
    "Subsequent Riva releases will include features such as  model registration to support multiple languages/voices with the same API and support for resampling to alternative sampling rates.  \n",
    "Refer to the [Riva TTS documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tts/tts-overview.html) for more information.  \n",
    "\n",
    "Now, let’s generate audio using Riva APIs with an OOTB English TTS pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements and setup\n",
    "\n",
    "1. Start the Riva server.\n",
    "Follow the instructions in the [Riva Quick Start Guide](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html#) to deploy OOTB TTS models on the Riva server before running this tutorial. By default, only the English models are deployed.\n",
    "\n",
    "2. Install the additional Python libraries to run this tutorial.\n",
    "Run the following commands to install the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need numpy to read the output from the Riva TTS request.\n",
    "!pip install numpy\n",
    "!pip install nvidia-riva-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Riva client libraries\n",
    "\n",
    "We first import some required libraries, including the Riva client libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import riva.client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Riva clients and connect to the Riva server\n",
    "\n",
    "The following URI assumes a local deployment of the Riva server on the default port. In case the server deployment is on a different host or via Helm chart on Kubernetes, use an appropriate URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = riva.client.Auth(uri='localhost:50051')\n",
    "\n",
    "riva_tts = riva.client.SpeechSynthesisService(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTS modes\n",
    "\n",
    "Riva TTS supports both streaming and batch inference modes. In batch mode, audio is not returned until the full audio sequence for the requested text is generated and can achieve higher throughput. But when making a streaming request, audio chunks are returned as soon as they are generated, significantly reducing the latency (as measured by time to first audio) for large requests. <br> \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the TTS API parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate_hz = 44100\n",
    "req = { \n",
    "        \"language_code\"  : \"en-US\",\n",
    "        \"encoding\"       : riva.client.AudioEncoding.LINEAR_PCM ,   # Currently only LINEAR_PCM is supported\n",
    "        \"sample_rate_hz\" : sample_rate_hz,                          # Generate 44.1KHz audio\n",
    "        \"voice_name\"     : \"English-US.Female-1\"                    # The name of the voice to generate\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding TTS API parameters\n",
    "\n",
    "Riva TTS supports a number of options while making a text-to-speech request to the gRPC endpoint, as shown above. Let's learn more about these parameters:\n",
    "- ``language_code`` - Language of the generated audio. ``en-US`` represents English (US) and is currently the only language supported OOTB.\n",
    "- ``encoding`` - Type of audio encoding to generate. Currently, only ``LINEAR_PCM`` is supported.\n",
    "- ``sample_rate_hz`` - Sample rate of the generated audio. Depends on the microphone and is usually ``22khz`` or ``44khz``.\n",
    "- ``voice_name`` - Voice used to synthesize the audio. Currently, Riva offers two OOTB voices (``English-US.Female-1``, ``English-US.Male-1``)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a gRPC request to the Riva server\n",
    "\n",
    "For batch inference mode, use `synthesize`. Results are returned when the entire audio is synthesized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "req[\"text\"] = \"Is it recognize speech or wreck a nice beach?\"\n",
    "resp = riva_tts.synthesize(**req)\n",
    "audio_samples = np.frombuffer(resp.audio, dtype=np.int16)\n",
    "ipd.Audio(audio_samples, rate=sample_rate_hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For online inference, use `synthesize_online`. Results are returned in chunks as they are synthesized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req[\"text\"] = \"Is it recognize speech or wreck a nice beach?\"\n",
    "resp = riva_tts.synthesize_online(**req)\n",
    "empty = np.array([])\n",
    "for i, rep in enumerate(resp):\n",
    "    audio_samples = np.frombuffer(rep.audio, dtype=np.int16) / (2**15)\n",
    "    print(\"Chunk: \",i)\n",
    "    ipd.display(ipd.Audio(audio_samples, rate=44100))\n",
    "    empty = np.concatenate((empty, audio_samples))\n",
    "\n",
    "print(\"Final synthesis:\")\n",
    "ipd.display(ipd.Audio(empty, rate=44100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing Riva TTS audio output with SSML\n",
    "\n",
    "Speech Synthesis Markup Language (SSML) specification is a markup for directing the performance of the virtual speaker. Riva supports portions of SSML, allowing you to adjust pitch, rate, and pronunciation of the generated audio.\n",
    "SSML support is available only for the FastPitch model at this time. The FastPitch model must be exported using NeMo>=1.5.1 and the nemo2riva>=1.8.0 tool.\n",
    "\n",
    "All SSML inputs must be a valid XML document and use the <speak> root tag. All non-valid XML and all valid XML with a different root tag are treated as raw input text.\n",
    "\n",
    "Riva TTS supports the following SSML tags:\n",
    "\n",
    "- The ``prosody`` tag, which supports attributes ``rate``, ``pitch``, and ``volume``, through which we can control the rate, pitch, and volume of the generated audio.\n",
    "\n",
    "- The ``phoneme`` tag, which allows us to control the pronunciation of the generated audio.\n",
    "    \n",
    "- The ``sub`` tag, which allows us to replace the pronounciation of the specified word or phrase with a different word or phrase.\n",
    "    \n",
    "Let's look at customization of Riva TTS with these SSML tags in some detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing rate, pitch, and volume with the `prosody` tag\n",
    "\n",
    "#### Pitch Attribute\n",
    "Riva supports an additive relative change to the pitch. The `pitch` attribute has a range of [-3, 3]. Values outside this range result in an error being logged and no audio returned. This value returns a pitch shift of the attribute value multiplied with the speaker’s pitch standard deviation when the FastPitch model is trained. For the pretrained checkpoint that was trained on LJSpeech, the standard deviation was 52.185. For example, a pitch shift of 1.25 results in a change of 1.25*52.185=~65.23Hz pitch shift up. \n",
    "Riva also supports the prosody tags as per the SSML specs. Prosody tags `x-low`, `low`, `medium`, `high`, `x-high`, and `default` are supported.\n",
    "\n",
    "The `pitch` attribute is expressed in the following formats:\n",
    "- `pitch=\"1\"`\n",
    "- `pitch=\"+1.8\"`\n",
    "- `pitch=\"-0.65\"`\n",
    "- `pitch=\"high\"`\n",
    "- `pitch=\"default\"`\n",
    "\n",
    "For the pretrained Female-1 checkpoint, the standard deviation is 53.33 Hz.\n",
    "For the pretrained Male-1 checkpoint, the standard deviation is 47.15 Hz.\n",
    "\n",
    "The `pitch` attribute does not support `Hz`, `st`, and `%` changes. Support is planned for a future Riva release.\n",
    "\n",
    "#### Rate Attribute\n",
    "Riva supports a percentage relative change to the rate. The `rate` attribute has a range of [25%, 250%]. Values outside this range result in an error being logged and no audio returned. \n",
    "Riva also supports the prosody tags as per the SSML specs. Prosody tags `x-low`, `low`, `medium`, `high`, `x-high`, and `default` are supported.\n",
    "\n",
    "The `rate` attribute is expressed in the following formats:\n",
    "- `rate=\"35%\"`\n",
    "- `rate=\"+200%\"`\n",
    "- `rate=\"low\"`\n",
    "- `rate=\"default\"`\n",
    "\n",
    "#### Volume Attribute\n",
    "\n",
    "Riva supports the volume attribute as described in the SSML specs. The volume attribute supports a range of [-13, 8]dB. Values outside this range result in an error being logged and no audio returned. Tags `silent`, `x-soft`, `soft`, `medium`, `loud`, `x-loud`, and `default` are supported.\n",
    "\n",
    "The volume attribute is expressed in the following formats:\n",
    "\n",
    "- `volume=\"+1dB\"`\n",
    "- `volume=\"-5.7dB\"`\n",
    "- `volume=\"x-loud\"`\n",
    "- `volume=\"default\"`\n",
    "\n",
    "\n",
    "Let’s look at an example showing the pitch, rate, and volume customizations for Riva TTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Raw text is \"Today is a sunny day. But it might rain tomorrow.\"\n",
    "    We are updating this raw text with SSML:\n",
    "    1. Envelope raw text in '<speak>' tags as is required for SSML\n",
    "    2. Add '<prosody>' tag with 'pitch' attribute set to '2.5'\n",
    "    3. Add '<prosody>' tag with 'rate' attribute set to 'high'\n",
    "    4. Add '<volume>' tag with 'volume' attribute set to '+1dB'\n",
    "\"\"\"\n",
    "raw_text = \"Today is a sunny day. But it might rain tomorrow.\"\n",
    "ssml_text = \"\"\"<speak><prosody pitch='2.5'>Today is a sunny day</prosody>. <prosody rate='high' volume='+1dB'>But it might rain tomorrow.</prosody></speak>\"\"\"\n",
    "print(\"Raw Text: \", raw_text)\n",
    "print(\"SSML Text: \", ssml_text)\n",
    "\n",
    "\n",
    "req[\"text\"] = ssml_text\n",
    "# Request to Riva TTS to synthesize audio\n",
    "resp = riva_tts.synthesize(**req)\n",
    "\n",
    "# Playing the generated audio from Riva TTS request\n",
    "audio_samples = np.frombuffer(resp.audio, dtype=np.int16)\n",
    "ipd.display(ipd.Audio(audio_samples, rate=sample_rate_hz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected results if you run the tutorial:\n",
    "`<prosody pitch='2.5'>Today is a sunny day</prosody>. <prosody rate='high' volume='+1dB'>But it might rain tomorrow.</prosody>`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_0.wav\" type=\"audio/ogg\"></audio>\n",
    "\n",
    "#### Note\n",
    "If the audio controls are not seen throughout notebook. Open the notebook in github dev or view it in the [riva docs](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tutorials/tts-python-basics-and-customization-with-ssml.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are more examples showing the effects of changes in pitch and rate attribute values on the generated audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SSML texts we want to try\n",
    "ssml_texts = [\n",
    "  \"\"\"<speak>This is a normal sentence</speak>\"\"\",\n",
    "  \"\"\"<speak><prosody pitch=\"0.\" rate=\"100%\">This is also a normal sentence</prosody></speak>\"\"\",\n",
    "  \"\"\"<speak><prosody rate=\"200%\">This is a fast sentence</prosody></speak>\"\"\",\n",
    "  \"\"\"<speak><prosody rate=\"60%\">This is a slow sentence</prosody></speak>\"\"\",\n",
    "  \"\"\"<speak><prosody pitch=\"+1.0\">Now, I'm speaking a bit higher</prosody></speak>\"\"\",\n",
    "  \"\"\"<speak><prosody pitch=\"-0.5\">And now, I'm speaking a bit lower</prosody></speak>\"\"\",\n",
    "  \"\"\"<speak>S S M L supports <prosody pitch=\"-1\">nested tags. So I can speak <prosody rate=\"150%\">faster</prosody>, <prosody rate=\"75%\">or slower</prosody>, as desired.</prosody></speak>\"\"\",\n",
    "  \"\"\"<speak><prosody volume='x-soft'>I'm speaking softly.</prosody><prosody volume='x-loud'> And now, This is loud.</prosody></speak>\"\"\",\n",
    "]\n",
    "\n",
    "# Loop through 'ssml_texts' list and synthesize audio with Riva TTS for each entry 'ssml_texts'\n",
    "for ssml_text in ssml_texts:\n",
    "    req[\"text\"] = ssml_text\n",
    "    resp = riva_tts.synthesize(**req)\n",
    "    audio_samples = np.frombuffer(resp.audio, dtype=np.int16)\n",
    "    print(\"SSML Text: \", ssml_text)\n",
    "    ipd.display(ipd.Audio(audio_samples, rate=sample_rate_hz))\n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected results if you run the tutorial:\n",
    "`This is a normal sentence`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_1.wav\" type=\"audio/ogg\"></audio>  \n",
    "\n",
    "`<prosody pitch=\"0.\" rate=\"100%\">This is also a normal sentence</prosody>`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_2.wav\" type=\"audio/ogg\"></audio> \n",
    "\n",
    "`<prosody rate=\"200%\">This is a fast sentence</prosody>`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_3.wav\" type=\"audio/ogg\"></audio> \n",
    "\n",
    "`<prosody rate=\"60%\">This is a slow sentence</prosody>`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_4.wav\" type=\"audio/ogg\"></audio> \n",
    "\n",
    "`<prosody pitch=\"+1.0\">Now, I'm speaking a bit higher</prosody>`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_5.wav\" type=\"audio/ogg\"></audio> \n",
    "\n",
    "`<prosody pitch=\"-0.5\">And now, I'm speaking a bit lower</prosody>`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_6.wav\" type=\"audio/ogg\"></audio> \n",
    "\n",
    "`S S M L supports <prosody pitch=\"-1\">nested tags. So I can speak <prosody rate=\"150%\">faster</prosody>, <prosody rate=\"75%\">or slower</prosody>, as desired.</prosody>`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_7.wav\" type=\"audio/ogg\"></audio> \n",
    "\n",
    "`<prosody volume='x-soft'>I'm speaking softly.</prosody><prosody volume='x-loud'> And now, This is loud.</prosody>`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_8.wav\" type=\"audio/ogg\"></audio>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing pronunciation with the `phoneme` tag\n",
    "\n",
    "We can use the `phoneme` tag to override the pronunciation of words from the predicted pronunciation. For a given word or sequence of words, use the `ph` attribute to provide an explicit pronunciation, and the `alphabet` attribute to provide the phone set.\n",
    "\n",
    "Starting with the Riva 2.8.0 release, `ipa` will be the only supported prounciation alphabet for TTS models. Older Riva models only support `x-arpabet`.\n",
    "\n",
    "#### IPA\n",
    "For the full list of supported `ipa` phonemes, refer to the [Riva TTS Phoneme Support](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tts/tts-phones.html) page.\n",
    "\n",
    "#### Arpabet\n",
    "The full list of phonemes in the CMUdict can be found at [cmudict.phone](https://github.com/cmusphinx/cmudict/blob/master/cmudict.phones). The list of supported symbols with stress can be found at [cmudict.symbols](https://github.com/cmusphinx/cmudict/blob/master/cmudict.symbols). For a mapping of these phones to English sounds, refer to the [ARPABET Wikipedia page](https://en.wikipedia.org/wiki/ARPABET).\n",
    "\n",
    "Let's look at an example showing this custom pronunciation for Riva TTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting up Riva TTS request with SynthesizeSpeechRequest\n",
    "\"\"\"\n",
    "    Raw text is \"You say tomato, I say tomato.\"\n",
    "    We are updating this raw text with SSML:\n",
    "    1. Envelope raw text in '<speak>' tags as is required for SSML\n",
    "    2. For a substring in the raw text, add '<phoneme>' tags with 'alphabet' attribute set to 'x-arpabet' \n",
    "       (currently the only supported value) and 'ph' attribute set to a custom pronunciation based on CMUdict and ARPABET\n",
    "\n",
    "\"\"\"\n",
    "raw_text = \"You say tomato, I say tomato.\"\n",
    "ssml_text = '<speak>You say <phoneme alphabet=\"ipa\" ph=\"təˈmeɪˌtoʊ\">tomato</phoneme>, I say <phoneme alphabet=\"ipa\" ph=\"təˈmɑˌtoʊ\">tomato</phoneme>.</speak>'\n",
    "# Older arpabet version\n",
    "# ssml_text = '<speak>You say <phoneme alphabet=\"x-arpabet\" ph=\"{@T}{@AH0}{@M}{@EY1}{@T}{@OW2}\">tomato</phoneme>, I say <phoneme alphabet=\"x-arpabet\" ph=\"{@T}{@AH0}{@M}{@AA1}{@T}{@OW2}\">tomato</phoneme>.</speak>'\n",
    "\n",
    "print(\"Raw Text: \", raw_text)\n",
    "print(\"SSML Text: \", ssml_text)\n",
    "\n",
    "req[\"text\"] = ssml_text\n",
    "# Request to Riva TTS to synthesize audio\n",
    "resp = riva_tts.synthesize(**req)\n",
    "\n",
    "# Playing the generated audio from Riva TTS request\n",
    "audio_samples = np.frombuffer(resp.audio, dtype=np.int16)\n",
    "ipd.display(ipd.Audio(audio_samples, rate=sample_rate_hz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected results if you run the tutorial:\n",
    "`You say <phoneme alphabet=\"ipa\" ph=\"təˈmeɪˌtoʊ\">tomato</phoneme>, I say <phoneme alphabet=\"ipa\" ph=\"təˈmɑˌtoʊ\">tomato</phoneme>.`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_9.wav\" type=\"audio/wav\"></audio> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are more examples showing the customization of pronunciation in generated audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SSML texts we want to try\n",
    "ssml_texts = [\n",
    "  \"\"\"<speak>You say <phoneme ph=\"ˈdeɪtə\">data</phoneme>, I say <phoneme ph=\"ˈdætə\">data</phoneme>.</speak>\"\"\",\n",
    "  \"\"\"<speak>Some people say <phoneme ph=\"ˈɹut\">route</phoneme> and some say <phoneme ph=\"ˈɹaʊt\">route</phoneme>.</speak>\"\"\",\n",
    "]\n",
    "# Older arpabet version\n",
    "# ssml_texts = [\n",
    "#   \"\"\"<speak>You say <phoneme alphabet=\"x-arpabet\" ph=\"{@D}{@EY1}{@T}{@AH0}\">data</phoneme>, I say <phoneme alphabet=\"x-arpabet\" ph=\"{@D}{@AE1}{@T}{@AH0}\">data</phoneme>.</speak>\"\"\",\n",
    "#   \"\"\"<speak>Some people say <phoneme alphabet=\"x-arpabet\" ph=\"{@R}{@UW1}{@T}\">route</phoneme> and some say <phoneme alphabet=\"x-arpabet\" ph=\"{@R}{@AW1}{@T}\">route</phoneme>.</speak>\"\"\",\n",
    "# ]\n",
    "\n",
    "# Loop through 'ssml_texts' list and synthesize audio with Riva TTS for each entry 'ssml_texts'\n",
    "for ssml_text in ssml_texts:\n",
    "    req[\"text\"] = ssml_text\n",
    "    resp = riva_tts.synthesize(**req)\n",
    "    audio_samples = np.frombuffer(resp.audio, dtype=np.int16)\n",
    "    print(\"SSML Text: \", ssml_text)\n",
    "    ipd.display(ipd.Audio(audio_samples, rate=sample_rate_hz))\n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected results if you run the tutorial:\n",
    "`You say <phoneme ph=\"ˈdeɪtə\">data</phoneme>, I say <phoneme ph=\"ˈdætə\">data</phoneme>.`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_10.wav\" type=\"audio/wav\"></audio>\n",
    "\n",
    "`Some people say <phoneme ph=\"ˈɹut\">route</phoneme> and some say <phoneme ph=\"ˈɹaʊt\">route</phoneme>.`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_11.wav\" type=\"audio/wav\"></audio>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing pronunciation with the `sub` tag\n",
    "\n",
    "We can use the `sub` tag to replace the pronounciation of the specified word or phrase with a different word or phrase. You can specify the pronunciation to substitute with the alias attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting up Riva TTS request with SynthesizeSpeechRequest\n",
    "\"\"\"\n",
    "    Raw text is \"WWW is know as the web\"\n",
    "    We are updating this raw text with SSML:\n",
    "    1. Envelope raw text in '<speak>' tags as is required for SSML\n",
    "    2. Add '<sub>' tag with 'alias' attribute set to replace www with `World Wide Web`\n",
    "\n",
    "\"\"\"\n",
    "raw_text = \"WWW is know as the web.\"\n",
    "ssml_text = '<speak><sub alias=\"World Wide Web\">WWW</sub> is known as the web.</speak>'\n",
    "\n",
    "print(\"Raw Text: \", raw_text)\n",
    "print(\"SSML Text: \", ssml_text)\n",
    "\n",
    "req[\"text\"] = ssml_text\n",
    "# Request to Riva TTS to synthesize audio\n",
    "resp = riva_tts.synthesize(**req)\n",
    "# Playing the generated audio from Riva TTS request\n",
    "audio_samples = np.frombuffer(resp.audio, dtype=np.int16)\n",
    "ipd.display(ipd.Audio(audio_samples, rate=sample_rate_hz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected results if you run the tutorial:\n",
    "`<sub alias=\"World Wide Web\">WWW</sub> is known as the web.`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_12.wav\" type=\"audio/ogg\"></audio>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emphasize words with the `emphasis` tag\n",
    "\n",
    "Use the emphasis tag to emphasize words. Use the `riva-build` command with the `enable_emphasis_tag`, `start_of_emphasis_token`, and `end_of_emphasis_token` tags to enable the emphasis feature. The emphasis tag should be used per word basis. If the word ends with a punctuation, only the word will be emphasized and not the punctuation.\n",
    "\n",
    "#### Limitation\n",
    "The emphasis tag is training data dependent and is available only in the `English-US` model. The models which are trained without the emphasis tag in the training data will not result in emphasized speech. Input text containing more than one word wrapped by the emphasis tag is an invalid input. Space wrapped inside the emphasis tag is also an invalid input.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> Warning: The emphasis tag feature does not support nesting of other SSML tags inside it. The emphasis tag does not support the level attribute. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting up Riva TTS request with SynthesizeSpeechRequest\n",
    "\"\"\"\n",
    "    Raw text is \"Hello World\"\n",
    "    We are updating this raw text with SSML:\n",
    "    1. Envelope raw text in '<speak>' tags as is required for SSML\n",
    "    2. Add '<emphasis>' tag around `love`\n",
    "\n",
    "\"\"\"\n",
    "ssml_texts = [\n",
    "   \"\"\"<speak>I would <emphasis>love</emphasis> to try that.</speak>\"\"\",\n",
    "   \"\"\"<speak><emphasis>Wow!</emphasis> Thats really cool.</speak>\"\"\"\n",
    "]\n",
    "\n",
    "print(\"Raw Text: \", raw_text)\n",
    "print(\"SSML Text: \", ssml_text)\n",
    "\n",
    "for ssml_text in ssml_texts:\n",
    "    req[\"text\"] = ssml_text\n",
    "    resp = riva_tts.synthesize(**req)\n",
    "    # Playing the generated audio from Riva TTS request\n",
    "    audio_samples = np.frombuffer(resp.audio, dtype=np.int16)\n",
    "    print(\"SSML Text: \", ssml_text)\n",
    "    ipd.display(ipd.Audio(audio_samples, rate=sample_rate_hz))\n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected results if you run the tutorial:\n",
    "`I would <emphasis>love</emphasis> to try that.`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_13.wav\" type=\"audio/ogg\"></audio>\n",
    "\n",
    "`<emphasis>Wow!</emphasis> Thats really cool.`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_14.wav\" type=\"audio/ogg\"></audio>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about customizing Riva TTS with SSML can also be found in the documentation [here](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tts/tts-ssml.html#). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
