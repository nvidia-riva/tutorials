{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_tts_tts-python-advanced-pretrain-tts-tao-training/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to train Riva TTS models (FastPitch and HiFiGAN) with TAO Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial walks you through the steps to train Riva TTS models (FastPitch and HiFiGAN) from scratch with LJSpeech dataset using TAO Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this tutorial, we will customize the Riva TTS pipeline by training Riva TTS models with NVIDIA's TAO Toolkit.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective is to synthesize reasonable and natural speech for given text. Since there are no universal standards to measure quality of synthesized speech, you will need to listen to some inferred speech to tell whether a TTS model is well trained.\n",
    "\n",
    "TTS consists of two models: [FastPitch](https://arxiv.org/pdf/2006.06873.pdf) and [HiFi-GAN](https://arxiv.org/pdf/2010.05646.pdf).\n",
    "\n",
    "* FastPitch is spectrogram model generates a Mel spectrogram from text input\n",
    "* HiFiGAN is a vocoder model to generate an audio output from the Mel spectrograms generated using FastPitch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TTS using TAO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TAO launcher uses Docker containers under the hood, and **for our data and results directory to be visible to Docker, they need to be mapped**. The launcher can be configured using the config file `~/.tao_mounts.json`. Apart from the mounts, you can also configure additional options like the environment variables and the amount of shared memory available to the TAO launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The following code creates a sample `~/.tao_mounts.json`  file. Here, we can map directories in which we save the data, specs, results, and cache. You should configure it for your specific use case so these directories are correctly visible to the Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directory for this tutorial\n",
    "WORKING_DIR = 'tts_training'\n",
    "\n",
    "# Defining paths on the local host machine\n",
    "%env HOST_DATA_DIR = {WORKING_DIR}/data\n",
    "%env HOST_SPECS_DIR = {WORKING_DIR}/specs\n",
    "%env HOST_RESULTS_DIR = {WORKING_DIR}/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p $WORKING_DIR\n",
    "! mkdir -p $HOST_DATA_DIR\n",
    "! mkdir -p $HOST_SPECS_DIR\n",
    "! mkdir -p $HOST_RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "import os\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "tao_configs = {\n",
    "   \"Mounts\":[\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_DATA_DIR\"],\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_SPECS_DIR\"],\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_RESULTS_DIR\"],\n",
    "           \"destination\": \"/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.path.expanduser(\"~/.cache\"),\n",
    "           \"destination\": \"/root/.cache\"\n",
    "       }\n",
    "   ],\n",
    "   \"DockerOptions\": {\n",
    "        \"shm_size\": \"128G\",\n",
    "        \"ulimits\": {\n",
    "            \"memlock\": -1,\n",
    "            \"stack\": 67108864\n",
    "         }\n",
    "   }\n",
    "}\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(tao_configs, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the Docker image versions and the tasks that it performs. You can also check by issuing `tao --help` or:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tao info --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Relevant Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TAO Docker.\n",
    "\n",
    "# The data is saved here:\n",
    "DATA_DIR = \"/data\"\n",
    "SPECS_DIR = \"/specs\"\n",
    "RESULTS_DIR = \"/results\"\n",
    "\n",
    "# Set your encryption key and use the same key for all commands:\n",
    "KEY = 'tlt_encode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command structure for the TAO interface can be broken down as follows: `tao <task name> <subcommand>` <br> \n",
    "\n",
    "Let's see this in further detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Downloading Specs\n",
    "TAO's conversational AI toolkit works off of spec files which make it easy to edit hyperparameters on the fly. We can proceed to downloading the spec files. You may choose to modify/rewrite these specs or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command. <br>\n",
    "\n",
    "The `-o` argument indicates the folder where the default specification files will be downloaded. The `-r` argument instructs the script on where to save the logs. **Ensure the `-o` points to an empty folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download spec files for FastPitch\n",
    "! tao spectro_gen download_specs \\\n",
    "    -r $RESULTS_DIR/spectro_gen \\\n",
    "    -o $SPECS_DIR/spectro_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download spec files for HiFiGAN\n",
    "! tao vocoder download_specs \\\n",
    "    -r $RESULTS_DIR/vocoder \\\n",
    "    -o $SPECS_DIR/vocoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will use the popular LJSpeech dataset. Let's download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -O $HOST_DATA_DIR/ljspeech.tar.bz2 https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading, untar the dataset and move it to the correct directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! tar -xvf $HOST_DATA_DIR/ljspeech.tar.bz2\n",
    "! rm -rf $HOST_DATA_DIR/ljspeech\n",
    "! mv LJSpeech-1.1 $HOST_DATA_DIR/ljspeech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step downloads audio to text file lists from NVIDIA for LJSpeech and generates the manifest files. Each row should look similar to:\n",
    "\n",
    "```\n",
    "DUMMY/<file_name>.wav|<text_of_the_audio>\n",
    "```\n",
    "\n",
    "An example row is:\n",
    "\n",
    "```\n",
    "DUMMY/LJ045-0096.wav|Mrs. De Mohrenschildt thought that Oswald,\n",
    "```\n",
    "\n",
    "After having those three files in your `data_dir`, run the following command:\n",
    "\n",
    "Be patient! This step can take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tao spectro_gen dataset_convert \\\n",
    "    -e $SPECS_DIR/spectro_gen/dataset_convert_ljs.yaml \\\n",
    "    -r $RESULTS_DIR/spectro_gen/dataset_convert \\\n",
    "    data_dir=$DATA_DIR/ljspeech \\\n",
    "    dataset_name=ljspeech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TAO interface enables you to configure the training parameters from the command-line interface. <br>\n",
    "\n",
    "The process of opening the training script, finding the parameters of interest (which might be spread across multiple files), and making the changes needed, is being replaced by a simple command-line interface.\n",
    "\n",
    "For example, if the number of epochs are needed to be modified along with a change in the learning rate, you can add `trainer.max_epochs=10` and `optim.lr=0.02` and train the model. Sample commands are given below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training TTS models in TAO, we use the `tao spectro_gen train` and `tao vocoder train` commands with the following arguments:\n",
    "<ul>\n",
    "    <li>`-e`: Path to the spec file </li>\n",
    "    <li>`-g`: Number of GPUs to use </li>\n",
    "    <li>`-r`: Path to the results folder </li>\n",
    "    <li>`-k`: User specified encryption key to use while saving/loading the model </li>\n",
    "    <li>Any overrides to the spec file. For example, `trainer.max_epochs`. </li>\n",
    "</ul>\n",
    "\n",
    "NOTE: In order to get a TTS pipeline, you need to train **BOTH** FastPitch (`spectro_gen`) and HiFi-GAN (`vocoder`). For HiFi-GAN, since it's universal for a specific language, the pretrained weights from NGC will itself give you good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training FastPitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior is needed for FastPitch training. If an empty folder is provided, prior will generate on-the-fly.\n",
    "! mkdir -p $HOST_RESULTS_DIR/spectro_gen/train/prior_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [],
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!tao spectro_gen train \\\n",
    "     -e $SPECS_DIR/spectro_gen/train.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -r $RESULTS_DIR/spectro_gen/train \\\n",
    "     train_dataset=$DATA_DIR/ljspeech/ljspeech_train.json \\\n",
    "     validation_dataset=$DATA_DIR/ljspeech/ljspeech_val.json \\\n",
    "     prior_folder=$RESULTS_DIR/spectro_gen/train/prior_folder \\\n",
    "     trainer.max_epochs=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training HiFi-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of passing `trainer.max_epochs`, HiFi-GAN requires the definition of `trainer.max_steps`. Defining `trainer.max_epochs` for HiFi-GAN has no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao vocoder train \\\n",
    "     -e $SPECS_DIR/vocoder/train.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -r $RESULTS_DIR/vocoder/train \\\n",
    "     train_dataset=$DATA_DIR/ljspeech/ljspeech_train.json \\\n",
    "     validation_dataset=$DATA_DIR/ljspeech/ljspeech_val.json \\\n",
    "     trainer.max_steps=10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TTS Inference with TAO Toolkit\n",
    "\n",
    "In this section, we are going to run inference on the trained TTS models. As previously mentioned, since there are no universal standards to measure quality of synthesized speech, you will need to listen to some inferred speech to tell whether a TTS model is well trained. Therefore, we do not provide `evaluate` functionality in TAO Toolkit for TTS but only provide `infer` functionality.\n",
    "\n",
    "The inference in the following cells is not optimized for real-time performance. For real-time inference and best latency, we would deploy this model using RIVA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTS Inference with TLT checkpoint\n",
    "\n",
    "In this section, we will run inference on the `.tlt` checkpoint trained with TAO Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate spectrogram\n",
    "\n",
    "The first step for inference is generating a spectrogram. That's a NumPy array (saved as `.npy` file) for a sentence which can be converted to voice by a vocoder. We use the FastPitch model we just trained to generate a spectrogram.\n",
    "\n",
    "You may have to work with the `infer.yaml` file to set the texts you want for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao spectro_gen infer \\\n",
    "     -e $SPECS_DIR/spectro_gen/infer.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/spectro_gen/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/spectro_gen/infer \\\n",
    "     output_path=$RESULTS_DIR/spectro_gen/infer/spectro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate sound file\n",
    "\n",
    "The second step for inference is generating a `.wav` sound file based on a spectrogram you generated in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao vocoder infer \\\n",
    "     -e $SPECS_DIR/vocoder/infer.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/vocoder/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/vocoder/infer \\\n",
    "     input_path=$RESULTS_DIR/spectro_gen/infer/spectro \\\n",
    "     output_path=$RESULTS_DIR/vocoder/infer/wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython.display as ipd\n",
    "# change path of the file here\n",
    "ipd.Audio(os.environ[\"HOST_RESULTS_DIR\"] + '/vocoder/infer/wav/0.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TTS model export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With TAO, you can also export your model in a format that can deployed using NVIDIA Riva; a highly performant application framework for multi-modal conversational AI services using GPUs. The same command for exporting to ONNX can be used here. The only small variation is the configuration for `export_format` in the spec file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to RIVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao spectro_gen export \\\n",
    "     -e $SPECS_DIR/spectro_gen/export.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/spectro_gen/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/spectro_gen/export \\\n",
    "     export_format=RIVA \\\n",
    "     export_to=spectro_gen.riva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao vocoder export \\\n",
    "     -e $SPECS_DIR/vocoder/export.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/vocoder/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/vocoder/export \\\n",
    "     export_format=RIVA \\\n",
    "     export_to=vocoder.riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to ONNX (Export to ONNX is not needed for RIVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao spectro_gen export \\\n",
    "     -e $SPECS_DIR/spectro_gen/export.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/spectro_gen/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/spectro_gen/export \\\n",
    "     export_format=ONNX \\\n",
    "     export_to=spectro_gen.eonnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao vocoder export \\\n",
    "     -e $SPECS_DIR/vocoder/export.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/vocoder/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/vocoder/export \\\n",
    "     export_format=ONNX \\\n",
    "     export_to=vocoder.eonnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving FastPitch and HiFiGAN .riva models to a common folder\n",
    "# This is required for Riva to build a TTS pipeline\n",
    "! mkdir -p $HOST_RESULTS_DIR/riva\n",
    "! cp $HOST_RESULTS_DIR/spectro_gen/export/spectro_gen.riva $HOST_RESULTS_DIR/riva/\n",
    "! cp $HOST_RESULTS_DIR/vocoder/export/vocoder.riva $HOST_RESULTS_DIR/riva/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now deploy these TTS models to production using Riva.\n",
    "\n",
    "Make sure to keep the path of `spectro_gen.riva` and `vocoder.riva` handy for deployment i.e. `tts_training/results/riva/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d07f1aa6807adb4e3490764d3413abc2e022e4483e8e90f694051be99589cf55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
