{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NpvEOMs_mKp"
      },
      "source": [
        "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/rivaasrasr-finetuning-conformer-ctc-nemo/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
        "\n",
        "# How to Fine-Tune a Riva ASR Acoustic Model with NVIDIA NeMo\n",
        "This tutorial walks you through how to fine-tune an NVIDIA Riva Parakeet acoustic model with NVIDIA NeMo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h7SXF6G_mKw"
      },
      "source": [
        "## NVIDIA Riva Overview\n",
        "\n",
        "NVIDIA Riva is a GPU-accelerated SDK for building speech AI applications that are customized for your use case and deliver real-time performance. <br/>\n",
        "Riva offers a rich set of speech and natural language understanding (NLU) services such as:\n",
        "\n",
        "- Automated speech recognition (ASR).\n",
        "- Text-to-Speech synthesis (TTS).\n",
        "- A collection of natural language processing (NLP) services, such as named entity recognition (NER), punctuation, and intent classification.\n",
        "\n",
        "In this tutorial, we will fine-tune a Riva ASR acoustic model with NeMo. <br>\n",
        "To understand the basics of Riva ASR APIs, refer to [Getting started with Riva ASR in Python](https://github.com/nvidia-riva/tutorials/blob/stable/asr-python-basics.ipynb). <br>\n",
        "\n",
        "For more information about Riva, refer to the [Riva developer documentation](https://developer.nvidia.com/riva)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv47EmBp_mK2"
      },
      "source": [
        "## NeMo (Neural Modules)\n",
        "[NVIDIA NeMo](https://developer.nvidia.com/nvidia-nemo) is an open-source framework for building, training, and fine-tuning GPU-accelerated speech AI and NLU models with a simple Python interface. For information about how to set up NeMo, refer to the [NeMo GitHub](https://github.com/NVIDIA/NeMo) instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1O2bGCMAQDs8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "You can run either this tutorial locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
        "\n",
        "Perform the following steps to setup in Google Colab:\n",
        "1. Open a new Python 3 notebook.\n",
        "2. Import this notebook from GitHub.\n",
        "   a. Click **File** > **Upload Notebook** > **GITHUB** tab > copy/paste the GitHub URL.\n",
        "3. Connect to an instance with a GPU.\n",
        "   a. Click **Runtime** > Change the runtime type > select **GPU** for the hardware accelerator.\n",
        "4. Run this cell to set up the dependencies.\n",
        "5. Restart the runtime.\n",
        "   a. Click **Runtime** > **Restart Runtime** for any upgraded packages to take effect.\n",
        "\"\"\"\n",
        "\n",
        "# Before running this notebook\n",
        "   # pull docker container: docker pull nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04 (recommended)\n",
        "   # launch container and run below\n",
        "      # `apt-get update`\n",
        "      # install python3.10.12 and pip\n",
        "         # `apt-get install -y --no-install-recommends   python3.10 python3.10-venv python3.10-distutils python3.10-dev`\n",
        "         # `apt-get install -y --no-install-recommends python3-pip`\n",
        "         # `update-alternatives --install /usr/bin/python python /usr/bin/python3.10 1`\n",
        "         # `update-alternatives --set python /usr/bin/python3.10`\n",
        "         # `python --version`\n",
        "\n",
        "# run this notebook (you might need to run this cell two times where the site-packages need to be synced in first run)\n",
        "\n",
        "# Install Dependencies\n",
        "!pip3 install wget\n",
        "!apt-get install -y wget sox libsndfile1 ffmpeg libsox-fmt-mp3 jq git # if failed, run `apt-get update` first\n",
        "!pip3 install text-unidecode\n",
        "!pip3 install matplotlib\n",
        "!pip3 install Cython\n",
        "!pip3 install torch==2.3.0\n",
        "!pip3 install librosa\n",
        "\n",
        "\n",
        "# Install NeMo\n",
        "BRANCH = 'v2.0.0'\n",
        "!python3 -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[asr]\n",
        "\n",
        "# Modify some package for dependency consideration\n",
        "!pip3 install numpy==1.24.1\n",
        "!pip3 install --no-cache-dir huggingface-hub==0.24.5\n",
        "!pip3 install transformers==4.40.2\n",
        "!pip3 install numba==0.59.0\n",
        "!pip3 install pytorch-lightning==2.3.3\n",
        "!pip3 install lightning-utilities==0.10.1\n",
        "!pip3 install omegaconf==2.3.0\n",
        "!pip3 install jiwer==3.0.4\n",
        "\n",
        "\"\"\"\n",
        "Remember to restart the runtime for the kernel to pick up any upgraded packages (e.g. matplotlib)!\n",
        "Alternatively, in the case where you want to use the \"Run All Cells\" (or similar) option,\n",
        "uncomment `exit()` below to crash and restart the kernel.\n",
        "\"\"\"\n",
        "# exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo27Unex_mLG"
      },
      "source": [
        "---\n",
        "## Fine-Tuning an ASR model with NeMo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfRpENJJ_mLy"
      },
      "source": [
        "### Download the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKrmQQuK_mLz"
      },
      "source": [
        "In this tutorial, we will use the popular AN4 dataset. Let's download it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eFF0r7z_mLz"
      },
      "outputs": [],
      "source": [
        "! wget https://dldata-public.s3.us-east-2.amazonaws.com/an4_sphere.tar.gz  # for the original source, please visit http://www.speech.cs.cmu.edu/databases/an4/an4_sphere.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAsvlh53_mL0"
      },
      "source": [
        "After downloading, untar the dataset and move it to the correct directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OipV7YVq_mL1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "DATA_DIR = os.getcwd()\n",
        "os.environ[\"DATA_DIR\"] = DATA_DIR\n",
        "! tar -xvf an4_sphere.tar.gz\n",
        "! mv an4 $DATA_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Tilg2Eg_mL2"
      },
      "source": [
        "### Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKud8fQ8_mL3"
      },
      "source": [
        "This step converts the `.mp3` files into `.wav` files and splits the data into training and testing sets. It also generates a \"meta-data\" file to be consumed by the data-loader for training and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54Pff6vr_mL3"
      },
      "outputs": [],
      "source": [
        "import json, librosa, os, glob\n",
        "import subprocess\n",
        "\n",
        "\n",
        "source_data_dir = f\"{DATA_DIR}/an4\"\n",
        "target_data_dir = f\"{DATA_DIR}/an4_converted\"\n",
        "\n",
        "def an4_build_manifest(transcripts_path, manifest_path, target_wavs_dir):\n",
        "    \"\"\"Build an AN4 manifest from a given transcript file.\"\"\"\n",
        "    with open(transcripts_path, 'r') as fin:\n",
        "        with open(manifest_path, 'w') as fout:\n",
        "            for line in fin:\n",
        "                # Lines look like this:\n",
        "                # <s> transcript </s> (fileID)\n",
        "                transcript = line[: line.find('(') - 1].lower()\n",
        "                transcript = transcript.replace('<s>', '').replace('</s>', '')\n",
        "                transcript = transcript.strip()\n",
        "\n",
        "                file_id = line[line.find('(') + 1 : -2]  # e.g. \"cen4-fash-b\"\n",
        "                audio_path = os.path.join(target_wavs_dir, file_id + '.wav')\n",
        "\n",
        "                duration = librosa.core.get_duration(filename=audio_path)\n",
        "\n",
        "                # Write the metadata to the manifest\n",
        "                metadata = {\"audio_filepath\": audio_path, \"duration\": duration, \"text\": transcript}\n",
        "                json.dump(metadata, fout)\n",
        "                fout.write('\\n')\n",
        "\n",
        "\"\"\"Process AN4 dataset.\"\"\"\n",
        "if not os.path.exists(source_data_dir):\n",
        "    link = 'http://www.speech.cs.cmu.edu/databases/an4/an4_sphere.tar.gz'\n",
        "    raise ValueError(\n",
        "        f\"Data not found at `{source_data_dir}`. Please download the AN4 dataset from `{link}` \"\n",
        "        f\"and extract it into the folder specified by the `source_data_dir` argument.\"\n",
        "    )\n",
        "\n",
        "# Convert SPH files to WAV files\n",
        "sph_list = glob.glob(os.path.join(source_data_dir, '**/*.sph'), recursive=True)\n",
        "target_wavs_dir = os.path.join(target_data_dir, 'wavs')\n",
        "if not os.path.exists(target_wavs_dir):\n",
        "    print(f\"Creating directories for {target_wavs_dir}.\")\n",
        "    os.makedirs(os.path.join(target_data_dir, 'wavs'))\n",
        "\n",
        "for sph_path in sph_list:\n",
        "    wav_path = os.path.join(target_wavs_dir, os.path.splitext(os.path.basename(sph_path))[0] + '.wav')\n",
        "    cmd = [\"sox\", sph_path, wav_path]\n",
        "    subprocess.run(cmd, check=True)\n",
        "\n",
        "# Build AN4 manifests\n",
        "train_transcripts = os.path.join(source_data_dir, 'etc/an4_train.transcription')\n",
        "train_manifest = os.path.join(target_data_dir, 'train_manifest.json')\n",
        "an4_build_manifest(train_transcripts, train_manifest, target_wavs_dir)\n",
        "\n",
        "test_transcripts = os.path.join(source_data_dir, 'etc/an4_test.transcription')\n",
        "test_manifest = os.path.join(target_data_dir, 'test_manifest.json')\n",
        "an4_build_manifest(test_transcripts, test_manifest, target_wavs_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RAtW4wg_mL4"
      },
      "source": [
        "Let's listen to a sample audio file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGH3btfn_mL6"
      },
      "outputs": [],
      "source": [
        "# change path of the file here\n",
        "import os\n",
        "import IPython.display as ipd\n",
        "path = os.environ[\"DATA_DIR\"] + '/an4_converted/wavs/an268-mbmg-b.wav'\n",
        "ipd.Audio(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FubCSAin_mMC"
      },
      "source": [
        "### Tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zBKAO36_mME"
      },
      "source": [
        "If training dataset is not enough (<50 hrs) we recommend using tokenizers from pretrained model itself. The following tutorials explains how to use tokenizers from pretrained models for finetuning Parakeet models. If there's a change in vocab or you wish to train your own tokenizers you can use [NeMo tokenizer training script](https://github.com/NVIDIA/NeMo/blob/main/scripts/tokenizers/process_asr_text_tokenizer.py) and use [Hybrid model training script](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/asr_hybrid_transducer_ctc/speech_to_text_hybrid_rnnt_ctc_bpe.py) to finetune the model on your data. Refer [asr-finetune-conformer-nemo](https://github.com/nvidia-riva/tutorials/blob/main/asr-finetune-conformer-ctc-nemo.ipynb) tutorial for more details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-yDMTMO_mMJ"
      },
      "source": [
        "#### Training Parakeet-Hybrid model\n",
        "\n",
        "Hybrid RNNT-CTC models is a group of models with both the RNNT and CTC decoders. Training a hybrid model would speedup the convergence for the CTC models and would enable the user to use a single model which works as both a CTC and RNNT model. This category can be used with any of the ASR models. Hybrid models uses two decoders of CTC and RNNT on the top of the encoder.\n",
        "\n",
        "NeMo uses `.yaml` files to configure the training parameters. You may update them directly by editing the configuration file or from the command-line interface. For example, if the number of epochs needs to be modified, along with a change in the learning rate, you can add `trainer.max_epochs=100` and `optim.lr=0.02` and train the model.\n",
        "\n",
        "The following sample command uses the `speech_to_text_finetune.py` script in the `examples` folder to train/fine-tune a Parakeet-Hybrid ASR model for 1 epoch. For other ASR models like Citrinet, Conformer, you may find the appropriate config files in the NeMo GitHub repo under [examples/asr/conf/](https://github.com/NVIDIA/NeMo/tree/main/examples/asr/conf).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-bkkgxxv_mMK"
      },
      "outputs": [],
      "source": [
        "# To fully train the model, you'll need to increase trainer.max_epochs from 1.\n",
        "# Empirical evidence suggests that around 200 epochs should suffice.\n",
        "NEMO_DIR = '/workspace/NeMo'\n",
        "BRANCH='v2.0.0'\n",
        "!git clone -b $BRANCH https://github.com/NVIDIA/NeMo $NEMO_DIR\n",
        "!python $NEMO_DIR/examples/asr/speech_to_text_finetune.py \\\n",
        "        --config-path=\"../asr/conf/fastconformer/hybrid_transducer_ctc/\" --config-name=fastconformer_hybrid_transducer_ctc_bpe \\\n",
        "        +init_from_pretrained_model=stt_en_fastconformer_hybrid_large_pc \\\n",
        "        ++model.train_ds.manifest_filepath=\"$DATA_DIR/an4_converted/train_manifest.json\" \\\n",
        "        ++model.validation_ds.manifest_filepath=\"$DATA_DIR/an4_converted/test_manifest.json\" \\\n",
        "        ++model.optim.sched.d_model=1024 \\\n",
        "        ++trainer.devices=1 \\\n",
        "        ++trainer.max_epochs=1 \\\n",
        "        ++trainer.precision=bf16 \\\n",
        "        ++model.optim.name=\"adamw\" \\\n",
        "        ++model.optim.lr=0.1 \\\n",
        "        ++model.optim.weight_decay=0.001 \\\n",
        "        ++model.optim.sched.warmup_steps=100 \\\n",
        "        ++exp_manager.version=test \\\n",
        "        ++exp_manager.use_datetime_version=False \\\n",
        "        ++exp_manager.exp_dir=$DATA_DIR/checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_w2KBGHaSf4S"
      },
      "outputs": [],
      "source": [
        "nemo_file_path = os.path.join(DATA_DIR, 'checkpoints/FastConformer-Hybrid-Transducer-CTC-BPE/test/checkpoints/FastConformer-Hybrid-Transducer-CTC-BPE.nemo')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKDipfZF_mMK"
      },
      "source": [
        "### ASR Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3svjWpR_mMN"
      },
      "source": [
        "Now that we have a model trained, we need to check how well it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4ATNCSe_mMN"
      },
      "outputs": [],
      "source": [
        "!python $NEMO_DIR/examples/asr/speech_to_text_eval.py \\\n",
        "    model_path=$nemo_file_path \\\n",
        "    dataset_manifest=$DATA_DIR/an4_converted/test_manifest.json \\\n",
        "    output_filename=./test_manifest_predictions.json \\\n",
        "    batch_size=32 \\\n",
        "    amp=True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9ygVAEC_mMS"
      },
      "source": [
        "### ASR Model Export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap5CjiV4_mMT"
      },
      "source": [
        "With NeMo, you can also export your model in a format that can be deployed using NVIDIA Riva: a highly performant application framework for multi-modal conversational AI services using GPUs. The same command for exporting to ONNX can be used here. The only small variation is the configuration for `export_format` in the spec file.\n",
        "The model above is trained with hybrid loss which means, it is a fusion of both CTC and RNNT model. We need to extract a specific decoder to use in RIVA. We can use [convert_nemo_asr_hybrid_to_ctc.py](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/asr_hybrid_transducer_ctc/helpers/convert_nemo_asr_hybrid_to_ctc.py) script to extract the required decoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nL_weOoISf4S"
      },
      "outputs": [],
      "source": [
        "ctc_model_path = os.path.join(DATA_DIR, 'checkpoints/FastConformer-Hybrid-Transducer-CTC-BPE/test/checkpoints/FastConformer-CTC-BPE_ctc.nemo')\n",
        "! python3 $NEMO_DIR/examples/asr/asr_hybrid_transducer_ctc/helpers/convert_nemo_asr_hybrid_to_ctc.py -i $nemo_file_path -o $ctc_model_path -t ctc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myI104s2pY13"
      },
      "source": [
        "#### Install the Packages\n",
        "\n",
        "We will now install the NeMo and `nemo2riva` packages. `nemo2riva` is available on [NVIDIA NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/resources/riva_quickstart/files?version=2.8.1). Make sure you install NGC CLI first before running the following commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd0Y3Gys_mMU"
      },
      "outputs": [],
      "source": [
        "!pip install nvidia-pyindex\n",
        "!ngc registry resource download-version \"nvidia/riva/riva_quickstart:\"$__riva_version__\n",
        "!pip install nemo2riva==2.19.0\n",
        "!pip install protobuf==3.20.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XNolXIPptAr"
      },
      "source": [
        "#### Convert to Riva\n",
        "\n",
        "Convert the downloaded model to the `.riva` format. We will set the encryption key with `--key=nemotoriva`. Choose a different encryption key value when generating `.riva` models for production.\n",
        "\n",
        "For deploying the RNNT model in RIVA, use `nemo2riva --out $riva_file_path --key=nemotoriva --format nemo $rnnt_file_path`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlUNmVHKp1ft"
      },
      "outputs": [],
      "source": [
        "riva_file_path = ctc_model_path[:-5]+\".riva\"\n",
        "!nemo2riva --key=nemotoriva --max-dim 1000 --onnx-opset 18 --out $riva_file_path $ctc_model_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAtoNe9krC2z"
      },
      "source": [
        "## More Resources\n",
        "You can find more information about working with NeMo's ASR models in the [ASR section](https://github.com/NVIDIA/NeMo/tree/main/tutorials/asr) of the NeMo tutorials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv7ZRPoc_mMa"
      },
      "source": [
        "## What's Next?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ7hWpRT_mMb"
      },
      "source": [
        "You can use NeMo to build custom models for your own applications, and deploy them with NVIDIA Riva! Refer to the [Conformer-CTC deployment tutorial](https://github.com/nvidia-riva/tutorials/blob/main/asr-deployment-conformer-ctc.ipynb)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
