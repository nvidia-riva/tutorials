{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/rivaasrasr-deploy-am-and-ngram-lm/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to Deploy a Custom Language Model (n-gram) Trained with NeMo on Riva\n",
    "This tutorial walks you through the deployment of a custom language model (n-gram) trained with NVIDIA NeMo on NVIDIA Riva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVIDIA Riva Overview\n",
    "\n",
    "NVIDIA Riva is a GPU-accelerated SDK for building speech AI applications that are customized for your use case and deliver real-time performance. <br/>\n",
    "Riva offers a rich set of speech and natural language understanding services such as:\n",
    "\n",
    "- Automated speech recognition (ASR).\n",
    "- Text-to-Speech synthesis (TTS).\n",
    "- A collection of natural language processing (NLP) services, such as named entity recognition (NER), punctuation, and intent classification.\n",
    "\n",
    "In this tutorial, we will deploy an ASR language model (n-gram) trained with NeMo on Riva. <br> \n",
    "To understand the basics of Riva ASR APIs, refer to [Getting started with Riva ASR in Python](https://github.com/nvidia-riva/tutorials/blob/main/asr-basics.ipynb). <br>\n",
    "To see how to pretrain and fine-tune an n-gram language model for ASR with NeMo, refer to [this tutorial](). <br>\n",
    "\n",
    "For more information about Riva, refer to the [Riva product page](https://www.nvidia.com/en-us/ai-data-science/products/riva/) and [Riva developer documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeMo (Neural Modules) and `nemo2riva`\n",
    "[NVIDIA NeMo](https://developer.nvidia.com/nvidia-nemo) is an open-source framework for building, training, and fine-tuning GPU-accelerated speech AI and natural language understanding (NLU) models with a simple Python interface. To fine-tune a Conformer-CTC acoustic model with NeMo, refer to the [Conformer-CTC fine-tuning tutorial](https://github.com/nvidia-riva/tutorials/blob/main/asr-finetuning-citrinet-nemo.ipynb).\n",
    "\n",
    "The [`nemo2riva`]() command-line tool provides the capability to export your `.nemo` model in a format that can be deployed using [NVIDIA Riva](https://developer.nvidia.com/riva), a highly performant application framework for multi-modal conversational AI services using GPUs. A Python `.whl` file for `nemo2riva` is included in the [Riva Quick Start](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/resources/riva_quickstart) resource folder. \\[Editor's Note: This next sentence is not yet true, but hopefully will be by the time this tutorial is released.\\] You can also install `nemo2riva` with `pip`, as shown in the [Conformer-CTC fine-tuning tutorial](https://github.com/nvidia-riva/tutorials/blob/main/asr-finetuning-conformer-ctc-nemo.ipynb). \n",
    "\n",
    "This tutorial explores taking a `.riva` model &mdash; the result of invoking the `nemo2riva` CLI tool (refer to the [Conformer-CTC fine-tuning tutorial](https://github.com/nvidia-riva/tutorials/blob/main/asr-finetuning-conformer-ctc-nemo.ipynb)) &mdash; and leveraging the Riva ServiceMaker framework to aggregate all the necessary artifacts for Riva deployment to a target environment. Once the model is deployed in Riva, you can issue inference requests to the server. We will demonstrate how quick and straightforward this whole process is.\n",
    "In this tutorial, you will learn how to:\n",
    "- Build an `.rmir` model pipeline from a `.riva` file with Riva ServiceMaker.\n",
    "- Deploy the model locally on the Riva server.\n",
    "- Send inference requests from a demo client using Riva API bindings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, ensure you have:\n",
    "- Access to NVIDIA NGC and are able to download the Riva Quick Start [resources](https://ngc.nvidia.com/catalog/resources/nvidia:riva:riva_quickstart).\n",
    "-  A _language_ model file that you want to deploy.\n",
    "    - For more information on training and exporting an n-gram language model, refer to the [NeMo Language Modeling documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/asr_language_modeling.html).  \n",
    "    - The language model file can be in either of the following formats: \n",
    "        - `.arpa`. You can download a pre-trained version from the [Riva ASR LM NGC model page](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/speechtotext_en_us_lm).\n",
    "        - `.binary`. You can download a pre-trained version from the [Riva ASR LM NGC model page](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/speechtotext_en_us_lm).\n",
    "- An _acoustic_ model file in the `.riva` format that you want to deploy. You can convert a `.nemo` model file to a `.riva` model file with the `nemo2riva` command.\n",
    "    - For more information on customizing a Conformer-CTC acoustic model with NeMo and exporting the resulting model with `nemo2riva`, refer to the [Conformer-CTC fine-tuning tutorial](). \n",
    "    - Alternatively, you can obtain a pre-trained Conformer-CTC `.riva` model for English ASR [here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/speechtotext_en_us_conformer). \n",
    "    - For more information on training NeMo models, refer to the [Training](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/core/core.html#training) section in the [NeMo documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/index.html). \n",
    "    - For more information on Conformer-CTC's architecture, refer to the [Conformer-CTC](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/models.html#conformer-ctc) section of the [NeMo ASR Models](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/models.html) page. \n",
    "    - For more information on the configuration files necessary for training Conformer-CTC with NeMo, refer to the [Conformer-CTC](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/configs.html#conformer-ctc) section of the [NeMo ASR Model Configuration Files](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/configs.html) page.\n",
    "- Weighted Finite State Transducer (WFST) tokenizer and verbalizer files for Inverse Text Normalization (ITN). \n",
    "    - For more information on WFST and ITN, refer to the [NeMo Inverse Text Normalization: From Development to Production](https://arxiv.org/pdf/2104.05055.pdf) paper.\n",
    "    - You can download pretrained WFST ITN model files from this [NVIDIA GPU Cloud (NGC)](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/inverse_normalization_en_us) model page. \n",
    "- A decoder vocabulary file. You can download one from the [Riva ASR LM NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/speechtotext_en_us_lm) model page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Riva ServiceMaker\n",
    "Riva ServiceMaker is a set of tools that aggregates all the necessary artifacts (models, files, configurations, and user settings) for Riva deployment to a target environment. It has two main components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riva-Build\n",
    "\n",
    "This step helps build a Riva-ready version of the model. Its only output is an intermediate format (called an RMIR) of an end-to-end pipeline for the supported services within Riva. Let's consider an ASR n-gram language model. <br>\n",
    "\n",
    "`riva-build` is responsible for the combination of one or more exported models (`.riva` files) into a single file containing an intermediate format called Riva Model Intermediate Representation (`.rmir`). This file contains a deployment-agnostic specification of the whole end-to-end pipeline along with all the assets required for the final deployment and inference. For more information, refer to the [documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-pipeline-configuration.html?highlight=pipeline%20configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riva_line_list = !wget -qO- https://docs.nvidia.com/deeplearning/riva/user-guide/docs/index.html | grep \"NVIDIA Riva Skills\"\n",
    "riva_line_string = riva_line_list[0]\n",
    "__riva_version__ = riva_line_string.split(' ')[3]\n",
    "# __riva_version__ = '2.14.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_TYPE=\"AMD64\" #Change this to `ARM64_linux` or `ARM64_l4t` in case of an ARM64 machine.\n",
    "TARGET_MACHINE=\"AMD64\" #Change this to `ARM64_linux` or `ARM64_l4t` in case of an ARM64 machine.\n",
    "# KEY = \"nemotoriva\" ##Encryption key used during nemo2riva # tlt_encode for the standard FastPitch and HiFiGAN RMIRs\n",
    "KEY = \"tlt_encode\" ##Encryption key used during nemo2riva # tlt_encode for the standard FastPitch and HiFiGAN RMIRs\n",
    "FORCE = True ## Whether to force-build a new TTS RMIR and replace any existing RMIRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Riva NGC, servicemaker image config.\n",
    "if MACHINE_TYPE.lower() in [\"amd64\", \"arm64_linux\"]:\n",
    "    RIVA_SM_CONTAINER = f\"nvcr.io/nvidia/riva/riva-speech:{__riva_version__}-servicemaker\"\n",
    "elif MACHINE_TYPE.lower()==\"arm64_l4t\":\n",
    "    RIVA_SM_CONTAINER = f\"nvcr.io/nvidia/riva/riva-speech:{__riva_version__}-servicemaker-l4t-aarch64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "ASR_MODEL_DIR = os.path.join(os.getcwd(), \"asr-models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All model paths relative to Riva Servicemaker docker include the _SM suffix\n",
    "\n",
    "ASR_MODEL_DIR_SM = \"/data\" # Path where we mount the downloaded ASR models in the Servicemaker docker\n",
    "\n",
    "# Relative path to Acoustic Model that we fine-tuned in Notebook 5 of this lab\n",
    "AM_SM = os.path.join(ASR_MODEL_DIR_SM, \"custom-models\", \"riva\", \"Conformer-CTC-BPE.riva\")\n",
    "\n",
    "# Relative path to LM model artifacts\n",
    "# Model that we fine-tuned in Notebook 4 of this lab\n",
    "NGRAM_DIR = \"ngram-results\"\n",
    "# DECODING_LM_BIN_SM = os.path.join(ASR_MODEL_DIR_SM, NGRAM_DIR, \"interpolated_lm_60-40.bin\")\n",
    "DECODING_LEXICON_SM = os.path.join(ASR_MODEL_DIR_SM, NGRAM_DIR, \"interpolated_lm_60-40.lexicon\")\n",
    "LM_DIR = glob.glob(os.path.join(ASR_MODEL_DIR, \"speechtotext_en_us_lm_vdeployable*\"))[-1].split('/')[-1]\n",
    "DECODING_LM_BIN_SM = os.path.join(ASR_MODEL_DIR_SM, LM_DIR, \"en-US_default_6.0.bin\")\n",
    "DECODING_VOCAB_SM = os.path.join(ASR_MODEL_DIR_SM, LM_DIR, \"en-US_default_6.0_dict_vocab.txt\")\n",
    "\n",
    "# Relative path to ITN artifacts\n",
    "ITN_DIR = glob.glob(os.path.join(ASR_MODEL_DIR, \"inverse_normalization_en_us_vdeployable*\"))[-1].split('/')[-1]\n",
    "WFST_TOKENIZER_MODEL_SM = os.path.join(ASR_MODEL_DIR_SM, ITN_DIR, \"tokenize_and_classify.far\")\n",
    "WFST_VERBALIZER_MODEL_SM = os.path.join(ASR_MODEL_DIR_SM, ITN_DIR, \"verbalize.far\")\n",
    "FAR_SPEECH_HINTS_SM = os.path.join(ASR_MODEL_DIR_SM, ITN_DIR, \"speech_class.far\")\n",
    "\n",
    "# Relative path where the generated .rmir file will be stored\n",
    "RMIR_DIR = \"custom-models/rmir\"\n",
    "!mkdir -p $ASR_MODEL_DIR/$RMIR_DIR\n",
    "ASR_RMIR_DIR_SM = os.path.join(ASR_MODEL_DIR_SM, RMIR_DIR)\n",
    "ASR_RMIR_SM = os.path.join(ASR_RMIR_DIR_SM, \"asr_lm_itn_offline_custom.rmir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the ServiceMaker Docker container\n",
    "! docker pull $RIVA_SM_CONTAINER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the `.rmir` file\n",
    "\n",
    "**Notes** \n",
    "1. If your language model is in the `.arpa` format, use the flag `--decoding_language_model_arpa=$DECODING_LM_ARPA_SM` when invoking `riva-build`.\n",
    "2. If your language model is in the `.binary` format, use the flag `--decoding_language_model_binary=$DECODING_LM_BINARY_SM` when invoking `riva-build`.\n",
    "3. Refer to the [Riva ASR Pipeline Configuration documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-pipeline-configuration.html) if you want to build an ASR pipeline for a supported language other than US English. To obtain the proper `riva-build` parameters for your particular application, select the acoustic model (the parameters below assume Conformer-CTC), language, and pipeline type (offline for the purposes of this tutorial) from the interactive web menu at the bottom of the first section of the page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to self: I tried replacing \n",
    "```sh\n",
    "--decoding_vocab=$DECODING_VOCAB_SM\n",
    "```\n",
    "with \n",
    "```sh\n",
    "--decoding_lexicon=$DECODING_LEXICON_SM\n",
    "```\n",
    "but I couldn't start the Riva server with the resulting RMIR and model files. \n",
    "\n",
    "Now, I'm trying to use the default LM and vocabulary, so the only customized component in the pipeline is the AM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Syntax: \n",
    "# riva-build <task-name> \\\n",
    "#     output-dir-for-rmir/model.rmir[:key] \\\n",
    "#     dir-for-riva/acoustic_model.riva[:key] \\\n",
    "#     --decoding_language_model_<arpa, binary>\n",
    "! docker run --rm --gpus all -v $ASR_MODEL_DIR:$ASR_MODEL_DIR_SM $RIVA_SM_CONTAINER -- \\\n",
    "    riva-build speech_recognition $ASR_RMIR_SM:$KEY $AM_SM:$KEY \\\n",
    "        --force \\\n",
    "        --offline \\\n",
    "        --name=custom-conformer-en-US-asr-offline \\\n",
    "        --return_separate_utterances=True \\\n",
    "        --featurizer.use_utterance_norm_params=False \\\n",
    "        --featurizer.precalc_norm_time_steps=0 \\\n",
    "        --featurizer.precalc_norm_params=False \\\n",
    "        --ms_per_timestep=40 \\\n",
    "        --endpointing.start_history=200 \\\n",
    "        --nn.fp16_needs_obey_precision_pass \\\n",
    "        --endpointing.residue_blanks_at_start=-2 \\\n",
    "        --chunk_size=4.8 \\\n",
    "        --left_padding_size=1.6 \\\n",
    "        --right_padding_size=1.6 \\\n",
    "        --max_batch_size=16 \\\n",
    "        --featurizer.max_batch_size=512 \\\n",
    "        --featurizer.max_execution_batch_size=512 \\\n",
    "        --decoder_type=flashlight \\\n",
    "        --flashlight_decoder.asr_model_delay=-1 \\\n",
    "        --decoding_language_model_binary=$DECODING_LM_BIN_SM \\\n",
    "        --decoding_vocab=$DECODING_VOCAB_SM \\\n",
    "        --flashlight_decoder.lm_weight=0.8 \\\n",
    "        --flashlight_decoder.word_insertion_score=1.0 \\\n",
    "        --flashlight_decoder.beam_size=32 \\\n",
    "        --flashlight_decoder.beam_threshold=20. \\\n",
    "        --flashlight_decoder.num_tokenization=1 \\\n",
    "        --language_code=en-US \\\n",
    "        --wfst_tokenizer_model=$WFST_TOKENIZER_MODEL_SM \\\n",
    "        --wfst_verbalizer_model=$WFST_VERBALIZER_MODEL_SM \\\n",
    "        --speech_hints_model=$FAR_SPEECH_HINTS_SM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riva-Deploy\n",
    "\n",
    "The deployment tool takes as input one or more RMIR files and a target model repository directory. It creates an ensemble configuration specifying the pipeline for the execution and finally writes all those assets to the output model repository directory.\n",
    "\n",
    "**Note:** \n",
    "1. If you added an encryption key to your `.rmir` file when building it with `riva-build`, make sure to append a colon and then the key's value to the model's name in the `riva-deploy` command, as shown below.\n",
    "2. When running `riva-deploy`, we map `$ASR_MODEL_DIR/custom-models` to `$ASR_MODEL_DIR_SM` (`/data`) inside the Riva ServiceMaker Docker container. This is because the scripts in the Riva Skills Quick Start resource folder expect the directory containing the `rmir` and `models` directories to be mapped to `/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the model repostory relative to the SM docker\n",
    "MODEL_REPO_SM = os.path.join(ASR_MODEL_DIR_SM, \"models\")\n",
    "# Reset the RMIR path relative to the ServiceMaker Docker container\n",
    "ASR_RMIR_SM = os.path.join(ASR_MODEL_DIR_SM, \"rmir\", \"asr_lm_itn_offline_custom.rmir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Syntax: riva-deploy -f dir-for-rmir/model.rmir[:key] output-dir-for-repository\n",
    "! docker run --rm --gpus all -v $ASR_MODEL_DIR/custom-models:$ASR_MODEL_DIR_SM $RIVA_SM_CONTAINER -- \\\n",
    "    riva-deploy -f  $ASR_RMIR_SM:$KEY $MODEL_REPO_SM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Start the Riva Server\n",
    "After the model repository is generated, we are ready to start the Riva server. If you didn't already do so in a previous tutorial in this lab, download the [Riva Quick Start](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/resources/riva_quickstart) resource from NGC. \n",
    "Set the path to the directory here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngc_download_and_get_dir(ngc_resource_name, resource_description, resource_type=\"model\", parent_dir=ASR_MODEL_DIR):\n",
    "    default_download_folder = \"_v\".join(ngc_resource_name.split(\"/\")[-1].split(\":\"))\n",
    "    download_path = os.path.join(parent_dir, default_download_folder)\n",
    "    if os.path.exists(download_path):\n",
    "        print(f\"{resource_description} exists, skipping download\")\n",
    "        return default_download_folder\n",
    "    ngc_output = !ngc registry $resource_type download-version $ngc_resource_name --dest $parent_dir\n",
    "    if not os.path.exists(download_path):\n",
    "        ngc_output_formatted='\\n'.join(ngc_output)\n",
    "        logging.error(\n",
    "            f\"NGC was not able to download the requested model {ngc_resource_name}. \"\n",
    "            \"Please check the NGC error message, remove all directories, and re-start the \"\n",
    "            f\"notebook. NGC message: {ngc_output_formatted}\"\n",
    "        )\n",
    "        return None\n",
    "    print(f\"Successfully downloaded {resource_description}\")\n",
    "    return default_download_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TARGET_MACHINE.lower() in [\"amd64\", \"arm64_linux\"]:\n",
    "    quickstart_link = f\"nvidia/riva/riva_quickstart:{__riva_version__}\"\n",
    "else:\n",
    "    quickstart_link = f\"nvidia/riva/riva_quickstart_arm64:{__riva_version__}\"\n",
    "\n",
    "RIVA_DIR = ngc_download_and_get_dir(quickstart_link, \"Riva Quick Start resource folder\", resource_type=\"resource\", parent_dir=os.getcwd())\n",
    "RIVA_DIR = os.path.join(os.getcwd(), RIVA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we modify the `config.sh` file to enable relevant Riva services (n-gram language model), provide the encryption key, and path to the model repository (`riva_model_loc`) generated in the previous step among other configurations. \n",
    "\n",
    "For example, if above the model repository is generated at `$ASR_MODEL_DIR/custom-models/models`, then you can specify `riva_model_loc` as the same directory as `ASR_MODEL_DIR/custom-models`. <br>\n",
    "\n",
    "Pretrained versions of models specified in `models_asr/nlp/tts/nmt` are fetched from NGC. Since we are using our custom model, we can comment it in `models_asr` (and any others that are not relevant to your use case). <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### config.sh snippet\n",
    "```sh\n",
    "### config.sh snippet  \n",
    "# Enable or Disable Riva Services\n",
    "# For any language other than en-US: service_enabled_nlp must be set to false\n",
    "service_enabled_asr=true\n",
    "service_enabled_nlp=true          ## MAKE CHANGES HERE - SET TO FALSE\n",
    "service_enabled_tts=true          ## MAKE CHANGES HERE - SET TO FALSE\n",
    "service_enabled_nmt=true          ## MAKE CHANGES HERE - SET TO FALSE\n",
    "\n",
    "...\n",
    "\n",
    "# Specify the encryption key to use to deploy models\n",
    "MODEL_DEPLOY_KEY=\"tlt_encode\"     ## MAKE CHANGES HERE (Replace with the key you used when running nemo2riva)\n",
    "\n",
    "# Locations to use for storing models artifacts\n",
    "#\n",
    "# If an absolute path is speccified, the data will be written to that location\n",
    "# Otherwise, a Docker volume will be used (default).\n",
    "#\n",
    "# riva_init.sh will create a `rmir` and `models` directory in the volume or\n",
    "# path specified.\n",
    "#\n",
    "# RMIR ($riva_model_loc/rmir)\n",
    "# Riva uses an intermediate representation (RMIR) for models\n",
    "# that are ready to deploy but not yet fully optimized for deployment. Pretrained\n",
    "# versions can be obtained from NGC (by specifying NGC models below) and will be\n",
    "# downloaded to $riva_model_loc/rmir by `riva_init.sh`\n",
    "#\n",
    "# Custom models produced by NeMo or TLT and prepared using riva-build\n",
    "# may also be copied manually to this location $(riva_model_loc/rmir).\n",
    "#\n",
    "# Models ($riva_model_loc/models)\n",
    "# During the riva_init process, the RMIR files in $riva_model_loc/rmir\n",
    "# are inspected and optimized for deployment. The optimized versions are\n",
    "# stored in $riva_model_loc/models. The riva server exclusively uses these\n",
    "# optimized versions.\n",
    "riva_model_loc=\"riva-model-repo\"  ## MAKE CHANGES HERE (Replace with the path ASR_MODEL_DIR/custom-models)\n",
    "\n",
    "if [[ $riva_target_gpu_family == \"tegra\" ]]; then\n",
    "    riva_model_loc=\"`pwd`/model_repository\"\n",
    "fi\n",
    "\n",
    "# The default RMIRs are downloaded from NGC by default in the above $riva_rmir_loc directory\n",
    "# If you'd like to skip the download from NGC and use the existing RMIRs in the $riva_rmir_loc\n",
    "# then set the below $use_existing_rmirs flag to true. You can also deploy your set of custom\n",
    "# RMIRs by keeping them in the riva_rmir_loc dir and use this quickstart script with the\n",
    "# below flag to deploy them all together.\n",
    "use_existing_rmirs=false          ## MAKE CHANGES HERE - SET TO TRUE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to make the following changes to `config.sh` without opening the file in a text editor:\n",
    "\n",
    "1. Set NLP, NMT, and TTS services to `false`\n",
    "2. Set the `riva_model_loc` path to the path also assigned to `ASR_MODEL_DIR/custom-models`\n",
    "3. Set the variable `use_existing_rmirs` to `true`\n",
    "4. Change the `MODEL_DEPLOY_KEY` variable from the default `tlt_encode` to the key you used when exporting the customized acoustic model with `nemo2riva`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(f\"{RIVA_DIR}/config.sh\", \"r\") as config_in:\n",
    "    config_file = config_in.readlines()\n",
    "\n",
    "for i, line in enumerate(config_file):\n",
    "    # Disable services\n",
    "    if line.startswith(\"service_enabled_asr\"):\n",
    "        config_file[i] = \"service_enabled_asr=true\\n\"\n",
    "    elif line.startswith(\"service_enabled_nlp\"):\n",
    "        config_file[i] = \"service_enabled_nlp=false\\n\"\n",
    "    elif line.startswith(\"service_enabled_nmt\"):\n",
    "        config_file[i] = \"service_enabled_nmt=false\\n\"\n",
    "    elif line.startswith(\"service_enabled_tts\"):\n",
    "        config_file[i] = \"service_enabled_tts=false\\n\"\n",
    "    # Update riva_model_loc to our rmir folder\n",
    "    elif line.startswith(\"riva_model_loc\"):\n",
    "        config_file[i] = f'riva_model_loc=\"{ASR_MODEL_DIR}/custom-models\"\\n'\n",
    "    elif line.startswith(\"use_existing_rmirs\"):\n",
    "        config_file[i] = \"use_existing_rmirs=true\\n\"\n",
    "    elif line.startswith(\"MODEL_DEPLOY_KEY\"):\n",
    "        config_file[i] = f'MODEL_DEPLOY_KEY=\"{KEY}\"\\n'\n",
    "\n",
    "with open(f\"{RIVA_DIR}/config.sh\", \"w\") as config_in:\n",
    "    config_in.writelines(config_file)\n",
    "\n",
    "print(\"\".join(config_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have permission to execute these scripts\n",
    "! cd $RIVA_DIR && chmod +x ./riva_init.sh && chmod +x ./riva_start.sh && chmod +x ./riva_stop.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, one runs `riva_init.sh` before `riva_start.sh`. However, since we've already built our `.rmir` file with `riva-build` and deployed the associated model files by running `riva-deploy`, we can skip straight to `riva_start.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Riva Start. This will deploy your model.\n",
    "! cd $RIVA_DIR && ./riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Inference\n",
    "After the Riva server is up and running with your models, you can send inference requests querying the server. \n",
    "\n",
    "To send gRPC requests, you can install the Riva Python API bindings for the client. This is available as a [Python module on PyPI](https://pypi.org/project/nvidia-riva-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Client API Bindings\n",
    "! pip install nvidia-riva-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import riva.client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the Riva Server and Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling this inference function queries the Riva server (using gRPC) to transcribe an audio file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(audio_file, server='localhost:50051', print_full_response=False):\n",
    "    with open(audio_file, 'rb') as fh:\n",
    "        data = fh.read()\n",
    "\n",
    "    auth = riva.client.Auth(uri=server)\n",
    "    client = riva.client.ASRService(auth)\n",
    "    config = riva.client.RecognitionConfig(\n",
    "        language_code=\"en-US\",\n",
    "        max_alternatives=1,\n",
    "        enable_automatic_punctuation=False,\n",
    "    )\n",
    "\n",
    "    response = client.offline_recognize(data, config)\n",
    "    if print_full_response: \n",
    "        print(response)\n",
    "    else:\n",
    "        print(response.results[0].alternatives[0].transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can actually query the Riva server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"audio_samples/en-US_wordboosting_sample2.wav\"\n",
    "run_inference(audio_file, print_full_response=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = riva.client.Auth(uri='localhost:50051')\n",
    "\n",
    "riva_asr = riva.client.ASRService(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample audio file from local disk\n",
    "# This example uses a .wav file with LINEAR_PCM encoding.\n",
    "audio_file = \"audio_samples/en-US_wordboosting_sample2.wav\"\n",
    "    \n",
    "# Listen to the sample audio we are looking to transcribe\n",
    "ipd.Audio(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = wave.open(audio_file, 'rb')\n",
    "with open(audio_file, 'rb') as fh:\n",
    "    content = fh.read()\n",
    "\n",
    "# Creating RecognitionConfig\n",
    "config = riva.client.RecognitionConfig(\n",
    "  language_code=\"en-US\",\n",
    "  max_alternatives=1,\n",
    "  enable_automatic_punctuation=True,\n",
    "  audio_channel_count = 1\n",
    ")\n",
    "\n",
    "# ASR Inference call with Recognize \n",
    "response = riva_asr.offline_recognize(content, config)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can stop the Riva ServiceMaker container (and thus shut down the Riva server) before shutting down the Jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker container stop riva-speech"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba-riva-tutorials",
   "language": "python",
   "name": "mamba-riva-tutorials"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
