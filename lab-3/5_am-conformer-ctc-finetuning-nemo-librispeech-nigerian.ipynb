{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NpvEOMs_mKp"
   },
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/rivaasrasr-finetuning-conformer-ctc-nemo/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to Fine-Tune a Riva ASR Acoustic Model with NVIDIA NeMo\n",
    "This tutorial walks you through how to fine-tune an NVIDIA Riva ASR acoustic model with NVIDIA NeMo.\n",
    "\n",
    "**Important**: If you plan to fine-tune an ASR acoustic model using the same tokenizer with which the model was trained, skip this tutorial and refer to the \"Sub-word Encoding CTC Model\" section (starting with the \"Load pre-trained model\" subsection) of the [NeMo ASR Language Finetuning tutorial](https://github.com/NVIDIA/NeMo/blob/main/tutorials/asr/ASR_CTC_Language_Finetuning.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6h7SXF6G_mKw"
   },
   "source": [
    "## NVIDIA Riva Overview\n",
    "\n",
    "NVIDIA Riva is a GPU-accelerated SDK for building speech AI applications that are customized for your use case and deliver real-time performance. <br/>\n",
    "Riva offers a rich set of speech and natural language understanding (NLU) services such as:\n",
    "\n",
    "- Automated speech recognition (ASR). \n",
    "- Text-to-Speech synthesis (TTS). \n",
    "- A collection of natural language processing (NLP) services, such as named entity recognition (NER), punctuation, and intent classification.\n",
    "\n",
    "In this tutorial, we will fine-tune a Riva ASR acoustic model with NeMo. <br> \n",
    "To understand the basics of Riva ASR APIs, refer to [Getting started with Riva ASR in Python](https://github.com/nvidia-riva/tutorials/blob/stable/asr-python-basics.ipynb). <br>\n",
    "\n",
    "For more information about Riva, refer to the [Riva developer documentation](https://developer.nvidia.com/riva)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rv47EmBp_mK2"
   },
   "source": [
    "## NeMo (Neural Modules)\n",
    "[NVIDIA NeMo](https://developer.nvidia.com/nvidia-nemo) is an open-source framework for building, training, and fine-tuning GPU-accelerated speech AI and NLU models with a simple Python interface. For information about how to set up NeMo, refer to the [NeMo GitHub](https://github.com/NVIDIA/NeMo) instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1O2bGCMAQDs8",
    "outputId": "a8731f17-b61c-4457-ae39-7079b8c4edd3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can run either this tutorial locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Perform the following steps to setup in Google Colab:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub.\n",
    "   a. Click **File** > **Upload Notebook** > **GITHUB** tab > copy/paste the GitHub URL.\n",
    "3. Connect to an instance with a GPU.\n",
    "   a. Click **Runtime** > Change the runtime type > select **GPU** for the hardware accelerator.\n",
    "4. Run this cell to set up the dependencies.\n",
    "5. Restart the runtime.\n",
    "   a. Click **Runtime** > **Restart Runtime** for any upgraded packages to take effect.\n",
    "\"\"\"\n",
    "\n",
    "# Install Dependencies\n",
    "!pip install wget\n",
    "!apt-get install sox libsndfile1 ffmpeg libsox-fmt-mp3\n",
    "!pip install text-unidecode\n",
    "!pip install matplotlib>=3.3.2\n",
    "!pip install Cython\n",
    "\n",
    "## Install NeMo\n",
    "BRANCH = 'main'\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n",
    "\n",
    "\"\"\"\n",
    "Remember to restart the runtime for the kernel to pick up any upgraded packages (e.g. matplotlib)!\n",
    "Alternatively, in the case where you want to use the \"Run All Cells\" (or similar) option, \n",
    "uncomment `exit()` below to crash and restart the kernel.\n",
    "\"\"\"\n",
    "# exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BRANCH = 'main'\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vo27Unex_mLG"
   },
   "source": [
    "---\n",
    "## Fine-Tuning an ASR model with NeMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='isc-prepare-data'></a>\n",
    "### Preparing the Dataset\n",
    "#### LibriSpeech ASR train-clean-100 Dataset\n",
    "For this tutorial, we use the clean, 100-hour version of the LibriSpeech ASR training dataset to train our Conformer-CTC acoustic model, and the clean development split to validate the model. The LibriSpeech ASR dataset is available [here](https://www.openslr.org/12/).\n",
    "\n",
    "#### Crowdsourced High-Quality Nigerian English Speech Dataset\n",
    "For this tutorial, we also use the Nigerian English speech dataset to evaluate and fine-tune our Conformer-CTC acoustic model. The Nigerian English speech dataset is available [here](https://www.openslr.org/70/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Downloading and Preprocessing the Datasets\n",
    "#### LibriSpeech ASR Dataset\n",
    "The `train-clean-100` split of the LibriSpeech ASR dataset, which we'll use as the training set, is publicly available [here](https://www.openslr.org/resources/12/train-clean-100.tar.gz) and can be downloaded directly. The `dev-clean` split of the LibriSpeech ASR dataset, which we'll use as the validation set, is publicly available [here](https://www.openslr.org/resources/12/dev-clean.tar.gz) and can also be downloaded directly. We've provided a script that downloads the splits for you. The preprocessing step entails converting the audio files from their native `.flac` format to `.wav` and generating a manifest file containing metadata for each audio file, both of which TAO Toolkit needs to train the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install modules that the downloading and preprocessing script requires which aren't part of the Python standard library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo apt install -y sox\n",
    "! pip install sox\n",
    "! pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some environmental variables for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MODEL_DIR = os.path.abspath(\"asr-models\")\n",
    "DATA_DIR  = os.path.abspath(\"asr-models/datasets\")\n",
    "os.environ[\"MODEL_DIR\"] = MODEL_DIR\n",
    "os.environ[\"DATA_DIR\"]  = DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ./get_librispeech_data.py --data_root=$DATA_DIR --data_sets='train_clean_100,dev_clean'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the `.tar.gz` archive files to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm $DATA_DIR/train_clean_100.tar.gz\n",
    "! rm $DATA_DIR/dev_clean.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's listen to a sample audio file from the preprocessed training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change path of the file here\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "path = os.path.join(DATA_DIR, 'LibriSpeech/train-clean-100-processed/163-121908-0000.wav')\n",
    "ipd.Audio(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crowdsourced High-Quality Nigerian English Speech Dataset\n",
    "The evaluation/fine-tuning data is publicly available in several files [here](https://www.openslr.org/resources/70/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the audio data\n",
    "!wget 'https://www.openslr.org/resources/70/en_ng_female.zip' -P $DATA_DIR\n",
    "!wget 'https://www.openslr.org/resources/70/en_ng_male.zip'   -P $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the evaluation/finetuning data\n",
    "# Ensure that the unzip utility is available. If not, install it.\n",
    "!unzip -nq $DATA_DIR/en_ng_female.zip -d $DATA_DIR/en_ng_female\n",
    "!mv $DATA_DIR/en_ng_female/line_index.tsv $DATA_DIR/en_ng_female/line_index_female.tsv\n",
    "!unzip -nq $DATA_DIR/en_ng_male.zip -d $DATA_DIR/en_ng_male\n",
    "!mv $DATA_DIR/en_ng_male/line_index.tsv $DATA_DIR/en_ng_male/line_index_male.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the `.zip` files to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm $DATA_DIR/en_ng_*.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to extract the relevant information from the `.tsv` metadata files included with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def process_en_ng_tsvs(data_dir):\n",
    "    genders = ['female','male']\n",
    "    entries = []\n",
    "    # Extract the relevant information from the tsv files\n",
    "    for gender in genders: \n",
    "        dataset  = f'en_ng_{gender}'\n",
    "        tsv_name = f'line_index_{gender}.tsv'\n",
    "        tsv_file = os.path.join(data_dir, dataset, tsv_name)\n",
    "        with open(tsv_file, encoding='utf-8') as fin:\n",
    "            for line in fin:\n",
    "                label, text = line[: line.index(\"\\t\")], line[line.index(\"\\t\") + 1 :]\n",
    "                speaker_id  = label.split('_')[1]\n",
    "                host_wav_file = os.path.join(data_dir, dataset, label + '.wav')\n",
    "                wav_file = os.path.join(data_dir, dataset, label + '.wav')\n",
    "                transcript_text = text.lower().strip()\n",
    "\n",
    "                # check duration\n",
    "                duration = subprocess.check_output(\"soxi -D {0}\".format(host_wav_file), shell=True)\n",
    "\n",
    "                entry = {}\n",
    "                entry['audio_filepath'] = wav_file\n",
    "                entry['duration'] = float(duration)\n",
    "                entry['text'] = transcript_text\n",
    "                entry['gender'] = gender\n",
    "                entry['speaker_id'] = speaker_id\n",
    "                entries.append(entry)\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to generate `*manifest.json` metadata files from the `.tsv` metadata files included with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def generate_en_ng_manifest(data_dir, random_seed=0, val_split=0.1, test_split=0.1):\n",
    "    # Extract the relevant information from the tsv files\n",
    "    entries = process_en_ng_tsvs(data_dir)\n",
    "    # Generate the manifest files\n",
    "    # Set the random seed for reproducibility\n",
    "    random.seed(random_seed)\n",
    "    random.shuffle(entries)\n",
    "    num_val_entries  = int(val_split  * len(entries))\n",
    "    num_test_entries = int(test_split * len(entries))\n",
    "    ft_manifest_file   = os.path.join(data_dir, 'en_ng_ft_manifest.json')\n",
    "    val_manifest_file  = os.path.join(data_dir, 'en_ng_val_manifest.json')\n",
    "    test_manifest_file = os.path.join(data_dir, 'en_ng_test_manifest.json')\n",
    "    with open(ft_manifest_file, 'w') as fout:\n",
    "        for m in entries[:-(num_val_entries+num_test_entries)]:\n",
    "            fout.write(json.dumps(m) + '\\n')\n",
    "    with open(val_manifest_file, 'w') as fout:\n",
    "        for m in entries[-(num_val_entries+num_test_entries):-num_test_entries]:\n",
    "            fout.write(json.dumps(m) + '\\n')\n",
    "    with open(test_manifest_file, 'w') as fout:\n",
    "        for m in entries[-num_test_entries:]:\n",
    "            fout.write(json.dumps(m) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the manifest files for the Nigerian English Speech dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_en_ng_manifest(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's listen to an audio file from the Nigerian English dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change path of the file here\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "path = os.path.join(DATA_DIR, 'en_ng_male/ngm_02436_00539200207.wav')\n",
    "ipd.Audio(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FubCSAin_mMC"
   },
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjHrIfDj_mMD"
   },
   "source": [
    "#### Create Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zBKAO36_mME"
   },
   "source": [
    "Before we can do the actual training, we need to create a tokenizer as this ASR model uses word-piece encoding. Character based models don't need the tokenizer creation as only single characters are regarded as elements in the vocabulary in their cases. We can use NeMo's `process_asr_text_tokenizer.py` script to create the tokenizer that generates the subword vocabulary for us for use in training. The size of the vocabulary (`vocab_size`) should be the same as the vocabulary size in the ASR model. We will clone the NeMo GitHub repository to use the scripts and examples available there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "maoasPkL_mMF",
    "outputId": "f468df62-ce9f-4828-ef6a-06e890de8b0e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clone NeMo locally\n",
    "# Change this path if you don't want to clone NeMo to the directory containing this tutorial\n",
    "# NEMO_DIR = os.path.join(os.getcwd(), \"NeMo\")\n",
    "NEMO_DIR = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), \"NeMo\")\n",
    "! git clone https://github.com/NVIDIA/NeMo $NEMO_DIR\n",
    "\n",
    "# create the tokenizer\n",
    "!python $NEMO_DIR/scripts/tokenizers/process_asr_text_tokenizer.py \\\n",
    "         --manifest=$DATA_DIR/en_ng_ft_manifest.json \\\n",
    "         --data_root=$DATA_DIR \\\n",
    "         --vocab_size=128 \\\n",
    "         --tokenizer=spe \\\n",
    "         --spe_type=unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-yDMTMO_mMJ"
   },
   "source": [
    "#### Training Conformer-CTC\n",
    "\n",
    "NeMo uses `.yml` files to configure the training parameters. You may update them directly by editing the configuration file or from the command-line interface. For example, if the number of epochs needs to be modified, along with a change in the learning rate, you can add `trainer.max_epochs=100` and `optim.lr=0.02` and train the model. \n",
    "\n",
    "The following sample command uses the `speech_to_text_ctc_bpe.py` script in the `examples` folder to train/fine-tune a Conformer-CTC ASR model for 1 epoch. For other ASR models like Citrinet, you may find the appropriate config files in the NeMo GitHub repo under [examples/asr/conf/](https://github.com/NVIDIA/NeMo/tree/main/examples/asr/conf).\n",
    "\n",
    "To fully train the model from scratch, you'll need to increase `trainer.max_epochs` from 1. Empirical evidence suggests that around 200 epochs should suffice. Fine-tuning a pre-trained model will likewise typically require more than 1 epoch. \n",
    "\n",
    "By default, `speech_to_text_ctc_bpe.py` trains an ASR acoustic model from scratch. \n",
    "\n",
    "To fine-tune a pretrained model, add the parameter `+init_from_pretrained_model=<model_name>`. Refer to [this table](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/results.html#speech-recognition-languages) in the NeMo documentation for a list of pretrained speech recognition model checkpoints. \n",
    "\n",
    "To continue fine-tuning a local model retroactively, add the parameter `+init_from_nemo_model=<path/to/model_name.nemo>`. \n",
    "\n",
    "To restrict NeMo to a particular GPU, place square brackets around the number passed into `trainer.devices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-bkkgxxv_mMK",
    "outputId": "9aeca99e-c3e0-4784-97f4-13cf997c2247",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE TO SELF: Remove trainer.val_check_interval (?)\n",
    "# ANOTHER NOTE TO SELF: Reset trainer.max_epochs to 1\n",
    "!python $NEMO_DIR/examples/asr/asr_ctc/speech_to_text_ctc_bpe.py \\\n",
    "    --config-path=../conf/conformer/ --config-name=conformer_ctc_bpe \\\n",
    "    +init_from_pretrained_model=stt_en_conformer_ctc_large \\\n",
    "    model.train_ds.manifest_filepath=$DATA_DIR/en_ng_ft_manifest.json \\\n",
    "    model.validation_ds.manifest_filepath=$DATA_DIR/en_ng_val_manifest.json \\\n",
    "    model.tokenizer.dir=$DATA_DIR/tokenizer_spe_unigram_v128 \\\n",
    "    model.train_ds.batch_size=4 \\\n",
    "    model.validation_ds.batch_size=4 \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.max_epochs=10 \\\n",
    "    trainer.val_check_interval=0.1 \\\n",
    "    model.optim.name=\"adamw\" \\\n",
    "    model.optim.lr=1.0 \\\n",
    "    model.optim.weight_decay=0.001 \\\n",
    "    model.optim.sched.warmup_steps=2000 \\\n",
    "    ++exp_manager.exp_dir=$MODEL_DIR/checkpoints \\\n",
    "    ++exp_manager.version=en_ng \\\n",
    "    ++exp_manager.use_datetime_version=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhGF0z_-rl8o",
    "outputId": "060b0b4c-224d-4242-bdef-72e0e346e137"
   },
   "outputs": [],
   "source": [
    "!ls $MODEL_DIR/checkpoints/Conformer-CTC-BPE/en_ng/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemo_file_path = os.path.join(MODEL_DIR, 'checkpoints/Conformer-CTC-BPE/en_ng/checkpoints/Conformer-CTC-BPE.nemo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKDipfZF_mMK"
   },
   "source": [
    "### ASR Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3svjWpR_mMN"
   },
   "source": [
    "Now that we have a model trained, we need to check how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls $NEMO_DIR/examples/asr/speech_to_text_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls $DATA_DIR/en_ng_test_manifest_predictions.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D4ATNCSe_mMN",
    "outputId": "b9039c6c-8840-4ce2-9089-689c29a8a162",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python $NEMO_DIR/examples/asr/speech_to_text_eval.py \\\n",
    "    model_path=$nemo_file_path \\\n",
    "    dataset_manifest=$DATA_DIR/en_ng_test_manifest.json \\\n",
    "    output_filename=$DATA_DIR/en_ng_test_manifest_predictions.json \\\n",
    "    batch_size=4 \\\n",
    "    amp=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9ygVAEC_mMS"
   },
   "source": [
    "### ASR Model Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ap5CjiV4_mMT"
   },
   "source": [
    "With NeMo, you can also export your model in a format that can be deployed using NVIDIA Riva: a highly performant application framework for multi-modal conversational AI services using GPUs. The same command for exporting to ONNX can be used here. The only small variation is the configuration for `export_format` in the spec file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myI104s2pY13"
   },
   "source": [
    "#### Install `nemo2riva` with `pip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pd0Y3Gys_mMU",
    "outputId": "31cb1a3a-e539-4d5a-e395-862525f3cb99"
   },
   "outputs": [],
   "source": [
    "!pip install nemo2riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XNolXIPptAr"
   },
   "source": [
    "#### Convert to Riva\n",
    "\n",
    "Convert the downloaded model to the `.riva` format. We will set the encryption key with `--key=tlt_encode`. Choose a different encryption key value when generating `.riva` models for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PlUNmVHKp1ft",
    "outputId": "f612c5e8-604e-4eee-8806-4eb463d1e810",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nemo_path_list = nemo_file_path.split('/')\n",
    "nemo_file_name = nemo_path_list[-1]\n",
    "riva_file_name = nemo_file_name[:-5] + \".riva\"\n",
    "riva_file_path = os.path.join(MODEL_DIR, \"custom-models\", \"riva\", riva_file_name)\n",
    "\n",
    "!mkdir -p $MODEL_DIR/custom-models/riva\n",
    "\n",
    "!nemo2riva --out $riva_file_path --key=tlt_encode --onnx-opset 18 $nemo_file_path "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAtoNe9krC2z"
   },
   "source": [
    "## More Resources\n",
    "You can find more information about working with NeMo's ASR models in the [ASR section](https://github.com/NVIDIA/NeMo/tree/main/tutorials/asr) of the NeMo tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lv7ZRPoc_mMa"
   },
   "source": [
    "## What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJ7hWpRT_mMb"
   },
   "source": [
    "You can use NeMo to build custom models for your own applications, and deploy them with NVIDIA Riva! Refer to the [Conformer-CTC deployment tutorial](https://github.com/nvidia-riva/tutorials/blob/main/asr-deployment-conformer-ctc.ipynb)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "mamba-riva-tutorials",
   "language": "python",
   "name": "mamba-riva-tutorials"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
