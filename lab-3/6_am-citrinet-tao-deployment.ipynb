{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_asr_asr-python-advanced-finetune-am-citrinet-tao-deployment/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to deploy custom Acoustic Model (Citrinet) trained with TAO Toolkit on Riva\n",
    "This tutorial walks you through the deployment of custom acoustic model (Citrinet) trained with TAO Toolkit on Riva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Riva ServiceMaker\n",
    "Riva ServiceMaker is a set of tools that aggregates all the necessary artifacts (models, files, configurations, and user settings) for Riva deployment to a target environment. It has two main components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riva-build\n",
    "\n",
    "This step helps build a Riva-ready version of the model. Itâ€™s only output is an intermediate format (called an RMIR) of an end-to-end pipeline for the supported services within Riva. Let's consider an ASR Citrinet model. <br>\n",
    "\n",
    "We'll use the customized acoustic model (from the previous notebook) to deploy the Riva ASR pipeline. In addition, we'll use the pre-trained language model and the inverse text normalization model that we downloaded from NGC in the first notebook.\n",
    "\n",
    "Let's set the path to the customized acoustic model (.riva), language model (.binary) and the ITN model (.far) which will be used during `riva build`.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: UPDATE AM_MODEL_LOC with `asr-model.riva` ABSOLUTE PATH\n",
    "# IMPORTANT: UPDATE LM_MODEL_LOC with `exported-model.binary` ABSOLUTE PATH\n",
    " \n",
    "import os\n",
    "# ServiceMaker Docker\n",
    "RIVA_SM_CONTAINER = \"nvcr.io/nvidia/riva/riva-speech:2.2.1-servicemaker\"\n",
    "\n",
    "# Directory where the asr-model.riva is stored $MODEL_LOC/*.riva\n",
    "AM_WORKING_DIR = os.path.join(os.getcwd(), \"asr_am_finetuning\")\n",
    "AM_MODEL_LOC = AM_WORKING_DIR + \"/results/citrinet/riva/\"\n",
    "\n",
    "# Directory where the LM and ITN model is stored\n",
    "LM_ITN_WORKING_DIR = os.path.join(os.getcwd(), \"asr-models\")\n",
    "LM_MODEL_LOC = LM_ITN_WORKING_DIR + \"/speechtotext_en_us_lm_vdeployable_v1.1\"\n",
    "ITN_MODEL_LOC = LM_ITN_WORKING_DIR + \"/inverse_normalization_en_us_vdeployable_v1.0\"\n",
    "\n",
    "# Name of the model files\n",
    "AM_MODEL_NAME = \"asr-model.riva\"\n",
    "LM_MODEL_NAME = \"riva_asr_train_datasets_3gram.binary\"\n",
    "ITN_TOKENIZER_MODEL_NAME = \"tokenize_and_classify.far\"\n",
    "ITN_VERBALIZER_MODEL_NAME = \"verbalize.far\"\n",
    "VOCAB_FILE = \"flashlight_decoder_vocab.txt\"\n",
    "\n",
    "# Key that model is encrypted with, while exporting with TAO\n",
    "KEY = \"tlt_encode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker run --rm --gpus 0 -v $AM_MODEL_LOC:/data_am -v $LM_MODEL_LOC:/data_lm \\\n",
    "            -v $ITN_MODEL_LOC:/data_itn $RIVA_SM_CONTAINER -- \\\n",
    "            riva-build speech_recognition /data_am/asr.rmir:$KEY /data_am/$AM_MODEL_NAME:$KEY --offline \\\n",
    "            --streaming=False \\\n",
    "            --wfst_tokenizer_model=/data_itn/$ITN_TOKENIZER_MODEL_NAME \\\n",
    "            --wfst_verbalizer_model=/data_itn/$ITN_VERBALIZER_MODEL_NAME \\\n",
    "            --ms_per_timestep=80 \\\n",
    "            --featurizer.use_utterance_norm_params=False \\\n",
    "            --featurizer.precalc_norm_time_steps=0 \\\n",
    "            --featurizer.precalc_norm_params=False \\\n",
    "            --vad.residue_blanks_at_start=-2 \\\n",
    "            --chunk_size=300 \\\n",
    "            --left_padding_size=0. \\\n",
    "            --right_padding_size=0. \\\n",
    "            --decoder_type=flashlight \\\n",
    "            --flashlight_decoder.asr_model_delay=-1 \\\n",
    "            --decoding_language_model_binary=/data_lm/$LM_MODEL_NAME \\\n",
    "            --decoding_vocab=/data_lm/$VOCAB_FILE \\\n",
    "            --flashlight_decoder.lm_weight=0.2 \\\n",
    "            --flashlight_decoder.word_insertion_score=0.2 \\\n",
    "            --flashlight_decoder.beam_threshold=20. \\\n",
    "            --language_code=en-US\n",
    "            --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riva-deploy\n",
    "\n",
    "The deployment tool takes as input one or more Riva Model Intermediate Representation (RMIR) files and a target model repository directory. It creates an ensemble configuration specifying the pipeline for the execution and finally writes all those assets to the output model repository directory.\n",
    "\n",
    "Be patient! This step could take 10-15 minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Syntax: riva-deploy -f dir-for-rmir/model.rmir:key output-dir-for-repository\n",
    "! docker run --rm --gpus 0 -v $AM_MODEL_LOC:/data $RIVA_SM_CONTAINER -- \\\n",
    "            riva-deploy -f  /data/asr.rmir:$KEY /data/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Start the Riva Server\n",
    "After the model repository is generated, we are ready to start the Riva server. First, download the Riva Quick Start resource from NGC. \n",
    "Set the path to the directory here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Riva Quick Start directory\n",
    "RIVA_DIR = os.path.join(os.getcwd(), \"riva_quickstart_v2.4.0\")\n",
    "\n",
    "# Checking if the quickstart exists, otherwise download it\n",
    "if os.path.exists(RIVA_DIR):\n",
    "    print(\"Quickstart scripts exists, skipping download\")\n",
    "else:\n",
    "    print(\"Quickstart scripts does not exist, downloading\")\n",
    "    ! ngc registry resource download-version \"nvidia/riva/riva_quickstart:2.4.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we modify the `config.sh` file to enable relevant Riva services (ASR for the Citrinet model), provide the encryption key, and path to the model repository (`riva_model_loc`) generated in the previous step among other configurations. \n",
    "\n",
    "For example, if above the model repository is generated at `$MODEL_LOC/models`, then you can specify `riva_model_loc` as the same directory as `MODEL_LOC`. <br>\n",
    "\n",
    "Pretrained versions of models specified in `models_asr/nlp/tts` are fetched from NGC. Since we are using our custom model, we can comment it in `models_asr` (and any others that are not relevant to your use case). <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### config.sh snippet\n",
    "```\n",
    "# Enable or Disable Riva Services \n",
    "service_enabled_asr=true                                                      ## MAKE CHANGES HERE\n",
    "service_enabled_nlp=false                                                      ## MAKE CHANGES HERE\n",
    "service_enabled_tts=false                                                     ## MAKE CHANGES HERE\n",
    "\n",
    "# Specify one or more GPUs to use\n",
    "# specifying more than one GPU is currently an experimental feature, and may result in undefined behaviours.\n",
    "gpus_to_use=\"device=0\"\n",
    "\n",
    "# Specify the encryption key to use to deploy models\n",
    "MODEL_DEPLOY_KEY=\"tlt_encode\"\n",
    "\n",
    "# Locations to use for storing models artifacts\n",
    "#\n",
    "# If an absolute path is specified, the data will be written to that location\n",
    "# Otherwise, a docker volume will be used (default).\n",
    "#\n",
    "# riva_init.sh will create a `rmir` and `models` directory in the volume or\n",
    "# path specified. \n",
    "#\n",
    "# RMIR ($riva_model_loc/rmir)\n",
    "# Riva uses an intermediate representation (RMIR) for models\n",
    "# that are ready to deploy but not yet fully optimized for deployment. Pretrained\n",
    "# versions can be obtained from NGC (by specifying NGC models below) and will be\n",
    "# downloaded to $riva_model_loc/rmir by `riva_init.sh`\n",
    "# \n",
    "# Custom models produced by NeMo or TAO and prepared using riva-build\n",
    "# may also be copied manually to this location $(riva_model_loc/rmir).\n",
    "#\n",
    "# Models ($riva_model_loc/models)\n",
    "# During the riva_init process, the RMIR files in $riva_model_loc/rmir\n",
    "# are inspected and optimized for deployment. The optimized versions are\n",
    "# stored in $riva_model_loc/models. The riva server exclusively uses these\n",
    "# optimized versions.\n",
    "riva_model_loc=\"<add path>\"                              ## MAKE CHANGES HERE (Replace with MODEL_LOC)                      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure to do the following before moving forward:**\n",
    "1. In the file navigator in Jupyter Lab, navigate to riva_quickstart_v2.* and open config.sh\n",
    "2. Configure settings as shown in the snippet above\n",
    "   - Set nlp and tts services to false\n",
    "   - Configure the riva_model_loc path to where the models resulting from riva-deploy are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set `riva-model-loc` to where the models resulting from riva-deploy are stored. In our case it is AM_MODEL_LOC\n",
    "!echo $AM_MODEL_LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have permission to execute these scripts\n",
    "! cd $RIVA_DIR && chmod +x ./riva_stop.sh && chmod +x ./riva_start.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop existing Riva deployments. \n",
    "! cd $RIVA_DIR && ./riva_stop.sh config.sh \n",
    "# Run Riva Start. This will deploy your model(s).\n",
    "! cd $RIVA_DIR && ./riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Inference\n",
    "Once the Riva server is up-and-running with your models, you can send inference requests querying the server. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Connect to the Riva Server and Run Inference\n",
    "Now we can actually query the Riva server. The following cell queries the Riva server (using gRPC) to yield a result.list_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import grpc\n",
    "import time\n",
    "try:\n",
    "    import riva.client # RIVA 2.3.0 and above\n",
    "except:\n",
    "    import riva_api.riva_audio_pb2 as ra # RIVA 2.0.0 and above\n",
    "    import riva_api.audio_pb2 as ra\n",
    "    import riva_api.riva_asr_pb2 as rasr\n",
    "    import riva_api.riva_asr_pb2_grpc as rasr_srv\n",
    "import wave\n",
    "\n",
    "audio_file = \"asr_am_finetuning/data/en_ng_male/ngm_09697_00751039644.wav\"\n",
    "\n",
    "wf = wave.open(audio_file, 'rb')\n",
    "with open(audio_file, 'rb') as fh:\n",
    "    data = fh.read()\n",
    "\n",
    "server = \"localhost:50051\"\n",
    "auth = riva.client.Auth(uri=server)\n",
    "riva_asr = riva.client.ASRService(auth)\n",
    "\n",
    "# Creating RecognitionConfig\n",
    "config = riva.client.RecognitionConfig(\n",
    "  language_code=\"en-US\",\n",
    "  max_alternatives=1,\n",
    "  enable_automatic_punctuation=True,\n",
    "  audio_channel_count = 1\n",
    ")\n",
    "\n",
    "response = riva_asr.offline_recognize(data, config)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can stop all Docker containers before shutting down the Jupyter kernel. **Caution: The following command will stop all running containers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker stop $(docker ps -a -q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
