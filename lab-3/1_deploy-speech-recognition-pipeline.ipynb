{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_asr_asr-python-advanced-finetune-am-citrinet-tao-deployment/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to deploy a Riva Speech Recognition Pipeline\n",
    "In this tutorial, you will learn how to deploy Riva speech recognition models - specifically the **Acoustic model (Conformer-CTC)**, **Language model (ngram)**, and **Inverse Text Normalization (WSFT)** pre-trained models downloaded from NVIDIA NGC. \n",
    "\n",
    "This will serve as a primer for customization tutorials in this lab, which require configuring the Riva speech pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, ensure that you have access to [**NVIDIA NGC**](https://ngc.nvidia.com/signin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Fetch ASR models from NGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "\n",
    "# Create a local directory to save models\n",
    "ASR_MODEL_DIR = os.path.join(os.getcwd(), \"asr-models\")\n",
    "!mkdir -p $ASR_MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function for downloading NGC resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngc_download_and_get_dir(ngc_resource_name, resource_description, resource_type=\"model\", parent_dir=ASR_MODEL_DIR):\n",
    "    default_download_folder = \"_v\".join(ngc_resource_name.split(\"/\")[-1].split(\":\"))\n",
    "    download_path = os.path.join(parent_dir, default_download_folder)\n",
    "    if os.path.exists(download_path):\n",
    "        print(f\"{resource_description} exists, skipping download\")\n",
    "        return default_download_folder\n",
    "    ngc_output = !ngc registry $resource_type download-version $ngc_resource_name --dest $parent_dir\n",
    "    if not os.path.exists(download_path):\n",
    "        ngc_output_formatted='\\n'.join(ngc_output)\n",
    "        logging.error(\n",
    "            f\"NGC was not able to download the requested model {ngc_resource_name}. \"\n",
    "            \"Please check the NGC error message, remove all directories, and re-start the \"\n",
    "            f\"notebook. NGC message: {ngc_output_formatted}\"\n",
    "        )\n",
    "        return None\n",
    "    print(f\"Successfully downloaded {resource_description}\")\n",
    "    return default_download_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Conformer-CTC Acoustic Model\n",
    "The Conformer-CTC Acoustic Model is located on NGC [here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/speechtotext_en_us_conformer). Let's download it to a local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where ngc will download the Acoustic Model\n",
    "AM_DIR = ngc_download_and_get_dir(\"nvidia/riva/speechtotext_en_us_conformer:deployable_v6.0_export_v2\", \"Acoustic model\")\n",
    "AM_PATH = os.path.join(ASR_MODEL_DIR, AM_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect downloaded files\n",
    "!ls -lt $AM_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download the n-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The n-gram LM is located on NGC [here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/speechtotext_en_us_lm). \n",
    "\n",
    "`NOTE:` This may take up to 30 minutes to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_DIR = ngc_download_and_get_dir(\"nvidia/riva/speechtotext_en_us_lm:deployable_v6.0\", \"Language model\")\n",
    "LM_PATH = os.path.join(ASR_MODEL_DIR, LM_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect downloaded files\n",
    "!ls -lt $LM_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download Inverse Text Normalization (ITN) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ITN model is located on NGC [here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/inverse_normalization_en_us)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITN_DIR = ngc_download_and_get_dir(\"nvidia/riva/inverse_normalization_en_us:deployable_v2.2\", \"Inverse text normalization model\")\n",
    "ITN_PATH = os.path.join(ASR_MODEL_DIR, ITN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect downloaded files\n",
    "!ls -lt $ITN_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Riva ServiceMaker\n",
    "Riva ServiceMaker is a set of tools that aggregates all the necessary artifacts (models, files, configurations, and user settings) for Riva deployment to a target environment. It has two main components: `riva-build` and `riva-deploy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Riva-build\n",
    "\n",
    "This step helps build a Riva-ready version of the model. Itâ€™s only output is an intermediate format (called an RMIR) of an end-to-end pipeline for the supported services within Riva. <br>\n",
    "\n",
    "`riva-build` is responsible for the combination of one or more exported models (`.riva` files) into a single file containing an intermediate format called Riva Model Intermediate Representation (`.rmir`). This file contains a deployment-agnostic specification of the whole end-to-end pipeline along with all the assets required for the final deployment and inference. For more information, refer to the [documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-customizing.html#pipeline-configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riva_line_list = !wget -qO- https://docs.nvidia.com/deeplearning/riva/user-guide/docs/index.html | grep \"NVIDIA Riva Skills\"\n",
    "riva_line_string = riva_line_list[0]\n",
    "__riva_version__ = riva_line_string.split(' ')[3]\n",
    "# __riva_version__ = '2.14.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_TYPE=\"AMD64\" #Change this to `ARM64_linux` or `ARM64_l4t` in case of an ARM64 machine.\n",
    "TARGET_MACHINE=\"AMD64\" #Change this to `ARM64_linux` or `ARM64_l4t` in case of an ARM64 machine.\n",
    "# KEY = \"nemotoriva\" ##Encryption key used during nemo2riva # tlt_encode for the standard FastPitch and HiFiGAN RMIRs\n",
    "KEY = \"tlt_encode\" ##Encryption key used during nemo2riva # tlt_encode for the standard FastPitch and HiFiGAN RMIRs\n",
    "FORCE = True ## Whether to force-build a new TTS RMIR and replace any existing RMIRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Riva NGC, servicemaker image config.\n",
    "if MACHINE_TYPE.lower() in [\"amd64\", \"arm64_linux\"]:\n",
    "    RIVA_SM_CONTAINER = f\"nvcr.io/nvidia/riva/riva-speech:{__riva_version__}-servicemaker\"\n",
    "elif MACHINE_TYPE.lower()==\"arm64_l4t\":\n",
    "    RIVA_SM_CONTAINER = f\"nvcr.io/nvidia/riva/riva-speech:{__riva_version__}-servicemaker-l4t-aarch64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the ServiceMaker Docker container\n",
    "! docker pull $RIVA_SM_CONTAINER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we execute `riva-build` to create a pipeline configured for Offline Recognition. This command for reference is also present in the [pipeline configuration](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-customizing.html#pipeline-configuration) section of the docs. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set relevant paths relative to where we will mount the models in the Servicemaker docker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All model paths relative to Riva Servicemaker docker include the _SM suffix\n",
    "\n",
    "ASR_MODEL_DIR_SM = \"/data\" # Path where we mount the downloaded ASR models in the Servicemaker docker\n",
    "\n",
    "# Relative path to Acoustic Model\n",
    "AM_SM = os.path.join(ASR_MODEL_DIR_SM, AM_DIR, \"Conformer-CTC-L_spe128_en-US_6.0.riva\")\n",
    "\n",
    "# Relative path to LM model artifacts\n",
    "DECODING_LM_BINARY_SM = os.path.join(ASR_MODEL_DIR_SM, LM_DIR, \"en-US_default_6.0.bin\")\n",
    "DECODING_VOCAB_SM = os.path.join(ASR_MODEL_DIR_SM, LM_DIR, \"en-US_default_6.0_dict_vocab.txt\")\n",
    "\n",
    "# Relative path to ITN artifacts\n",
    "WFST_TOKENIZER_MODEL_SM = os.path.join(ASR_MODEL_DIR_SM, ITN_DIR, \"tokenize_and_classify.far\")\n",
    "WFST_VERBALIZER_MODEL_SM = os.path.join(ASR_MODEL_DIR_SM, ITN_DIR, \"verbalize.far\")\n",
    "FAR_SPEECH_HINTS_SM = os.path.join(ASR_MODEL_DIR_SM, ITN_DIR, \"speech_class.far\")\n",
    "\n",
    "# Relative path where the generated .rmir file will be stored\n",
    "RMIR_DIR = \"default-models/rmir\"\n",
    "!mkdir -p $ASR_MODEL_DIR/$RMIR_DIR\n",
    "ASR_RMIR_DIR_SM = os.path.join(ASR_MODEL_DIR_SM, RMIR_DIR)\n",
    "ASR_RMIR_SM = os.path.join(ASR_RMIR_DIR_SM, \"asr_lm_itn_offline.rmir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Riva ServiceMaker Docker container to run `riva-build`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! docker run --rm --gpus all -v $ASR_MODEL_DIR:$ASR_MODEL_DIR_SM $RIVA_SM_CONTAINER -- \\\n",
    "    riva-build speech_recognition $ASR_RMIR_SM:$KEY $AM_SM:$KEY \\\n",
    "        --force \\\n",
    "        --offline \\\n",
    "        --name=conformer-en-US-asr-offline \\\n",
    "        --return_separate_utterances=True \\\n",
    "        --featurizer.use_utterance_norm_params=False \\\n",
    "        --featurizer.precalc_norm_time_steps=0 \\\n",
    "        --featurizer.precalc_norm_params=False \\\n",
    "        --ms_per_timestep=40 \\\n",
    "        --endpointing.start_history=200 \\\n",
    "        --nn.fp16_needs_obey_precision_pass \\\n",
    "        --endpointing.residue_blanks_at_start=-2 \\\n",
    "        --chunk_size=4.8 \\\n",
    "        --left_padding_size=1.6 \\\n",
    "        --right_padding_size=1.6 \\\n",
    "        --max_batch_size=16 \\\n",
    "        --featurizer.max_batch_size=512 \\\n",
    "        --featurizer.max_execution_batch_size=512 \\\n",
    "        --decoder_type=flashlight \\\n",
    "        --flashlight_decoder.asr_model_delay=-1 \\\n",
    "        --decoding_language_model_binary=$DECODING_LM_BINARY_SM \\\n",
    "        --decoding_vocab=$DECODING_VOCAB_SM \\\n",
    "        --flashlight_decoder.lm_weight=0.8 \\\n",
    "        --flashlight_decoder.word_insertion_score=1.0 \\\n",
    "        --flashlight_decoder.beam_size=32 \\\n",
    "        --flashlight_decoder.beam_threshold=20. \\\n",
    "        --flashlight_decoder.num_tokenization=1 \\\n",
    "        --language_code=en-US \\\n",
    "        --wfst_tokenizer_model=$WFST_TOKENIZER_MODEL_SM \\\n",
    "        --wfst_verbalizer_model=$WFST_VERBALIZER_MODEL_SM \\\n",
    "        --speech_hints_model=$FAR_SPEECH_HINTS_SM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments we used above are just an example, and there are many more optional parameters you can configure! For now, let's take a look into what those arguments we used above mean -\n",
    "\n",
    "* General pipeline parameters:\n",
    "    * `--name`: Name of the ASR pipeline, used to set the model names in the Riva model repository\n",
    "    * `--offline`: The Riva ASR pipeline can be configured for both streaming and offline recognition use cases. Here, we mark it to use it in the `offline` setting. More details on recommended configuration for offline/streaming are in the [documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-pipeline-configuration.html#streaming-offline-recognition).\n",
    "    * `--language_code`: Language of the model\n",
    "    * `--force`: Overwrites the specified `.rmir` file if it exists.\n",
    "    * `--max_batch_size`: Default maximum parallel requests in a single forward pass\n",
    "    * `--chunk_size`: Size of audio chunks to use during inference. If not specified, default will be selected based on online/offline setting. \n",
    "    * `--left_padding_size`: The duration in seconds of the backward looking padding to prepend to the audio chunk. The acoustic model input corresponds to a duration of (left_padding_size + chunk_size + right_padding_size) seconds\n",
    "    * `--right_padding_size`: The duration in seconds of the forward looking padding to append to the audio chunk. The acoustic model input corresponds to a duration of (left_padding_size + chunk_size + right_padding_size) seconds\n",
    "* ITN model specific parameters\n",
    "    * `--wfst_tokenizer_model`: Sparrowhawk model to use for tokenization and classification, must be in `.far` (finite-state archive) format. \n",
    "    * `--wfst_verbalizer_model`: Sparrowhawk model to use for verbalizer, must be in `.far` (finite-state archive) format.\n",
    "    * `--speech_hints_model`: Speechhints class `.far` file used to enable speechhints.\n",
    "* Acoustic model specific parameters\n",
    "    * `--ms_per_timestep`: The duration in milliseconds of one timestep of the acoustic model output.\n",
    "* Neural network specific parameters\n",
    "    * `--nn.fp16_needs_obey_precision_pass`: Flag to explicitly mark layers as float when parsing the ONNX network\n",
    "* Featurizer specific parameters\n",
    "    * `--featurizer.use_utterance_norm_params`: Apply normalization at utterance level\n",
    "    * `--featurizer.precalc_norm_time_steps`: Weight of the precomputed normalization parameters, in timesteps. Setting to 0 will disable use of precalculated normalization parameters.\n",
    "    * `--featurizer.precalc_norm_params`: Boolean that controls if precalculated normalization parameters should be used\n",
    "    * `--vad.residue_blanks_at_start`: Number of time steps to ignore at the beginning of the acoustic model output when trying to detect start/end of speech\n",
    "* Decoder & Language Model specific parameters\n",
    "    * `--decoder_type`: Type of decoder to use. Valid entries are greedy, os2s, flashlight or kaldi. In this example, we used the flashlight decoder.\n",
    "    * `--flashlight_decoder.asr_model_delay`: Number of time steps by which the acoustic model output should be shifted when computing timestamps. This parameter must be tuned since the CTC model is not guaranteed to predict correct alignment.\n",
    "    * `--decoding_language_model_binary`: Language model .binary used during decoding\n",
    "    * `--decoding_vocab`: File of unique words separated by white space. Only used if decoding_lexicon not provided\n",
    "    * `--flashlight_decoder.lm_weight`: Weight of language model. This affects the overall contribution of the language model score to the overall hypothesis score.\n",
    "    * `--flashlight_decoder.word_insertion_score`: Word insertion score used when scoring hypothesis\n",
    "    * `--flashlight_decoder.beam_threshold`: Threshold to prune hypothesis\n",
    "\n",
    "This information is also accessible through the `riva-build speech_recognition -h` command, and more information about additional parameters to `riva-build` can be found in the [riva-build optional parameters](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-pipeline-configuration.html#riva-build-optional-parameters) documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker run --rm $RIVA_SM_CONTAINER -- riva-build speech_recognition -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the .rmir\n",
    "!ls -lt $ASR_MODEL_DIR/$RMIR_DIR/*.rmir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Riva-deploy\n",
    "\n",
    "The deployment tool takes as input one or more Riva Model Intermediate Representation (RMIR) files and a target model repository directory. It creates an ensemble configuration specifying the pipeline for the execution and finally writes all those assets to the output model repository directory.\n",
    "\n",
    "`NOTE`: \n",
    "1. This step may take about 10 mins to complete.\n",
    "2. When running `riva-deploy`, we map `$ASR_MODEL_DIR/default-models` to `$ASR_MODEL_DIR_SM` (`/data`) inside the Riva ServiceMaker Docker container. This is because the scripts in the Riva Skills Quick Start resource folder (which we'll download later) expect the directory containing the `rmir` and `models` directories to be mapped to `/data`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the model repository relative to the ServiceMaker Docker container\n",
    "MODEL_REPO_SM = os.path.join(ASR_MODEL_DIR_SM, \"models\")\n",
    "# Reset the RMIR path relative to the ServiceMaker Docker container\n",
    "ASR_RMIR_SM = os.path.join(ASR_MODEL_DIR_SM, \"rmir\", \"asr_lm_itn_offline.rmir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Syntax: riva-deploy -f dir-for-rmir/model.rmir:key output-dir-for-repository\n",
    "! docker run --rm --gpus all -v $ASR_MODEL_DIR/default-models:$ASR_MODEL_DIR_SM $RIVA_SM_CONTAINER -- \\\n",
    "            riva-deploy -f  $ASR_RMIR_SM:$KEY $MODEL_REPO_SM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the models directory\n",
    "!ls -lt $ASR_MODEL_DIR/default-models/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Start the Riva Server\n",
    "After the model repository is generated, we are ready to start the Riva server. First, download the Riva Skills Quick Start resources from NGC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Riva Skills Quick Start guide\n",
    "The [Riva Skills Quick Start](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/resources/riva_quickstart) guide contains easy-to-use scripts to download and deploy models. \n",
    "\n",
    "`NOTE:` The scripts in Quick Start can download and deploy the default models. We downloaded the ASR models above just to demonstrate how to use Riva ServiceMaker tools, which will be used during customization tutorials to re-deploy the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TARGET_MACHINE.lower() in [\"amd64\", \"arm64_linux\"]:\n",
    "    quickstart_link = f\"nvidia/riva/riva_quickstart:{__riva_version__}\"\n",
    "else:\n",
    "    quickstart_link = f\"nvidia/riva/riva_quickstart_arm64:{__riva_version__}\"\n",
    "\n",
    "RIVA_DIR = ngc_download_and_get_dir(quickstart_link, \"Riva Quick Start resource folder\", resource_type=\"resource\", parent_dir=os.getcwd())\n",
    "RIVA_DIR = os.path.join(os.getcwd(), RIVA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configure Riva Quick Start \n",
    "This configures the scripts to deploy the ASR models we obtained as a result of Riva servicemaker tools in the previous section. <br>\n",
    "For this, we modify the `config.sh` file to enable relevant Riva services (ASR for the Citrinet model), provide the encryption key, and path to the model repository (`riva_model_loc`) generated in the previous step among other configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lt $RIVA_DIR/config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if above the model repository is generated at `$ASR_MODEL_DIR/default-models/models`, then you can specify `riva_model_loc` as the same directory as `ASR_MODEL_DIR/default-models`. <br>\n",
    "\n",
    "Pretrained versions of models specified in `models_asr/nlp/tts/nmt` are fetched from NGC. Since we are using our custom model, we can comment it in `models_asr` (and any others that are not relevant to your use case). <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### config.sh snippet\n",
    "```sh\n",
    "### config.sh snippet  \n",
    "# Enable or Disable Riva Services\n",
    "# For any language other than en-US: service_enabled_nlp must be set to false\n",
    "service_enabled_asr=true\n",
    "service_enabled_nlp=true          ## MAKE CHANGES HERE - SET TO FALSE\n",
    "service_enabled_tts=true          ## MAKE CHANGES HERE - SET TO FALSE\n",
    "service_enabled_nmt=true          ## MAKE CHANGES HERE - SET TO FALSE\n",
    "\n",
    "...\n",
    "\n",
    "# Locations to use for storing models artifacts\n",
    "#\n",
    "# If an absolute path is speccified, the data will be written to that location\n",
    "# Otherwise, a Docker volume will be used (default).\n",
    "#\n",
    "# riva_init.sh will create a `rmir` and `models` directory in the volume or\n",
    "# path specified.\n",
    "#\n",
    "# RMIR ($riva_model_loc/rmir)\n",
    "# Riva uses an intermediate representation (RMIR) for models\n",
    "# that are ready to deploy but not yet fully optimized for deployment. Pretrained\n",
    "# versions can be obtained from NGC (by specifying NGC models below) and will be\n",
    "# downloaded to $riva_model_loc/rmir by `riva_init.sh`\n",
    "#\n",
    "# Custom models produced by NeMo or TLT and prepared using riva-build\n",
    "# may also be copied manually to this location $(riva_model_loc/rmir).\n",
    "#\n",
    "# Models ($riva_model_loc/models)\n",
    "# During the riva_init process, the RMIR files in $riva_model_loc/rmir\n",
    "# are inspected and optimized for deployment. The optimized versions are\n",
    "# stored in $riva_model_loc/models. The riva server exclusively uses these\n",
    "# optimized versions.\n",
    "riva_model_loc=\"riva-model-repo\"  ## MAKE CHANGES HERE (Replace with the path ASR_MODEL_DIR/default-models)\n",
    "\n",
    "if [[ $riva_target_gpu_family == \"tegra\" ]]; then\n",
    "    riva_model_loc=\"`pwd`/model_repository\"\n",
    "fi\n",
    "\n",
    "# The default RMIRs are downloaded from NGC by default in the above $riva_rmir_loc directory\n",
    "# If you'd like to skip the download from NGC and use the existing RMIRs in the $riva_rmir_loc\n",
    "# then set the below $use_existing_rmirs flag to true. You can also deploy your set of custom\n",
    "# RMIRs by keeping them in the riva_rmir_loc dir and use this quickstart script with the\n",
    "# below flag to deploy them all together.\n",
    "use_existing_rmirs=false          ## MAKE CHANGES HERE - SET TO TRUE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to make the following changes to `config.sh` without opening the file in a text editor:\n",
    "\n",
    "1. Set NLP, NMT, and TTS services to `false`\n",
    "2. Set the `riva_model_loc` path to the path also assigned to `ASR_MODEL_DIR/default-models`\n",
    "3. Set the variable `use_existing_rmirs` to `true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(f\"{RIVA_DIR}/config.sh\", \"r\") as config_in:\n",
    "    config_file = config_in.readlines()\n",
    "\n",
    "for i, line in enumerate(config_file):\n",
    "    # Disable services\n",
    "    if line.startswith(\"service_enabled_asr\"):\n",
    "        config_file[i] = \"service_enabled_asr=true\\n\"\n",
    "    elif line.startswith(\"service_enabled_nlp\"):\n",
    "        config_file[i] = \"service_enabled_nlp=false\\n\"\n",
    "    elif line.startswith(\"service_enabled_nmt\"):\n",
    "        config_file[i] = \"service_enabled_nmt=false\\n\"\n",
    "    elif line.startswith(\"service_enabled_tts\"):\n",
    "        config_file[i] = \"service_enabled_tts=false\\n\"\n",
    "    # Update riva_model_loc to our rmir folder\n",
    "    elif line.startswith(\"riva_model_loc\"):\n",
    "        config_file[i] = f'riva_model_loc=\"{ASR_MODEL_DIR}/default-models\"\\n'\n",
    "    elif line.startswith(\"use_existing_rmirs\"):\n",
    "        config_file[i] = \"use_existing_rmirs=true\\n\"\n",
    "\n",
    "with open(f\"{RIVA_DIR}/config.sh\", \"w\") as config_in:\n",
    "    config_in.writelines(config_file)\n",
    "\n",
    "print(\"\".join(config_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have permission to execute these scripts\n",
    "! cd $RIVA_DIR && chmod +x ./riva_start.sh && chmod +x ./riva_stop.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, one runs `riva_init.sh` before `riva_start.sh`. However, since we've already built our `.rmir` file with `riva-build` and deployed the associated model files by running `riva-deploy`, we can skip straight to `riva_start.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run Riva Start to start the server. This will deploy your model(s).\n",
    "! cd $RIVA_DIR && ./riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Inference\n",
    "Once the Riva server is up and running with the models, you can send inference requests querying the server. \n",
    "\n",
    "To send gRPC requests, you can install the Riva Python API bindings for the client. This is available as a `pip` [package](https://pypi.org/project/nvidia-riva-client/). Feel free to read more about the python client [here](https://github.com/nvidia-riva/python-clients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the Client API Bindings\n",
    "! pip install nvidia-riva-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the Riva Server and Run Automatic Speech Recognition\n",
    "The following cells queries the Riva server (using gRPC) with an input audio to yield a transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import IPython.display as ipd\n",
    "import grpc\n",
    "import time\n",
    "\n",
    "try:\n",
    "    import riva.client # RIVA 2.3.0 and above\n",
    "except:\n",
    "    import riva_api.riva_audio_pb2 as ra # RIVA 2.0.0 and above\n",
    "    import riva_api.audio_pb2 as ra\n",
    "    import riva_api.riva_asr_pb2 as rasr\n",
    "    import riva_api.riva_asr_pb2_grpc as rasr_srv\n",
    "import wave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following URI assumes a local deployment of the Riva Speech API server is on the default port. In case the server deployment is on a different host or via a Helm chart on Kubernetes, use an appropriate URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = riva.client.Auth(uri='localhost:50051')\n",
    "\n",
    "riva_asr = riva.client.ASRService(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample audio file from local disk\n",
    "# This example uses a .wav file with LINEAR_PCM encoding.\n",
    "audio_file = \"audio_samples/en-US_wordboosting_sample1.wav\"\n",
    "    \n",
    "# Listen to the sample audio we are looking to transcribe\n",
    "ipd.Audio(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = wave.open(audio_file, 'rb')\n",
    "with open(audio_file, 'rb') as fh:\n",
    "    content = fh.read()\n",
    "\n",
    "# Creating RecognitionConfig\n",
    "config = riva.client.RecognitionConfig(\n",
    "  language_code=\"en-US\",\n",
    "  max_alternatives=1,\n",
    "  enable_automatic_punctuation=True,\n",
    "  audio_channel_count = 1\n",
    ")\n",
    "\n",
    "# ASR Inference call with Recognize \n",
    "response = riva_asr.offline_recognize(content, config)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, you should see a transcription result for the input audio sequence. Now you have a speech recognition pipeline running! \n",
    "\n",
    "The ground truth transcription is: *AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein sequences for particular target antigens.*\n",
    "\n",
    "So, it looks like the the domain-specific terms like `AntiBERTa` and `ABlooper` were not transcribed well. In the next notebook, you will look into how you can improve transcription of such words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can stop the Riva ServiceMaker container (and thus shut down the Riva server) before shutting down the Jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker container stop riva-speech"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba-riva-tutorials",
   "language": "python",
   "name": "mamba-riva-tutorials"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
