{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_tts_tts-python-advanced-customization-with-ssml/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How do I customize Riva TTS audio output with SSML?\n",
    "\n",
    "This tutorial walks you through some of the advanced features for customization of Riva TTS audio output with Speech Synthesis Markup Language (SSML)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVIDIA Riva Overview\n",
    "\n",
    "NVIDIA Riva is a GPU-accelerated SDK for building Speech AI applications that are customized for your use case and deliver real-time performance. <br/>\n",
    "Riva offers a rich set of speech and natural language understanding services such as:\n",
    "\n",
    "- Automated speech recognition (ASR)\n",
    "- Text-to-Speech synthesis (TTS)\n",
    "- A collection of natural language processing (NLP) services, such as named entity recognition (NER), punctuation, and intent classification.\n",
    "\n",
    "In this tutorial, we will customize Riva TTS audio output with SSML. <br> \n",
    "To understand the basics of Riva TTS APIs, refer to [How do I use Riva TTS APIs with out-of-the-box models?](https://github.com/nvidia-riva/tutorials/tree/stable/tts-python-basics.ipynb). <br>\n",
    "\n",
    "For more information about Riva, refer to the [Riva developer documentation](https://developer.nvidia.com/riva)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing Riva TTS audio output with SSML\n",
    "\n",
    "Speech Synthesis Markup Language (SSML) specification is a markup for directing the performance of the virtual speaker. Riva supports portions of SSML, allowing you to adjust pitch, rate, and pronunciation of the generated audio.  \n",
    "SSML support is available only for the FastPitch model at this time. The FastPitch model must be exported using NeMo>=1.5.1 and the nemo2riva>=1.8.0 tool.\n",
    "\n",
    "All SSML inputs must be a valid XML document and use the <speak> root tag. All non-valid XML and all valid XML with a different root tag are treated as raw input text.\n",
    "\n",
    "Riva TTS supports the following SSML tags:  \n",
    "\n",
    "- The ``prosody`` tag, which supports attributes ``rate``, ``pitch`` and ``volume``, through which we can control the rate, pitch and volume of the generated audio.  \n",
    "\n",
    "- The ``phoneme`` tag, which allows us to control the pronunciation of the generated audio.\n",
    "    \n",
    "- The ``sub`` tag, which allows us to replace the pronounciation of the specified word or phrase with a different word or phrase.\n",
    "    \n",
    "Let's look at customization of Riva TTS with these SSML tags in some detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements and setup\n",
    "\n",
    "1. Start the Riva Speech Skills server.  \n",
    "Follow the instructions in the [Riva Quick Start Guide](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html#) to deploy OOTB TTS models on the Riva Speech Skills server before running this tutorial.  \n",
    "\n",
    "2. Install the additional Python libraries to run this tutorial.  \n",
    "Run the following commands to install the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We need numpy to read the output from Riva TTS request\n",
    "!pip install numpy\n",
    "!pip install nvidia-riva-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Riva Client Libraries\n",
    "\n",
    "Let's first import some required libraries, including the Riva Client libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import grpc\n",
    "\n",
    "import riva.client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Riva Clients and connect to the Riva Speech API server\n",
    "\n",
    "The below URI assumes a local deployment of the Riva Speech API server is on the default port. In case the server deployment is on a different host or via Helm chart on Kubernetes, use an appropriate URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = riva.client.Auth(uri='localhost:50051')\n",
    "\n",
    "riva_tts = riva.client.SpeechSynthesisService(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_rate_hz = 44100\n",
    "req = { \n",
    "        \"language_code\"  : \"en-US\",\n",
    "        \"encoding\"       : riva.client.AudioEncoding.LINEAR_PCM ,\n",
    "        \"sample_rate_hz\" : sample_rate_hz,\n",
    "        \"voice_name\"     : \"English-US.Female-1\" \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing rate, pitch and volume with the `prosody` tag\n",
    "\n",
    "#### Pitch Attribute\n",
    "Riva supports an additive relative change to the pitch. The `pitch` attribute has a range of [-3, 3]. Values outside this range result in an error being logged and no audio returned. This value returns a pitch shift of the attribute value multiplied with the speakerâ€™s pitch standard deviation when the FastPitch model is trained. For the pretrained checkpoint that was trained on LJSpeech, the standard deviation was 52.185. For example, a pitch shift of 1.25 results in a change of 1.25*52.185=~65.23Hz pitch shift up. \n",
    "Riva also supports the prosody tags as per the SSML specs. Prosody tags `x-low`, `low`, `medium`, `high`, `x-high`, and `default` are supported.\n",
    "\n",
    "The `pitch` attribute is expressed in the following formats:\n",
    "- `pitch=\"1\"`\n",
    "- `pitch=\"+1.8\"`\n",
    "- `pitch=\"-0.65\"`\n",
    "- `pitch=\"high\"`\n",
    "- `pitch=\"default\"`\n",
    "\n",
    "For the pretrained Female-1 checkpoint, the standard deviation is 53.33 Hz.\n",
    "For the pretrained Male-1 checkpoint, the standard deviation is 47.15 Hz.\n",
    "\n",
    "The `pitch` attribute does not support `Hz`, `st`, and `%` changes. Support is planned for a future Riva release.\n",
    "\n",
    "#### Rate Attribute\n",
    "Riva supports a percentage relative change to the rate. The `rate` attribute has a range of [25%, 250%]. Values outside this range result in an error being logged and no audio returned. \n",
    "Riva also supports the prosody tags as per the SSML specs. Prosody tags `x-low`, `low`, `medium`, `high`, `x-high`, and `default` are supported.\n",
    "\n",
    "The `rate` attribute is expressed in the following formats:\n",
    "- `rate=\"35%\"`\n",
    "- `rate=\"+200%\"`\n",
    "- `rate=\"low\"`\n",
    "- `rate=\"default\"`\n",
    "\n",
    "#### Volume Attribute\n",
    "\n",
    "Riva supports the volume attribute as described in the SSML specs. The volume attribute supports a range of [-13, 8]dB. Values outside this range result in an error being logged and no audio returned. Tags silent, x-soft, soft, medium, loud, x-loud, and default are supported.\n",
    "\n",
    "The volume attribute is expressed in the following formats:\n",
    "\n",
    "- `volume=\"+1dB\"`\n",
    "- `volume=\"-5.7dB\"`\n",
    "- `volume=\"x-loud\"`\n",
    "- `volume=\"default\"`\n",
    "\n",
    "\n",
    "Let's look at an example showing these pitch, rate and volume customizations for Riva TTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Raw text is \"Today is a sunny day. But it might rain tomorrow.\"\n",
    "    We are updating this raw text with SSML:\n",
    "    1. Envelope raw text in '<speak>' tags as is required for SSML\n",
    "    2. Add '<prosody>' tag with 'pitch' attribute set to '2.5'\n",
    "    3. Add '<prosody>' tag with 'rate' attribute set to 'high'\n",
    "    4. Add '<volume>' tag with 'volume' attribute set to '+1dB'\n",
    "\"\"\"\n",
    "raw_text = \"Today is a sunny day. But it might rain tomorrow.\"\n",
    "ssml_text = \"\"\"<speak><prosody pitch='2.5'>Today is a sunny day</prosody>. <prosody rate='high' volume='+1dB'>But it might rain tomorrow.</prosody></speak>\"\"\"\n",
    "print(\"Raw Text: \", raw_text)\n",
    "print(\"SSML Text: \", ssml_text)\n",
    "\n",
    "\n",
    "req[\"text\"] = ssml_text\n",
    "# Request to Riva TTS to synthesize audio\n",
    "resp = riva_tts.synthesize(**req)\n",
    "\n",
    "# Playing the generated audio from Riva TTS request\n",
    "audio_samples = np.frombuffer(resp.audio, dtype=np.int16)\n",
    "ipd.display(ipd.Audio(audio_samples, rate=sample_rate_hz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected results if you run the notebook:\n",
    "`<prosody pitch='2.5'>Today is a sunny day</prosody>. <prosody rate='high' volume='+1dB'>But it might rain tomorrow.</prosody>`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_0.wav\" type=\"audio/ogg\"></audio>  \n",
    "##### Note: If the audio controls are not seen in the notebook. Open the notebook in github dev or view it in the [riva docs](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tts/tts-ssml.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are more examples showing the effects of changes in pitch, and rate attribute values on the generated audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SSML texts we want to try\n",
    "ssml_texts = [\n",
    "  \"\"\"<speak>This is a normal sentence</speak>\"\"\",\n",
    "  \"\"\"<speak><prosody pitch=\"0.\" rate=\"100%\">This is also a normal sentence</prosody></speak>\"\"\",\n",
    "  \"\"\"<speak><prosody rate=\"200%\">This is a fast sentence</prosody></speak>\"\"\",\n",
    "  \"\"\"<speak><prosody rate=\"60%\">This is a slow sentence</prosody></speak>\"\"\",\n",
    "  \"\"\"<speak><prosody pitch=\"+1.0\">Now, I'm speaking a bit higher</prosody></speak>\"\"\",\n",
    "  \"\"\"<speak><prosody pitch=\"-0.5\">And now, I'm speaking a bit lower</prosody></speak>\"\"\",\n",
    "  \"\"\"<speak>S S M L supports <prosody pitch=\"-1\">nested tags. So I can speak <prosody rate=\"150%\">faster</prosody>, <prosody rate=\"75%\">or slower</prosody>, as desired.</prosody></speak>\"\"\",\n",
    "  \"\"\"<speak><prosody volume='x-soft'>I'm speaking softly.</prosody><prosody volume='x-loud'> And now, This is loud.</prosody></speak>\"\"\",\n",
    "]\n",
    "\n",
    "# Loop through 'ssml_texts' list and synthesize audio with Riva TTS for each entry 'ssml_texts'\n",
    "for ssml_text in ssml_texts:\n",
    "    req[\"text\"] = ssml_text\n",
    "    resp = riva_tts.synthesize(**req)\n",
    "    audio_samples = np.frombuffer(resp.audio, dtype=np.int16)\n",
    "    print(\"SSML Text: \", ssml_text)\n",
    "    ipd.display(ipd.Audio(audio_samples, rate=sample_rate_hz))\n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected results if you run the notebook:\n",
    "`This is a normal sentence`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_1.wav\" type=\"audio/ogg\"></audio>  \n",
    "\n",
    "`<prosody pitch=\"0.\" rate=\"100%\">This is also a normal sentence</prosody>`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_2.wav\" type=\"audio/ogg\"></audio> \n",
    "\n",
    "`<prosody rate=\"200%\">This is a fast sentence</prosody>`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_3.wav\" type=\"audio/ogg\"></audio> \n",
    "\n",
    "`<prosody rate=\"60%\">This is a slow sentence</prosody>`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_4.wav\" type=\"audio/ogg\"></audio> \n",
    "\n",
    "`<prosody pitch=\"+1.0\">Now, I'm speaking a bit higher</prosody>`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_5.wav\" type=\"audio/ogg\"></audio> \n",
    "\n",
    "`<prosody pitch=\"-0.5\">And now, I'm speaking a bit lower</prosody>`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_6.wav\" type=\"audio/ogg\"></audio> \n",
    "\n",
    "`S S M L supports <prosody pitch=\"-1\">nested tags. So I can speak <prosody rate=\"150%\">faster</prosody>, <prosody rate=\"75%\">or slower</prosody>, as desired.</prosody>`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_7.wav\" type=\"audio/ogg\"></audio> \n",
    "\n",
    "`<prosody volume='x-soft'>I'm speaking softly.</prosody><prosody volume='x-loud'> And now, This is loud.</prosody>`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_8.wav\" type=\"audio/ogg\"></audio>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing pronunciation with the `phoneme` tag\n",
    "\n",
    "We can use the `phoneme` tag to override the pronunciation of words from the predicted pronunciation. For a given word or sequence of words, use the `ph` attribute to provide an explicit pronunciation, and the `alphabet` attribute to provide the phone set.  \n",
    "Currently, only `x-arpabet` is supported for pronunciation dictionaries based on CMUdict. IPA support will be added soon.\n",
    "\n",
    "The full list of phonemes in the CMUdict can be found at [cmudict.phone](https://github.com/cmusphinx/cmudict/blob/master/cmudict.phones). The list of supported symbols with stress can be found at [cmudict.symbols](https://github.com/cmusphinx/cmudict/blob/master/cmudict.symbols). For a mapping of these phones to English sounds, refer to the [ARPABET Wikipedia page](https://en.wikipedia.org/wiki/ARPABET).\n",
    "\n",
    "Let's look at an example showing this custom pronunciation for Riva TTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting up Riva TTS request with SynthesizeSpeechRequest\n",
    "\"\"\"\n",
    "    Raw text is \"You say tomato, I say tomato.\"\n",
    "    We are updating this raw text with SSML:\n",
    "    1. Envelope raw text in '<speak>' tags as is required for SSML\n",
    "    2. For a substring in the raw text, add '<phoneme>' tags with 'alphabet' attribute set to 'x-arpabet' \n",
    "       (currently the only supported value) and 'ph' attribute set to a custom pronunciation based on CMUdict and ARPABET\n",
    "\n",
    "\"\"\"\n",
    "raw_text = \"You say tomato, I say tomato.\"\n",
    "ssml_text = '<speak>You say <phoneme alphabet=\"x-arpabet\" ph=\"{@T}{@AH0}{@M}{@EY1}{@T}{@OW2}\">tomato</phoneme>, I say <phoneme alphabet=\"x-arpabet\" ph=\"{@T}{@AH0}{@M}{@AA1}{@T}{@OW2}\">tomato</phoneme>.</speak>'\n",
    "\n",
    "print(\"Raw Text: \", raw_text)\n",
    "print(\"SSML Text: \", ssml_text)\n",
    "\n",
    "req[\"text\"] = ssml_text\n",
    "# Request to Riva TTS to synthesize audio\n",
    "resp = riva_tts.synthesize(**req)\n",
    "\n",
    "# Playing the generated audio from Riva TTS request\n",
    "audio_samples = np.frombuffer(resp.audio, dtype=np.int16)\n",
    "ipd.display(ipd.Audio(audio_samples, rate=sample_rate_hz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected results if you run the notebook:\n",
    "`<speak>You say <phoneme alphabet=\"x-arpabet\" ph=\"{@T}{@AH0}{@M}{@EY1}{@T}{@OW2}\">tomato</phoneme>, I say <phoneme alphabet=\"x-arpabet\" ph=\"{@T}{@AH0}{@M}{@AA1}{@T}{@OW2}\">tomato</phoneme>.`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_9.wav\" type=\"audio/ogg\"></audio> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are more examples showing the customization of pronunciation in generated audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SSML texts we want to try\n",
    "ssml_texts = [\n",
    "  \"\"\"<speak>You say <phoneme alphabet=\"x-arpabet\" ph=\"{@D}{@EY1}{@T}{@AH0}\">data</phoneme>, I say <phoneme alphabet=\"x-arpabet\" ph=\"{@D}{@AE1}{@T}{@AH0}\">data</phoneme>.</speak>\"\"\",\n",
    "  \"\"\"<speak>Some people say <phoneme alphabet=\"x-arpabet\" ph=\"{@R}{@UW1}{@T}\">route</phoneme> and some say <phoneme alphabet=\"x-arpabet\" ph=\"{@R}{@AW1}{@T}\">route</phoneme>.</speak>\"\"\",\n",
    "]\n",
    "\n",
    "# Loop through 'ssml_texts' list and synthesize audio with Riva TTS for each entry 'ssml_texts'\n",
    "for ssml_text in ssml_texts:\n",
    "    req[\"text\"] = ssml_text\n",
    "    resp = riva_tts.synthesize(**req)\n",
    "    audio_samples = np.frombuffer(resp.audio, dtype=np.int16)\n",
    "    print(\"SSML Text: \", ssml_text)\n",
    "    ipd.display(ipd.Audio(audio_samples, rate=sample_rate_hz))\n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected results if you run the notebook:\n",
    "`You say <phoneme alphabet=\"x-arpabet\" ph=\"{@D}{@EY1}{@T}{@AH0}\">data</phoneme>, I say <phoneme alphabet=\"x-arpabet\" ph=\"{@D}{@AE1}{@T}{@AH0}\">data</phoneme>`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_10.wav\" type=\"audio/ogg\"></audio>\n",
    "\n",
    "`Some people say <phoneme alphabet=\"x-arpabet\" ph=\"{@R}{@UW1}{@T}\">route</phoneme> and some say <phoneme alphabet=\"x-arpabet\" ph=\"{@R}{@AW1}{@T}\">route</phoneme>.`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_11.wav\" type=\"audio/ogg\"></audio>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing pronunciation with the `sub` tag\n",
    "\n",
    "We can use the `sub` tag to replace the pronounciation of the specified word or phrase with a different word or phrase. You can specify the pronunciation to substitute with the alias attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting up Riva TTS request with SynthesizeSpeechRequest\n",
    "\"\"\"\n",
    "    Raw text is \"WWW is know as the web\"\n",
    "    We are updating this raw text with SSML:\n",
    "    1. Envelope raw text in '<speak>' tags as is required for SSML\n",
    "    2. Add '<sub>' tag with 'alias' attribute set to replace www with `World Wide Web`\n",
    "\n",
    "\"\"\"\n",
    "raw_text = \"WWW is know as the web.\"\n",
    "ssml_text = '<speak><sub alias=\"World Wide Web\">WWW</sub> is known as the web.</speak>'\n",
    "\n",
    "print(\"Raw Text: \", raw_text)\n",
    "print(\"SSML Text: \", ssml_text)\n",
    "\n",
    "req[\"text\"] = ssml_text\n",
    "# Request to Riva TTS to synthesize audio\n",
    "resp = riva_tts.synthesize(**req)\n",
    "# Playing the generated audio from Riva TTS request\n",
    "audio_samples = np.frombuffer(resp.audio, dtype=np.int16)\n",
    "ipd.display(ipd.Audio(audio_samples, rate=sample_rate_hz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected results if you run the notebook:\n",
    "`<sub alias=\"World Wide Web\">WWW</sub> is known as the web.`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_12.wav\" type=\"audio/ogg\"></audio>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emphasize words with the `emphasis` tag\n",
    "\n",
    "Use the emphasis tag to emphasize words. Use riva-build with enable_emphasis_tag, start_of_emphasis_token, and end_of_emphasis_token to enable the emphasis feature. \n",
    "The emphasis tag should be used per word basis. If the word ends with a punctuation, only the word will be emphasized and not the punctuation.\n",
    "\n",
    "#### Limitation\n",
    "The emphasis tag is training data dependent and is available only in a few models. The models which are trained without the emphasis tag in the training data will not result in emphasized speech. Input text containing more than one word wrapped by the emphasis tag is an invalid input. Space wrapped inside the emphasis tag is also an invalid input.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> Warning: The emphasis tag feature does not support nesting of other SSML tags inside it. The emphasis tag does not support the level attribute. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting up Riva TTS request with SynthesizeSpeechRequest\n",
    "\"\"\"\n",
    "    Raw text is \"Hello World\"\n",
    "    We are updating this raw text with SSML:\n",
    "    1. Envelope raw text in '<speak>' tags as is required for SSML\n",
    "    2. Add '<emphasis>' tag around `love`\n",
    "\n",
    "\"\"\"\n",
    "ssml_texts = [\n",
    "   \"\"\"<speak>I would <emphasis>love</emphasis> to try that.</speak>\"\"\",\n",
    "   \"\"\"<speak><emphasis>Wow!</emphasis> Thats really cool.</speak>\"\"\"\n",
    "]\n",
    "\n",
    "print(\"Raw Text: \", raw_text)\n",
    "print(\"SSML Text: \", ssml_text)\n",
    "\n",
    "for ssml_text in ssml_texts:\n",
    "    req[\"text\"] = ssml_text\n",
    "    resp = riva_tts.synthesize(**req)\n",
    "    # Playing the generated audio from Riva TTS request\n",
    "    audio_samples = np.frombuffer(resp.audio, dtype=np.int16)\n",
    "    print(\"SSML Text: \", ssml_text)\n",
    "    ipd.display(ipd.Audio(audio_samples, rate=sample_rate_hz))\n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected results if you run the notebook:\n",
    "`I would <emphasis>love</emphasis> to try that.`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_13.wav\" type=\"audio/ogg\"></audio>\n",
    "\n",
    "`<emphasis>Wow!</emphasis> Thats really cool.`  \n",
    "<audio controls src=\"https://raw.githubusercontent.com/nvidia-riva/tutorials/stable/audio_samples/tts_samples/ssml_sample_14.wav\" type=\"audio/ogg\"></audio>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about customizing Riva TTS with SSML can also be found in the documentation [here](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tts/tts-ssml.html#). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go deeper into Riva capabilities\n",
    "\n",
    "### Additional Riva tutorials\n",
    "\n",
    "Checkout more Riva TTS (and ASR) tutorials [here](https://github.com/nvidia-riva/tutorials). These tutorials provide a deeper understanding of the advanced features of Riva TTS, including customizing TTS for your specific needs.\n",
    "\n",
    "### Sample applications\n",
    "\n",
    "Riva comes with various sample applications. They demonstrate how to use the APIs to build applications such as a [chatbot](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/samples/weather.html), a domain specific speech recognition, [keyword (entity) recognition system](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/samples/callcenter.html), or simply how Riva allows scaling out for handling massive amounts of requests at the same time. Refer to ([SpeechSquad)](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/samples/speechsquad.html) for more information.  \n",
    "Refer to the *Sample Application* section in the [Riva developer documentation](https://developer.nvidia.com/) for more information.\n",
    "\n",
    "\n",
    "###  Riva Automated Speech Recognition (ASR)\n",
    "\n",
    "Riva's ASR offering comes with OOTB pipelines for English, German, Spanish, Russian and Mandarin. It can be used in streaming or batch inference modes and easily deployed using the [Riva Quick Start scripts](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html). Follow [this link](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-overview.html) to better understand Riva's ASR capabilities. Explore how to use Riva ASR APIs with the OOTB voices with [this Riva ASR tutorial](https://github.com/nvidia-riva/tutorials/blob/dev/22.04/asr-python-basics.ipynb).\n",
    "\n",
    "\n",
    "### Additional resources\n",
    "\n",
    "For more information about each of the APIs and their functionalities, refer to the [documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/protobuf-api/protobuf-api-root.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
