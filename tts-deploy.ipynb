{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caa47ff5",
   "metadata": {},
   "source": [
    "# TTS Deploy\n",
    "\n",
    "This tutorial explains the process of generating a TTS RMIR (Riva Model Intermediate Representation). A RMIR is an intermediate file that has all the necessary artifacts (models, files, configurations, and user settings) required to deploy a Riva service.  \n",
    "\n",
    "## Learning Objectives\n",
    "In this tutorial, you will learn how to:  \n",
    "- Use Riva ServiceMaker to take two `.riva` files and convert it to `.rmir` for either a `AMD64` (data center, `86_64`) or a `ARM64` (embedded, `AArch64`) machine.\n",
    "  - For users who have `.nemo` files, [`nemo2riva`](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/model-overview.html#export-models-with-nemo2riva) can be used to generate `.riva` files from `.nemo` checkpoints.\n",
    "- Launch and deploy the `.rmir` locally on the Riva server.\n",
    "- Send inference requests from a demo client using Riva API bindings.\n",
    "\n",
    "## Prerequisties\n",
    "To use this tutorial, ensure that you:\n",
    "- Have access to NGC through the [NGC Command-Line Interface (CLI)](https://docs.ngc.nvidia.com/cli/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a6684",
   "metadata": {},
   "source": [
    "## Riva ServiceMaker\n",
    "ServiceMaker is a set of tools that aggregates all the necessary artifacts (models, files, configurations, and user settings) for Riva deployment to a target environment. It has two main components:\n",
    "\n",
    "* `riva-build`\n",
    "* `riva-deploy`\n",
    "\n",
    "The first step is `riva-build`, which can be run on either data center or embedded machines to build an `.rmir` file.\n",
    "\n",
    "The second step is `riva-deploy`, which should be run on the machine that the Riva server is to be served on.\n",
    "\n",
    "If you are building an `.rmir` file on a data center machine to target an embedded deployment, follow this tutorial up to and including the [Riva-build section](#Run-riva-build). Copy the built `.rmir` to the target embedded machine, run the [set configs and params section](#Set-the-Configurations-and-Parameters), and continue to the [Riva-deploy section](#Run-riva-deploy).\n",
    "\n",
    "### Riva-build\n",
    "\n",
    "This step helps build a Riva-ready version of the model. It’s only output is an intermediate format (called a Riva Model Intermediate Representation (`.rmir`)) of an end-to-end pipeline for the supported services within Riva. Let’s consider two TTS models:\n",
    "\n",
    "* [FastPitch](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/speechsynthesis_en_us_fastpitch_ipa) (spectrogram generator)\n",
    "* [HiFi-GAN](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/speechsynthesis_en_us_hifigan_ipa) (vocoder).<br>\n",
    "\n",
    "`riva-build` is responsible for the combination of one or more exported models (`.riva` files) into a single file\n",
    "containing an intermediate format called `.rmir`. This file contains a\n",
    "deployment-agnostic specification of the whole end-to-end pipeline along with all the assets required for the\n",
    "final deployment and inference. Refer to the [Riva documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tts/tts-custom.html#fastpitch-and-hifi-gan) for more information.\n",
    "\n",
    "### Riva-deploy\n",
    "\n",
    "The deployment tool takes as input one or more `.rmir` files and a target model repository directory. It creates an ensemble configuration specifying the pipeline for\n",
    "the execution and finally writes all those assets to the output model repository directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d3303d",
   "metadata": {},
   "source": [
    "---\n",
    "### Set the Configurations and Parameters\n",
    "Update the parameters in the following code block:\n",
    "- `machine_type`: Type of machine the tutorial is being run on. Acceptable values are `AMD64`, `ARM64_linux`, `ARM64_l4t`. Defaults to `AMD64`.  \n",
    "- `target_machine`: Type of machine the RMIR will be deployed on. Acceptable values are `AMD64`, `ARM64_linux`, `ARM64_l4t`. Defaults to `AMD64`.  \n",
    "- `acoustic_model`: Full path for acoustic model `.riva` file. Defaults to `None`. This can be replaced with a custom acoustic model `.riva` checkpoint.  \n",
    "- `vocoder`: Full path for vocoder `.riva` file. Defaults to `None`. This can be replaced with a custom vocoder `.riva` checkpoint.  \n",
    "- `out_dir`: Directory to put the `TTS.rmir` file. The RMIR will be placed in `${out_dir}/RMIR/RMIR_NAME.rmir`. Defaults to `$pwd/out`.  \n",
    "- `voice`: Set the voice name of the model. Default to `\"test\"`.  \n",
    "- `key`: This is the encryption key used in `nemo2riva`. The same key will be used to deploy the RMIR generated in this tutorial. Defaults to `tlt_encode`.  \n",
    "- `use_ipa`: Set to `\"y\"` or `\"Y\"` if the model uses IPA phones, `\"no\"` if the model uses ARPAbet. Defaults to `\"yes\"`.  \n",
    "- `lang`: Model language. This is only used for the client, and has no effect on generated speech. Defaults to `\"en-US\"`.  \n",
    "- `sample_rate`: Sample rate of generated audios in Hz. Defaults to 44100.  \n",
    "- `num_speakers`: Number of speakers in the model. Defaults to 2, the number of speakers in the NGC example model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579b6420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "from version import __riva_version__\n",
    "\n",
    "machine_type=\"AMD64\" #Change this to `ARM64_linux` or `ARM64_l4t` in case of an ARM64 machine.\n",
    "target_machine=\"AMD64\" #Change this to `ARM64_linux` or `ARM64_l4t` in case of an ARM64 machine.\n",
    "acoustic_model = None ##acoustic_model .riva location\n",
    "vocoder = None ##vocoder .riva location\n",
    "out_dir = pathlib.Path.cwd() / \"out\" ##Output directory to store the generated RMIR. The RMIR will be placed in `${out_dir}/RMIR/RMIR_NAME.rmir`.\n",
    "voice = \"test\" ##Voice name\n",
    "key = \"tlt_encode\" ##Encryption key used during nemo2riva\n",
    "use_ipa = \"yes\" ##`\"y\"` or `\"Y\"` if the model uses `ipa`, no otherwise.\n",
    "lang = \"en-US\" ##Language\n",
    "sample_rate = 44100 ##Sample rate of the audios\n",
    "num_speakers = 2 ## Number of speakers\n",
    "\n",
    "riva_aux_files = None ##Riva model repo path. In the case of a custom model repo, change this to the full path of the custom Riva model repo.\n",
    "riva_tn_files = None ##Riva model repo path. In the case of a custom model repo, change this to the full path of the custom Riva model repo.\n",
    "\n",
    "## Riva NGC, servicemaker image config.\n",
    "if machine_type.lower() in [\"amd64\", \"arm64_linux\"]:\n",
    "    riva_init_image = f\"nvcr.io/nvidia/riva/riva-speech:{__riva_version__}-servicemaker\"\n",
    "elif machine_type.lower()==\"arm64_l4t\":\n",
    "    riva_init_image = f\"nvcr.io/nvidia/riva/riva-speech:{__riva_version__}-servicemaker-l4t-aarch64\"\n",
    "rmir_dir = out_dir / \"rmir\"\n",
    "\n",
    "if not out_dir.exists():\n",
    "    out_dir.mkdir()\n",
    "if not rmir_dir.exists():\n",
    "    rmir_dir.mkdir()\n",
    "\n",
    "def ngc_download_and_get_dir(ngc_resource_name, var, var_name, resource_type=\"model\"):\n",
    "    default_download_folder = \"_v\".join(ngc_resource_name.split(\"/\")[-1].split(\":\"))\n",
    "    !rm -rf ./riva_artifacts/{default_download_folder}\n",
    "    ngc_output = !ngc registry {resource_type} download-version {ngc_resource_name} --dest riva_artifacts\n",
    "    output = pathlib.Path(f\"./riva_artifacts/{default_download_folder}\")\n",
    "    if not output.exists():\n",
    "        ngc_output_formatted='\\n'.join(ngc_output)\n",
    "        logging.error(\n",
    "            f\"NGC was not able to download the requested model {ngc_resource_name}. \"\n",
    "            \"Please check the NGC error message, removed all directories, and re-start the \"\n",
    "            f\"notebook. NGC message: {ngc_output_formatted}\"\n",
    "        )\n",
    "        return None\n",
    "    riva_files_in_dir = list(output.glob(\"*.riva\"))\n",
    "    if len(riva_files_in_dir) > 0:\n",
    "        output = riva_files_in_dir[0]\n",
    "    if output is not None and var is not None:\n",
    "        warnings.warn(\n",
    "            f\"`{var_name}` had a non-default value of `{var}`. `{var_name}` will be updated to `{var}`\"\n",
    "        )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670aea87",
   "metadata": {},
   "source": [
    "### Download models\n",
    "\n",
    "The following code block will download the default NGC models: [FastPitch](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/speechsynthesis_en_us_fastpitch_ipa) and [HiFi-GAN](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/speechsynthesis_en_us_hifigan_ipa). They will be downloaded to a folder called `riva_artifacts`. If a current folder already exists, it will be removed.\n",
    "\n",
    "The code block can be skipped in case of custom models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca2a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "riva_ngc_artifacts = pathlib.Path.cwd() / \"riva_artifacts\"\n",
    "if not riva_ngc_artifacts.exists():\n",
    "    riva_ngc_artifacts.mkdir()\n",
    "\n",
    "acoustic_model = ngc_download_and_get_dir(\"nvidia/riva/speechsynthesis_en_us_fastpitch_ipa:deployable_v1.0\", acoustic_model, \"acoustic_model\")\n",
    "vocoder = ngc_download_and_get_dir(\"nvidia/riva/speechsynthesis_en_us_hifigan_ipa:deployable_v1.0\", vocoder, \"vocoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d8e550",
   "metadata": {},
   "source": [
    "The following code block will download some additional TTS files used for deployment. This will include the following files:  \n",
    "- ARPAbet dictionary file\n",
    "- IPA dictionary file\n",
    "- abbreviation mapping file\n",
    "- two text normalization (TN) files\n",
    "  - tokenize_and_classify.far\n",
    "  - verbalize.far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69861f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "riva_aux_files = ngc_download_and_get_dir(\"nvidia/riva/speechsynthesis_en_us_auxiliary_files:deployable_v1.3\", riva_aux_files, \"riva_aux_files\")\n",
    "riva_tn_files = ngc_download_and_get_dir(\"nvidia/riva/normalization_en_us:deployable_v1.1\", riva_tn_files, \"riva_tn_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab4ef3",
   "metadata": {},
   "source": [
    "---\n",
    "## Run riva-build\n",
    "Stop running Docker, run `riva_servicemaker`, and run again with the necessary paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72a1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run the riva servicemaker.\n",
    "!docker stop riva_rmir_gen &> /dev/null\n",
    "!set -x && docker run -td --gpus all --rm -v {str(riva_aux_files.resolve())}:/riva_aux \\\n",
    "            -v {str(acoustic_model.parent.resolve())}/:/synt \\\n",
    "            -v {str(vocoder.parent.resolve())}:/voc -v {str(riva_tn_files.resolve())}:/riva_tn \\\n",
    "            -v {str(rmir_dir.resolve())}:/data --name riva_rmir_gen --entrypoint=\"/bin/bash\" {riva_init_image}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58db5092",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "    Using <b>--force</b> tag in <b>riva-build</b> this will replace any existing RMIR.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9897d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.warn(\"Using --force in riva-build will replace any existing RMIR.\")\n",
    "riva_build=(\n",
    "    f\"riva-build speech_synthesis --force --voice_name={voice} --language_code={lang} \"\n",
    "    f\"--sample_rate={sample_rate} /data/FastPitch_HifiGan.rmir:{key} /synt/{str(acoustic_model.name)}:{key} \"\n",
    "    f\"/voc/{str(vocoder.name)}:{key} --abbreviations_file=/riva_aux/abbr.txt \"\n",
    "    f\"--wfst_tokenizer_model=/riva_tn/tokenize_and_classify.far --wfst_verbalizer_model=/riva_tn/verbalize.far\"\n",
    ")\n",
    "if target_machine==\"arm\":\n",
    "    riva_build += \"\"\"--max_batch_size 1 --postprocessor.max_batch_size 1 --preprocessor.max_batch_size 1 \\\n",
    "                --encoderFastPitch.max_batch_size 1 --chunkerFastPitch.max_batch_size 1 --hifigan.max_batch_size 1\"\"\"\n",
    "if use_ipa.lower() in [\"y\", \"yes\"]:\n",
    "    riva_build+=\" --phone_set=ipa --phone_dictionary_file=/riva_aux/ipa_cmudict-0.7b_nv22.08.txt --upper_case_chars=True\"\n",
    "else:\n",
    "    riva_build+=\" --phone_set=arpabet --phone_dictionary_file=/riva_aux/cmudict-0.7b_nv22.08\"\n",
    "if num_speakers > 1:\n",
    "    riva_build+=f\" --num_speakers={num_speakers}\"\n",
    "    riva_build+=\" --subvoices \" + \",\".join([f\"{i}:{i}\" for i in range(num_speakers)])\n",
    "print(riva_build)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7988d2",
   "metadata": {},
   "source": [
    "Execute the riva build command and stop the riva_servicemaker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec riva_rmir_gen {riva_build}\n",
    "!docker stop riva_rmir_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e170f",
   "metadata": {},
   "source": [
    "---\n",
    "## Run riva-deploy\n",
    "\n",
    "So far in this tutorial, we have learned how to generate RMIR files from .riva files. We would see that a `FastPitch_HifiGan.rmir` has been generated in the `${out_dir}/rmir` location we defined earlier.  \n",
    "\n",
    "The RMIR file generated in this tutorial can be deployed using [riva_quickstart](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html).\n",
    "\n",
    "### Steps to deploy the RMIR\n",
    "- Download the Riva Quick Start resource\n",
    "- Open `config.sh` and update the following params:  \n",
    "    - set `service_enabled_asr` to `false`.  \n",
    "    - set `service_enabled_nlp` to `false`.  \n",
    "    - set `service_enabled_tts` to `true`.  \n",
    "    - `riva_model_loc` to the location of your `out_dir`.  \n",
    "    - set `use_existing_rmirs` to `true`.  \n",
    "- run `riva_init.sh`.  \n",
    "- run `riva_start.sh`.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52f044f",
   "metadata": {},
   "source": [
    "Let's download the Riva Quick Start resource from NGC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b31c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_machine.lower() in [\"amd64\", \"arm64_linux\"]:\n",
    "    quickstart_link = f\"nvidia/riva/riva_quickstart:{__riva_version__}\"\n",
    "else:\n",
    "    quickstart_link = f\"nvidia/riva/riva_quickstart_arm64:{__riva_version__}\"\n",
    "\n",
    "quickstart_dir = ngc_download_and_get_dir(quickstart_link, None, None, resource_type=\"resource\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717e0c93",
   "metadata": {},
   "source": [
    "Next, we modify the `config.sh` file to enable the relevant Riva services (TTS in this case for FastPitch and HiFi-GAN), and provide the encryption key and path to the model repository (riva_model_loc) generated in the previous step.\n",
    "\n",
    "For example, if the above model repository is generated at `${out_dir}/rmir`, then you can specify `riva_model_loc` as the same directory as `${out_dir}/rmir`\n",
    "\n",
    "Here is how the `config.sh` should look:\n",
    "### config.sh snippet  \n",
    "    # Enable or Disable Riva Services \n",
    "    service_enabled_asr=false                                                      ## MAKE CHANGES HERE  \n",
    "    service_enabled_nlp=false                                                      ## MAKE CHANGES HERE  \n",
    "    service_enabled_tts=true                                                     ## MAKE CHANGES HERE  \n",
    "\n",
    "    # Specify one or more GPUs to use\n",
    "    # specifying more than one GPU is currently an experimental feature, and may result in undefined behaviours.\n",
    "    gpus_to_use=\"device=0\"\n",
    "\n",
    "    # Specify the encryption key to use to deploy models\n",
    "    MODEL_DEPLOY_KEY=\"tlt_encode\"                                                  ## MAKE CHANGES HERE\n",
    "\n",
    "    # Locations to use for storing models artifacts\n",
    "    #\n",
    "    # If an absolute path is specified, the data will be written to that location\n",
    "    # Otherwise, a docker volume will be used (default).\n",
    "    #\n",
    "    # riva_init.sh will create a `rmir` and `models` directory in the volume or\n",
    "    # path specified. \n",
    "    #\n",
    "    # RMIR ($riva_model_loc/rmir)\n",
    "    # Riva uses an intermediate representation (RMIR) for models\n",
    "    # that are ready to deploy but not yet fully optimized for deployment. Pretrained\n",
    "    # versions can be obtained from NGC (by specifying NGC models below) and will be\n",
    "    # downloaded to $riva_model_loc/rmir by `riva_init.sh`\n",
    "    # \n",
    "    # Custom models produced by NeMo and prepared using riva-build\n",
    "    # may also be copied manually to this location $(riva_model_loc/rmir).\n",
    "    #\n",
    "    # Models ($riva_model_loc/models)\n",
    "    # During the riva_init process, the RMIR files in $riva_model_loc/rmir\n",
    "    # are inspected and optimized for deployment. The optimized versions are\n",
    "    # stored in $riva_model_loc/models. The riva server exclusively uses these\n",
    "    # optimized versions.\n",
    "    riva_model_loc=\"<add path>\"                              ## MAKE CHANGES HERE (Replace with MODEL_LOC)    \n",
    "\n",
    "    # The default RMIRs are downloaded from NGC by default in the above $riva_rmir_loc directory\n",
    "    # If you'd like to skip the download from NGC and use the existing RMIRs in the $riva_rmir_loc\n",
    "    # then set the below $use_existing_rmirs flag to true. You can also deploy your set of custom\n",
    "    # RMIRs by keeping them in the riva_rmir_loc dir and use this quickstart script with the\n",
    "    # below flag to deploy them all together.\n",
    "    use_existing_rmirs=false                                ## MAKE CHANGES HERE (Set to true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55bd8d",
   "metadata": {},
   "source": [
    "Let's make the necessary changes to the `config.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494f42c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{quickstart_dir}/config.sh\", \"r\") as config_in:\n",
    "    config_file = config_in.readlines()\n",
    "\n",
    "for i, line in enumerate(config_file):\n",
    "    # Disable services\n",
    "    if \"service_enabled_asr\" in line:\n",
    "        config_file[i] = \"service_enabled_asr=false\\n\"\n",
    "    elif \"service_enabled_nlp\" in line:\n",
    "        config_file[i] = \"service_enabled_nlp=false\\n\"\n",
    "    elif \"service_enabled_nmt\" in line:\n",
    "        config_file[i] = \"service_enabled_nmt=false\\n\"\n",
    "    elif \"service_enabled_tts\" in line:\n",
    "        config_file[i] = \"service_enabled_tts=true\\n\"\n",
    "    # Update riva_model_loc to our rmir folder\n",
    "    elif \"riva_model_loc\" in line:\n",
    "        config_file[i] = config_file[i].split(\"riva_model_loc\")[0]+f\"riva_model_loc={out_dir}\\n\"\n",
    "    elif \"use_existing_rmirs\" in line:\n",
    "        config_file[i] = \"use_existing_rmirs=true\\n\"\n",
    "    elif \"MODEL_DEPLOY_KEY\" in line:\n",
    "        config_file[i] = f\"MODEL_DEPLOY_KEY=\\\"{key}\\\"\\n\"\n",
    "    elif \"fastpitch\" in line:\n",
    "        config_file[i] = f\"#{line}\"\n",
    "\n",
    "with open(f\"{quickstart_dir}/config.sh\", \"w\") as config_in:\n",
    "    config_in.writelines(config_file)\n",
    "\n",
    "print(\"\".join(config_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add4aa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have permission to execute these scripts\n",
    "! cd {quickstart_dir} && chmod +x ./riva_init.sh && chmod +x ./riva_start.sh && chmod +x ./riva_stop.sh\n",
    "! cd {quickstart_dir} && ./riva_stop.sh config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec024539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run `riva_init.sh`. This will fetch the containers/models and run `riva-deploy`.\n",
    "# YOU CAN SKIP THIS STEP IF YOU DID RIVA DEPLOY\n",
    "! cd {quickstart_dir} && ./riva_init.sh config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf577dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run `riva_start.sh`. This will start the Riva server and serve your model.\n",
    "! cd {quickstart_dir} && ./riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a07a50",
   "metadata": {},
   "source": [
    "# Run Inference\n",
    "Once the Riva server is up and running with your models, you can send inference requests querying the server.\n",
    "\n",
    "To send gRPC requests, install the Riva Python API bindings for the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f1c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install client API bindings\n",
    "! pip install nvidia-riva-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d1f021",
   "metadata": {},
   "source": [
    "### Connect to the Riva server and run inference\n",
    "Now, we can query the Riva server; let’s get started. The following cell queries the Riva server (using gRPC) to yield a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0fc79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import riva.client\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "\n",
    "server = \"localhost:50051\"                # location of riva server\n",
    "auth = riva.client.Auth(uri=server)\n",
    "tts_service = riva.client.SpeechSynthesisService(auth)\n",
    "\n",
    "\n",
    "text = \"Is it recognize speech or wreck a nice beach?\"\n",
    "language_code = lang                   # currently required to be \"en-US\"\n",
    "sample_rate_hz = sample_rate                    # the desired sample rate\n",
    "voice_name = voice      # subvoice to generate the audio output.\n",
    "data_type = np.int16                      # For RIVA version < 1.10.0 please set this to np.float32\n",
    "\n",
    "resp = tts_service.synthesize(text, voice_name=voice_name, language_code=language_code, sample_rate_hz=sample_rate_hz)\n",
    "audio = resp.audio\n",
    "meta = resp.meta\n",
    "processed_text = meta.processed_text\n",
    "predicted_durations = meta.predicted_durations\n",
    "\n",
    "audio_samples = np.frombuffer(resp.audio, dtype=data_type)\n",
    "print(processed_text)\n",
    "ipd.Audio(audio_samples, rate=sample_rate_hz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
