{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to improve accuracy on specific speech patterns by fine-tuning the Acoustic Model (Citrinet) in the Riva ASR pipeline \n",
    "\n",
    "This tutorial walks you through some of the advanced customization features of the Riva ASR pipeline by fine-tuning the Acoustic Model (Citrinet). These customization features improve accuracy on specific speech patterns, like background noise and different acoustic environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVIDIA Riva Overview\n",
    "\n",
    "NVIDIA Riva is a GPU-accelerated SDK for building Speech AI applications that are customized for your use case and deliver real-time performance. <br/>\n",
    "Riva offers a rich set of speech and natural language understanding services such as:\n",
    "\n",
    "- Automated speech recognition (ASR)\n",
    "- Text-to-Speech synthesis (TTS)\n",
    "- A collection of natural language processing (NLP) services, such as named entity recognition (NER), punctuation, and intent classification.\n",
    "\n",
    "In this tutorial, we will customize the Riva ASR pipeline by fine-tuning the Acoustic Model (Citrinet) with NVIDIA's TAO Toolkit to improve accuracy on audio with background noise.  \n",
    "To understand the basics of Riva ASR APIs, refer to [Getting started with Riva ASR in Python](https://github.com/nvidia-riva/tutorials/blob/dev/22.04/asr-python-basics.ipynb). <br>\n",
    "\n",
    "For more information about Riva, refer to the [Riva developer documentation](https://developer.nvidia.com/riva)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Riva Acoustic Model (Citrinet) with NVIDIA TAO\n",
    "\n",
    "The following flow diagram shows the Riva speech recognition pipeline along with all the possible customizations. \n",
    "\n",
    "Raw temporal audio signals first pass through a feature extraction block which segments the data into blocks (for example, of 80 ms each), then converts the blocks from temporal domain to frequency domain (MFCC). This data is then fed into an acoustic model which outputs probabilities over text tokens at each time step. A decoder converts this matrix of probabilities into a sequence of text tokens which is then `detokenized` into an actual sentence (or character sequence). An advanced decoder can also do beam search and score multiple possible hypotheses (i.e. sentences) in conjunction with a language model. The decoder output comes without punctuation and capitalization, which is the job of the Punctuation and Capitalization model. Finally, Inverse Text Normalization (ITN) rules are applied to transform the text in verbal format into a desired written format.\n",
    "\n",
    "<img src=\"./imgs/riva-asr-customizations-amfinetuning.PNG\" style=\"float: center;\">\n",
    "\n",
    "For this tutorial, we need to fine-tune the pre-trained Riva acoustic model. \n",
    "\n",
    "There are multiple options available for the acoustic model with Riva - Conformer-CTC, Citrinet, Jasper, and Quartznet. In this tutorial we are going to use the Citrinet model and demonstrate how it can be fine-tuned.  \n",
    "Fine-tuning a Conformer-CTC model is not yet supported. Support for this is planned in a future release.    \n",
    "For more information about these acoustic models and when to use them, refer to the Riva documentation [here](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/reference/models/asr.html).\n",
    "\n",
    "You can use NVIDIA TAO Toolkit to fine-tune the Citrinet acoustic model in the Riva ASR pipeline.\n",
    "\n",
    "#### NVIDIA TAO Toolkit Overview\n",
    "\n",
    "NVIDIA Train Adapt Optimize (TAO) Toolkit is a python-based AI toolkit for transfer learning that takes purpose-built pre-trained AI models and customizes them on your own data. TAO enables developers with limited AI expertise to create highly accurate AI models for production deployments.  \n",
    "TAO follows zero coding paradigm. There is no need to write any code to train models with TAO. Training can be done by just running a few commands with the TAO command-line interface.  \n",
    "\n",
    "Riva supports fine-tuning with TAO. The fine-tuned TAO model can easily be deployed for real-time inference on the Riva Speech Skills server.\n",
    "\n",
    "For more information about the NVIDIA TAO framework, refer to the documentation [here](https://docs.nvidia.com/tao/tao-toolkit/text/overview.html).\n",
    "\n",
    "### Fine-tune the Citrinet model with NVIDIA TAO:\n",
    "\n",
    "The process of fine-tuning a Riva Citrinet acoustic model with NVIDIA TAO can be split into three steps:\n",
    "1. Data preprocessing.\n",
    "2. Fine-tuning the Citrinet model with TAO.\n",
    "3. Deploying the fine-tuned Citrinet TAO model on the Riva Speech Skills server.\n",
    "Let's walk through each of these steps in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Data preprocessing\n",
    "\n",
    "For fine-tuning we need audio data with background noise. If you already have such data, then you can use it directly.  \n",
    "In this tutorial, we will take the AN4 dataset and augment it with noise data from the Room Impulse Response and Noise Database from the [openslr database](https://www.openslr.org/28/).\n",
    "NVIDIA TAO Toolkit does not currently support audio data augmentation. This support will be added in a future release.\n",
    "In this tutorial, we will be using NVIDIA NeMo for the data preprocessing step.\n",
    "\n",
    "#### NVIDIA NeMo Overview\n",
    "\n",
    "NVIDIA NeMo is a toolkit for building new state-of-the-art conversational AI models. NeMo has separate collections for Automatic Speech Recognition (ASR), Natural Language Processing (NLP), and Text-to-Speech (TTS) models. Each collection consists of prebuilt modules that include everything needed to train on your data. Every module can easily be customized, extended, and composed to create new conversational AI model architectures.\n",
    "For more information about NeMo, refer to the [NeMo product page](https://developer.nvidia.com/nvidia-nemo) and [documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/starthere/intro.html). The open-source NeMo repository can be found [here](https://github.com/NVIDIA/NeMo).\n",
    "\n",
    "NVIDIA NeMo and NVIDIA TAO are both training toolkits. TAO abstracts the training details from the user, whereas NeMo exposes them. TAO follows the zero-coding paradigm, therefore, TAO is better suited for users who want to quickly fine-tune models on their custom dataset. NeMo is the preferred option for researches.  \n",
    "TAO is the preferred training toolkit for Riva because of it's ease-of-use.\n",
    "\n",
    "In this tutorial, we will be using NeMo only for data preprocessing. We will use the TAO Toolkit for the actual training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements and setup for data preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using NeMo for this data preprocessing step - Easiest way to get NeMo is to use NeMo's docker container. If you are not already running this notebook in the NeMo docker container, please follow instructions here to restart this tutorial from NeMo docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install sox and ffmpeg.\n",
    "!apt-get update && apt-get install -y sox ffmpeg\n",
    "\n",
    "# 2. Install Cython and Pytorch. These libraries are needed for NeMo.\n",
    "!pip install Cython\n",
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "\n",
    "# 3. Install unicode and wget. We will need wget to download the datasets.\n",
    "!pip install unidecode\n",
    "!pip install wget\n",
    "#!pip install matplotlib>=3.3.2\n",
    "\n",
    "# 4. Clone and install NeMo.\n",
    "BRANCH = 'main'\n",
    "!git clone -b $BRANCH https://github.com/NVIDIA/NeMo.git\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@{BRANCH}#egg=nemo_toolkit[all]\n",
    "\"\"\"\n",
    "Remember to restart the runtime for the kernel to pick up any upgraded packages (e.g. matplotlib)!\n",
    "Alternatively, you can uncomment the exit() below to crash and restart the kernel, in the case\n",
    "that you want to use the \"Run All Cells\" (or similar) option.\n",
    "\"\"\"\n",
    "# exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For alternate options to install NeMo, refer to the NeMo repository [here](https://github.com/NVIDIA/NeMo#installation).  \n",
    "\n",
    "#### Download and process the AN4 dataset\n",
    "AN4 is a small dataset recorded and distributed by Carnegie Mellon University (CMU). It consists of recordings of people spelling out addresses, names, etc. Information about this dataset can be found on the official CMU site.\n",
    "\n",
    "Let's download the AN4 dataset tar file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [........................................................................] 64327561 / 64327561AN4 dataset downloaded at: /tutorials/am_finetuning/an4_sphere.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# This is the working directory for this tutorial. \n",
    "BRANCH = 'main'\n",
    "working_dir = 'am_finetuning/'\n",
    "!mkdir -p $working_dir\n",
    "\n",
    "# Import the necessary dependencies.\n",
    "import wget\n",
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "import tarfile\n",
    "\n",
    "# The AN4 directory will be created in `data_dir`. It is currently set to the `working_dir`.\n",
    "data_dir = os.path.abspath(working_dir)\n",
    "\n",
    "# Download the AN4 dataset if it doesn't already exist in `data_dir`. \n",
    "# This will take a few moments...\n",
    "# We also set `an4_path` which points to the downloaded an4 dataset\n",
    "if not os.path.exists(data_dir + '/an4_sphere.tar.gz'):\n",
    "    an4_url = 'https://dldata-public.s3.us-east-2.amazonaws.com/an4_sphere.tar.gz'\n",
    "    an4_path = wget.download(an4_url, data_dir)\n",
    "    print(f\"AN4 dataset downloaded at: {an4_path}\")\n",
    "else:\n",
    "    print(\"AN4 dataset tarfile already exists. Proceed to the next step.\")\n",
    "    an4_path = data_dir + '/an4_sphere.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's untar the tar file to give us the dataset audio files in `.sph` format. Then, we'll convert the `.sph` files to 16kHz `.wav` files using the SoX library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed untarring the an4 tarfile\n",
      "Converting .sph to .wav...\n",
      "Finished converting the .sph files to .wav files\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(data_dir + '/an4/'):\n",
    "    # Untar\n",
    "    tar = tarfile.open(an4_path)\n",
    "    tar.extractall(path=data_dir)\n",
    "    print(\"Completed untarring the an4 tarfile\")\n",
    "    # Convert .sph to .wav (using sox)\n",
    "    print(\"Converting .sph to .wav...\")\n",
    "    sph_list = glob.glob(data_dir + '/an4/**/*.sph', recursive=True)\n",
    "    for sph_path in sph_list:\n",
    "        wav_path = sph_path[:-4] + '.wav'\n",
    "        #converting to 16kHz wav\n",
    "        cmd = [\"sox\", sph_path, \"-r\", \"16000\", wav_path]\n",
    "        subprocess.run(cmd)\n",
    "    print(\"Finished converting the .sph files to .wav files\")\n",
    "else:\n",
    "    print(\"Can't find the an4 dataset directory. Please download the dataset first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's build the manifest files for the AN4 dataset. The manifest file is a `.json` file that maps the `.wav` clip to its corresponding text.\n",
    "\n",
    "Each entry in the AN4 dataset's manifest `.json` file follows the template:  \n",
    "`{\"audio_filepath\": \"<.wav file location>\", \"duration\": <duration of the .wav file>, \"text\": \"<text from the .wav file>\"}`  \n",
    "Example: `{\"audio_filepath\": \"/tutorials/am_finetuning/an4/wav/an4_clstk/fash/an251-fash-b.wav\", \"duration\": 1.0, \"text\": \"yes\"}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Building manifest files***\n",
      "Training manifest created at /tutorials/am_finetuning/an4/train_manifest.json\n",
      "Test manifest created at /tutorials/am_finetuning/an4/test_manifest.json\n",
      "***Done***\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries.\n",
    "import json\n",
    "import librosa\n",
    "\n",
    "# Method to build a manifest.\n",
    "def build_manifest(transcripts_path, manifest_path, wav_path):\n",
    "    with open(transcripts_path, 'r') as fin:\n",
    "        with open(manifest_path, 'w') as fout:\n",
    "            for line in fin:\n",
    "                # Lines look like this:\n",
    "                # <s> transcript </s> (fileID)\n",
    "                transcript = line[: line.find('(')-1].lower()\n",
    "                transcript = transcript.replace('<s>', '').replace('</s>', '')\n",
    "                transcript = transcript.strip()\n",
    "\n",
    "                file_id = line[line.find('(')+1 : -2]  # e.g. \"cen4-fash-b\"\n",
    "                audio_path = os.path.join(\n",
    "                    data_dir, wav_path,\n",
    "                    file_id[file_id.find('-')+1 : file_id.rfind('-')],\n",
    "                    file_id + '.wav')\n",
    "\n",
    "                duration = librosa.core.get_duration(filename=audio_path)\n",
    "\n",
    "                # Write the metadata to the manifest\n",
    "                metadata = {\n",
    "                    \"audio_filepath\": audio_path,\n",
    "                    \"duration\": duration,\n",
    "                    \"text\": transcript\n",
    "                }\n",
    "                json.dump(metadata, fout)\n",
    "                fout.write('\\n')\n",
    "                \n",
    "# Building the manifest files.\n",
    "print(\"***Building manifest files***\")\n",
    "\n",
    "# Building manifest files for the training data\n",
    "train_transcripts = data_dir + '/an4/etc/an4_train.transcription'\n",
    "train_manifest = data_dir + '/an4/train_manifest.json'\n",
    "if not os.path.isfile(train_manifest):\n",
    "    build_manifest(train_transcripts, train_manifest, 'an4/wav/an4_clstk')\n",
    "    print(\"Training manifest created at\", train_manifest)\n",
    "\n",
    "# Building manifest files for the test data\n",
    "test_transcripts = data_dir + '/an4/etc/an4_test.transcription'\n",
    "test_manifest = data_dir + '/an4/test_manifest.json'\n",
    "if not os.path.isfile(test_manifest):\n",
    "    build_manifest(test_transcripts, test_manifest, 'an4/wav/an4test_clstk')\n",
    "    print(\"Test manifest created at\", test_manifest)\n",
    "\n",
    "print(\"***Done***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and process the background noise dataset\n",
    "\n",
    "For background noise, we will use the background noise samples from the Room Impulse Response and Noise database from the openslr database. For each 30 second isotropic noise sample in the dataset we use the first 15 seconds for training and the last 15 seconds for evaluation.\n",
    "\n",
    "Let's first download this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [....................................................................] 1311166223 / 1311166223Background noise dataset download complete.\n"
     ]
    }
   ],
   "source": [
    "# Download the background noise dataset if it doesn't already exist in `data_dir`. \n",
    "# This will take a few moments...\n",
    "# We also set `noise_path` which points to the downloaded background noise dataset.\n",
    "\n",
    "if not os.path.exists(data_dir + '/rirs_noises.zip'):\n",
    "    slr28_url = 'https://www.openslr.org/resources/28/rirs_noises.zip'\n",
    "    noise_path = wget.download(slr28_url, data_dir)\n",
    "    print(\"Background noise dataset download complete.\")\n",
    "else:\n",
    "    print(\"Background noise dataset already exists. Please proceed to the next step.\")\n",
    "    noise_path = data_dir + '/rirs_noises.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to untar the tar file, which gives us the dataset audio files in `.sph` format. Then, we convert the `.sph` files to 16kHz `.wav` files using the SoX library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting noise data complete\n"
     ]
    }
   ],
   "source": [
    "# Extract noise data\n",
    "from zipfile import ZipFile\n",
    "try:\n",
    "    with ZipFile(noise_path, \"r\") as zipObj:\n",
    "        zipObj.extractall(data_dir)\n",
    "        print(\"Extracting noise data complete\")\n",
    "except Exception:\n",
    "    print(\"Not extracting. Extracted noise data might already exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's build the manifest files for the noise data. The manifest file is a `.json` file that maps the `.wav` clip to its corresponding text.\n",
    "\n",
    "Each entry in the noise data's manifest `.json` file follows the template:  \n",
    "`{\"audio_filepath\": \"<.wav file location>\", \"duration\": <duration of the .wav file>, \"offset\": <offset value>, \"text\": \"-\"}`  \n",
    "Example: `{\"audio_filepath\": \"/tutorials/am_finetuning/RIRS_NOISES/real_rirs_isotropic_noises/RVB2014_type1_noise_largeroom1_1.wav\", \"duration\": 30.0, \"offset\": 0, \"text\": \"-\"}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing manifest file to /tutorials/am_finetuning/test_noise.json complete\n",
      "Writing manifest file to /tutorials/am_finetuning/train_noise.json complete\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "iso_path = os.path.join(data_dir,\"RIRS_NOISES/real_rirs_isotropic_noises\")\n",
    "iso_noise_list = os.path.join(iso_path, \"noise_list\")\n",
    "\n",
    "# Create the manifest files from noise files\n",
    "def process_row(row, offset, duration):\n",
    "  try:\n",
    "    entry = {}\n",
    "    wav_f = row['wav_filename']\n",
    "    newfile = wav_f\n",
    "    duration = subprocess.check_output('soxi -D {0}'.format(newfile), shell=True)\n",
    "    entry['audio_filepath'] = newfile\n",
    "    entry['duration'] = float(duration)\n",
    "    entry['offset'] = offset\n",
    "    entry['text'] = row['transcript']\n",
    "    return entry\n",
    "  except Exception as e:\n",
    "    wav_f = row['wav_filename']\n",
    "    newfile = wav_f\n",
    "    print(f\"Error processing {newfile} file!!!\")\n",
    "    \n",
    "train_rows = []\n",
    "test_rows = []\n",
    "\n",
    "with open(iso_noise_list,\"r\") as in_f:\n",
    "    for line in in_f:\n",
    "        row = {}\n",
    "        data = line.rstrip().split()\n",
    "        row['wav_filename'] = os.path.join(data_dir,data[-1])\n",
    "        row['transcript'] = \"-\"\n",
    "        train_rows.append(process_row(row, 0 , 15))\n",
    "        test_rows.append(process_row(row, 15 , 15))\n",
    "\n",
    "# Writing manifest files\n",
    "def write_manifest(manifest_file, manifest_lines):\n",
    "    with open(manifest_file, 'w') as fout:\n",
    "      for m in manifest_lines:\n",
    "        fout.write(json.dumps(m) + '\\n')\n",
    "      print(\"Writing manifest file to\", manifest_file, \"complete\")\n",
    "\n",
    "# Writing training and test manifest files\n",
    "test_noise_manifest = os.path.join(data_dir, \"test_noise.json\")\n",
    "train_noise_manifest = os.path.join(data_dir, \"train_noise.json\")\n",
    "write_manifest(test_noise_manifest, test_rows)\n",
    "write_manifest(train_noise_manifest, train_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the noise-augmented dataset\n",
    "\n",
    "Finally, let's create a noise-augmented dataset by adding noise to the the AN4 dataset with the `add_noise.py` NeMo script. This script generates the noise-augmented audio clips as well as the manifest files. \n",
    "\n",
    "Each entry in the noise-augmented data's manifest file follows the template:  \n",
    "`{\"audio_filepath\": \"<.wav file location>\", \"duration\": <duration of the .wav file>, \"text\": \"<text from the .wav file>\"}`\n",
    "Example: `{\"audio_filepath\": \"/tutorials/am_finetuning/noise_data/train_manifest/train_noise_0db/an251-fash-b.wav\", \"duration\": 1.0, \"text\": \"yes\"}`\n",
    "\n",
    "##### Training dataset\n",
    "Let's create a noise-augmented training dataset using the AN4 training dataset. We'll add noise at different SNRS ranging from 0 to 15 dB SNR using a NeMo script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_dir = data_dir + '/noise_data'\n",
    "\n",
    "run = f\"python NeMo/scripts/dataset_processing/add_noise.py \\\n",
    "    --input_manifest={train_manifest} \\\n",
    "    --noise_manifest={train_noise_manifest} \\\n",
    "    --snrs 0 5 10 15 \\\n",
    "    --out_dir={final_data_dir}\"\n",
    "\n",
    "!{run}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above script generates a .json manifest file each for every SNR value, i.e., one manifest file each for 0, 5, 10 and 15db SNR.  \n",
    "Let's combine all the manifest into a single manifest for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise-augmented training dataset created at /tutorials/am_finetuning/noise_data/noisy_train.json\n"
     ]
    }
   ],
   "source": [
    "run = f\"cat {data_dir}/noise_data/manifests/train* >{final_data_dir}/noisy_train.json\"\n",
    "!{run}\n",
    "\n",
    "print(\"Noise-augmented training dataset created at\", final_data_dir + \"/noisy_train.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test dataset\n",
    "\n",
    "Let's create a noise-augmented evaluation dataset using the an4 test dataset, by adding noise at 5 dB, using a NeMo script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmention - Add noise to test set.\n",
    "run = f\"python NeMo/scripts/dataset_processing/add_noise.py \\\n",
    "    --input_manifest={test_manifest} \\\n",
    "    --noise_manifest={test_noise_manifest} \\\n",
    "    --snrs=0 \\\n",
    "    --out_dir={final_data_dir}\"\n",
    "\n",
    "!{run}\n",
    "\n",
    "print(\"Noise-augmented testing dataset created at\", final_data_dir+\"/test_manifest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, step 1 of 3, the data preprocessing step is complete. Now onto the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Fine-tuning the Citrinet model with TAO.\n",
    "Proceed to this tutorial to fine-tune the Citrinet model with TAO.\n",
    "\n",
    "### Step 3. Deploying the fine-tuned Citrinet TAO model on the Riva Speech Skills server.\n",
    "Proceed to this tutorial to deploy the fine-tuned Citrinet TAO model on the Riva Speech Skills server for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ASR_with_NeMo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "venv-riva-tutorials",
   "language": "python",
   "name": "venv-riva-tutorials"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
