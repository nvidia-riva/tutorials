{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NpvEOMs_mKp"
   },
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/rivaasrasr-finetuning-conformer-ctc-nemo/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Training and Deploying N-GPU Language Models for Parakeet RNNT with NVIDIA NIM\n",
    "\n",
    "This comprehensive tutorial demonstrates how to train and deploy an NVIDIA N-GPU Language Model (LM) for Parakeet RNNT acoustic models using NVIDIA NeMo and deploy them as NVIDIA NIM (NVIDIA Inference Microservices). You'll learn the complete pipeline from data preparation to model deployment and inference.\n",
    "\n",
    "## What You'll Learn\n",
    "- How to train n-gram language models using NeMo and KenLM for Parakeet RNNT models\n",
    "- How to integrate language models with Parakeet RNNT acoustic models\n",
    "- How to deploy custom models using NVIDIA Riva NIM\n",
    "- How to perform inference with your deployed models\n",
    "\n",
    "## Prerequisites\n",
    "- Basic understanding of automatic speech recognition (ASR)\n",
    "- Familiarity with Python and Jupyter notebooks\n",
    "- Access to NVIDIA GPU resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6h7SXF6G_mKw"
   },
   "source": [
    "## NVIDIA Riva NIM Overview\n",
    "\n",
    "**NVIDIA Riva NIM (NVIDIA Inference Microservices)** provides enterprise-grade, production-ready AI services with optimized performance and scalability. The Riva ASR NIM specifically offers:\n",
    "\n",
    "### Key Features\n",
    "- **Multi-language Support**: State-of-the-art automatic speech recognition models for multiple languages\n",
    "- **GPU Acceleration**: Built on NVIDIA's software platform with CUDA, TensorRT, and Triton integration\n",
    "- **Production Ready**: Optimized for enterprise deployment with high throughput and low latency\n",
    "- **Easy Integration**: Simple REST and gRPC APIs for seamless application integration\n",
    "- **Custom Model Support**: Ability to deploy your own trained models\n",
    "\n",
    "### Architecture Benefits\n",
    "- **Containerized Deployment**: Easy deployment using Docker containers\n",
    "- **Model Optimization**: Automatic model optimization for target hardware\n",
    "- **Real-time Processing**: Support for both streaming and batch inference\n",
    "\n",
    "In this tutorial, we'll deploy a custom Parakeet RNNT model with an n-gram language model to demonstrate the complete workflow from training to production deployment.\n",
    "\n",
    "For comprehensive documentation, visit the [Riva NIM documentation](https://docs.nvidia.com/nim/riva/asr/latest/overview.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rv47EmBp_mK2"
   },
   "source": [
    "## NVIDIA NeMo Framework\n",
    "\n",
    "**[NVIDIA NeMo](https://developer.nvidia.com/nvidia-nemo)** is a powerful, open-source framework designed for building, training, and fine-tuning GPU-accelerated conversational AI models. NeMo provides a comprehensive toolkit for:\n",
    "\n",
    "### Core Capabilities\n",
    "- **Speech AI**: Automatic Speech Recognition (ASR), Text-to-Speech (TTS), and Voice Activity Detection (VAD)\n",
    "- **Multi-GPU Training**: Distributed training across multiple GPUs for faster model development\n",
    "- **Pre-trained Models**: Access to state-of-the-art pre-trained models for quick prototyping\n",
    "- **Model Export**: Easy export to various deployment formats for Riva NIM\n",
    "\n",
    "### Why NeMo for This Tutorial?\n",
    "- **Language Model Training**: Built-in support for n-gram language model training with KenLM integration\n",
    "- **Model Integration**: Seamless integration between acoustic models and language models\n",
    "- **Production Deployment**: Direct export capabilities to Riva NIM format\n",
    "- **Research to Production**: Smooth transition from research experiments to production deployment\n",
    "\n",
    "### Getting Started\n",
    "For detailed setup instructions and comprehensive documentation, visit the [NeMo GitHub repository](https://github.com/NVIDIA/NeMo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding N-gram Language Models\n",
    "\n",
    "Language models are crucial components in automatic speech recognition systems, helping to improve accuracy by incorporating linguistic knowledge. There are two primary approaches to language modeling:\n",
    "\n",
    "### N-gram Language Models\n",
    "**N-gram models** are statistical language models that predict the next word based on the previous `n-1` words. They work by:\n",
    "\n",
    "- **Frequency Analysis**: Learning probability distributions from word sequence frequencies in training data\n",
    "- **Context Window**: Using a fixed context window of `n` words to make predictions\n",
    "- **Efficiency**: Providing fast inference with predictable computational requirements\n",
    "- **Scalability**: Offering a clear space-time tradeoff - larger `n` values capture more context but require more memory\n",
    "\n",
    "**Advantages:**\n",
    "- Simple and interpretable\n",
    "- Fast inference and training\n",
    "- Well-understood mathematical foundation\n",
    "- Excellent for domain-specific applications\n",
    "- Low computational overhead\n",
    "\n",
    "### Neural Language Models\n",
    "**Neural language models** use deep learning architectures (RNNs, Transformers, etc.) to model language:\n",
    "\n",
    "- **Superior Performance**: Generally achieve better language modeling capabilities\n",
    "- **Context Awareness**: Can capture long-range dependencies and complex patterns\n",
    "- **Computational Cost**: Require more computational resources for training and inference\n",
    "\n",
    "### Why N-gram Models for ASR?\n",
    "For speech recognition applications, n-gram models offer several practical advantages:\n",
    "- **Real-time Performance**: Fast enough for streaming ASR applications\n",
    "- **Resource Efficiency**: Lower memory and computational requirements\n",
    "- **Domain Adaptation**: Easy to retrain on domain-specific data\n",
    "- **Integration**: Seamless integration with existing ASR pipelines\n",
    "\n",
    "In this tutorial, we'll train a 6-gram language model using the KenLM toolkit integrated with NeMo, then deploy it as an N-GPU Language Model in NVIDIA Riva NIM for production use.\n",
    "\n",
    "For deeper understanding, refer to the [Stanford NLP course on n-gram models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1O2bGCMAQDs8"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can run either this tutorial locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Perform the following steps to setup in Google Colab:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub.\n",
    "   a. Click **File** > **Upload Notebook** > **GITHUB** tab > copy/paste the GitHub URL.\n",
    "3. Connect to an instance with a GPU.\n",
    "   a. Click **Runtime** > Change the runtime type > select **GPU** for the hardware accelerator.\n",
    "4. Run this cell to set up the dependencies.\n",
    "5. Restart the runtime.\n",
    "   a. Click **Runtime** > **Restart Runtime** for any upgraded packages to take effect.\n",
    "\"\"\"\n",
    "\n",
    "# Install Dependencies\n",
    "! pip install wget\n",
    "! apt-get install sox libsndfile1 ffmpeg libsox-fmt-mp3 jq\n",
    "! pip install text-unidecode\n",
    "! pip install matplotlib>=3.3.2\n",
    "! pip install Cython\n",
    "\n",
    "## Install NeMo\n",
    "BRANCH = 'v2.4.0'\n",
    "! python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n",
    "\n",
    "\"\"\"\n",
    "Remember to restart the runtime for the kernel to pick up any upgraded packages (e.g. matplotlib)!\n",
    "Alternatively, in the case where you want to use the \"Run All Cells\" (or similar) option,\n",
    "uncomment `exit()` below to crash and restart the kernel.\n",
    "\"\"\"\n",
    "# exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before starting this tutorial, ensure you have the following requirements in place:\n",
    "\n",
    "### 1. NVIDIA NGC Account\n",
    "- **NGC Account**: You need access to NVIDIA NGC (NVIDIA GPU Cloud) for downloading models and containers\n",
    "- **Authentication**: Ensure you're logged into your NGC account\n",
    "- **API Key**: Have your NGC API key ready for container downloads\n",
    "\n",
    "**Setup Instructions**: Follow the [NGC Getting Started Guide](https://docs.nvidia.com/ngc/ngc-overview/index.html#registering-activating-ngc-account) for account creation and authentication.\n",
    "\n",
    "### 2. System Requirements\n",
    "- **GPU**: NVIDIA GPU with CUDA support (recommended: RTX 3080 or better)\n",
    "- **Memory**: At least 16GB RAM (32GB recommended for large models)\n",
    "- **Storage**: 60GB+ free disk space for models and datasets\n",
    "- **Docker**: Docker with NVIDIA Container Toolkit installed\n",
    "\n",
    "### 3. Software Dependencies\n",
    "- **Python**: Python 3.8 or higher\n",
    "- **CUDA**: CUDA 12.8 or compatible version\n",
    "- **Docker**: Latest version with GPU support\n",
    "- **Git**: For cloning repositories\n",
    "\n",
    "### 4. Environment Setup\n",
    "- **Jupyter**: Jupyter Notebook or JupyterLab\n",
    "- **Virtual Environment**: Python virtual environment (recommended)\n",
    "- **Internet Connection**: Stable connection for downloading models and datasets\n",
    "\n",
    "**Note**: This tutorial can be run locally or on cloud platforms like Google Colab with GPU support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vo27Unex_mLG"
   },
   "source": [
    "---\n",
    "\n",
    "## Training an N-gram Language Model with NeMo\n",
    "\n",
    "This section covers the complete process of training an n-gram language model using NVIDIA NeMo and the KenLM toolkit. We'll walk through:\n",
    "\n",
    "1. **Environment Setup**: Installing required dependencies and tools\n",
    "2. **Data Preparation**: Processing the LibriSpeech dataset for training\n",
    "3. **Model Training**: Building the n-gram language model with KenLM\n",
    "4. **Model Integration**: Preparing the model for deployment with Parakeet RNNT\n",
    "\n",
    "The training process leverages KenLM, a highly optimized library for building and using n-gram language models, integrated seamlessly with NeMo's workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing the required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing NeMo and KenLM Dependencies\n",
    "\n",
    "This step performs the following operations:\n",
    "\n",
    "1. **Clone NeMo Repository**: Downloads the latest NeMo framework from GitHub\n",
    "2. **Install KenLM**: Builds and installs the KenLM toolkit for n-gram language model training\n",
    "3. **Setup Dependencies**: Installs all required libraries and tools\n",
    "\n",
    "**What is KenLM?**\n",
    "- **KenLM** is a highly optimized library for building and using n-gram language models\n",
    "- **Performance**: Provides fast training and inference for large-scale language models\n",
    "- **Integration**: Seamlessly integrates with NeMo's ASR pipeline\n",
    "- **Optimization**: Includes advanced pruning and quantization techniques\n",
    "\n",
    "**Installation Process:**\n",
    "- The build process may take 10-15 minutes depending on your system\n",
    "- Requires sufficient disk space for compilation\n",
    "- Automatically handles dependency resolution\n",
    "\n",
    "**Note**: This installation is required for the n-gram language model training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "NEMO_ROOT = \"NeMo\" # Path to clone the NeMo repository.\n",
    "os.environ[\"NEMO_ROOT\"] = NEMO_ROOT\n",
    "! git clone -b $BRANCH --single-branch https://github.com/NVIDIA/NeMo.git $NEMO_ROOT\n",
    "! cd $NEMO_ROOT/scripts/asr_language_modeling/ngram_lm/ && bash install_beamsearch_decoders.sh $NEMO_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfRpENJJ_mLy"
   },
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "### LibriSpeech Language Model Dataset\n",
    "\n",
    "For this tutorial, we'll use the **LibriSpeech Language Model dataset**, which is a widely-used benchmark dataset for training language models in speech recognition applications.\n",
    "\n",
    "#### Dataset Overview\n",
    "- **Source**: Derived from LibriVox audiobooks, providing high-quality, read speech\n",
    "- **Size**: Approximately 800 million words of training text\n",
    "- **Format**: Normalized text data suitable for language model training\n",
    "- **Language**: English\n",
    "- **License**: Public domain\n",
    "\n",
    "#### Why LibriSpeech LM?\n",
    "- **Quality**: High-quality, professionally read text\n",
    "- **Diversity**: Covers various topics and speaking styles\n",
    "- **Standard**: Widely used benchmark in ASR research\n",
    "- **Size**: Large enough for robust n-gram model training\n",
    "- **Compatibility**: Well-suited for integration with ASR systems\n",
    "\n",
    "#### Dataset Structure\n",
    "The dataset contains:\n",
    "- **Training Text**: Normalized text files for language model training\n",
    "- **Vocabulary**: Common English words and phrases\n",
    "- **Format**: Plain text files, one sentence per line\n",
    "\n",
    "**Download Links:**\n",
    "- [LibriSpeech LM Dataset](https://www.openslr.org/11/)\n",
    "- [Direct Download](https://www.openslr.org/resources/11/librispeech-lm-corpus.tgz)\n",
    "\n",
    "### Downloading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKrmQQuK_mLz"
   },
   "source": [
    "Now let's download the LibriSpeech Language Model dataset. This process will:\n",
    "\n",
    "1. **Download** the compressed dataset file (~1.4GB)\n",
    "2. **Extract** the text data for processing\n",
    "3. **Prepare** the data for language model training\n",
    "\n",
    "The download may take several minutes depending on your internet connection speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0eFF0r7z_mLz"
   },
   "outputs": [],
   "source": [
    "# Set the path to a folder where you want your data and results to be saved.\n",
    "DATA_DOWNLOAD_DIR=\"content/datasets\"\n",
    "MODELS_DIR=\"content/models\"\n",
    "\n",
    "os.environ[\"DATA_DOWNLOAD_DIR\"] = DATA_DOWNLOAD_DIR\n",
    "os.environ[\"MODELS_DIR\"] = MODELS_DIR\n",
    "\n",
    "! mkdir -p $DATA_DOWNLOAD_DIR $MODELS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAsvlh53_mL0"
   },
   "source": [
    "After downloading, untar the dataset and move it to the correct directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OipV7YVq_mL1"
   },
   "outputs": [],
   "source": [
    "# Note: Ensure that wget and unzip utilities are available. If not, install them.\n",
    "! wget 'https://www.openslr.org/resources/11/librispeech-lm-norm.txt.gz' -P $DATA_DOWNLOAD_DIR\n",
    "\n",
    "# Extract the data\n",
    "! gzip -dk $DATA_DOWNLOAD_DIR/librispeech-lm-norm.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of reducing the time this tutorial takes, we reduced the number of lines of the training dataset. Feel free to modify the number of used lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a random 100,000 lines for training\n",
    "!shuf -n 100000 $DATA_DOWNLOAD_DIR/librispeech-lm-norm.txt | tr '[:upper:]' '[:lower:]' > $DATA_DOWNLOAD_DIR/reduced_training.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Tilg2Eg_mL2"
   },
   "source": [
    "The N-GPU LMs for Parakeet RNNT models are token based. So we need access to ASR's tokenizer model to tokenize the training data. Lets download the RNNT model we want to deploy the N-GPU LM with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -P $MODELS_DIR https://huggingface.co/nvidia/parakeet-rnnt-1.1b/resolve/main/parakeet-rnnt-1.1b.nemo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKud8fQ8_mL3"
   },
   "source": [
    "### Training the N-GPU Language Model\n",
    "\n",
    "Now that we have all the required components in place, we can proceed with training the n-gram language model. This process involves:\n",
    "\n",
    "**Training Components:**\n",
    "- **Acoustic Model**: Parakeet RNNT 1.1B model (for tokenizer integration)\n",
    "- **Training Data**: Processed LibriSpeech text corpus\n",
    "- **Toolkit**: KenLM for efficient n-gram model building\n",
    "- **Framework**: NeMo for seamless integration\n",
    "\n",
    "**Training Parameters:**\n",
    "- **N-gram Order**: 6-gram model (captures 5-word context)\n",
    "- **Model Format**: N-GPU optimized format for Riva NIM deployment\n",
    "- **Tokenizer**: Uses Parakeet RNNT's SentencePiece tokenizer\n",
    "- **Output**: `.nemo` format for easy deployment\n",
    "\n",
    "**Training Process:**\n",
    "The training will:\n",
    "1. Load the Parakeet RNNT model to access its tokenizer\n",
    "2. Tokenize the training text using the model's vocabulary\n",
    "3. Build the 6-gram language model using KenLM\n",
    "4. Save the model in N-GPU format for deployment\n",
    "\n",
    "**Expected Duration**: 5-15 minutes depending on your hardware and dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54Pff6vr_mL3"
   },
   "outputs": [],
   "source": [
    "! cd $NEMO_ROOT/scripts/asr_language_modeling/ngram_lm/ && python3 train_kenlm.py \\\n",
    "              nemo_model_file=$MODELS_DIR/parakeet-rnnt-1.1b.nemo \\\n",
    "              train_paths=['{DATA_DOWNLOAD_DIR}/reduced_training.txt'] \\\n",
    "              kenlm_bin_path=$NEMO_ROOT/decoders/kenlm/build/bin \\\n",
    "              kenlm_model_file=$MODELS_DIR/ngpu_6g \\\n",
    "              ngram_length=6 save_nemo=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGH3btfn_mL6"
   },
   "source": [
    "### Training Complete! ðŸŽ‰\n",
    "\n",
    "The n-gram language model has been successfully trained and saved as `ngpu_6g.nemo`. \n",
    "\n",
    "**Model Details:**\n",
    "- **File**: `ngpu_6g.nemo`\n",
    "- **Type**: 6-gram language model\n",
    "- **Format**: N-GPU optimized for Riva NIM deployment\n",
    "- **Tokenizer**: Integrated with Parakeet RNNT vocabulary\n",
    "- **Size**: Optimized for production deployment\n",
    "\n",
    "**What's Next?**\n",
    "Now that we have our trained language model, we can proceed to:\n",
    "1. **Deploy** the model using NVIDIA Riva NIM\n",
    "2. **Integrate** it with the Parakeet RNNT acoustic model\n",
    "3. **Test** the complete ASR pipeline with language model enhancement\n",
    "\n",
    "The model is ready for deployment and will significantly improve the accuracy of our ASR system by incorporating linguistic knowledge into the recognition process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FubCSAin_mMC"
   },
   "source": [
    "---\n",
    "\n",
    "## Deploying the N-GPU Language Model with Parakeet RNNT in NVIDIA NIM\n",
    "\n",
    "This section covers the complete deployment pipeline for our trained n-gram language model with the Parakeet RNNT acoustic model using NVIDIA Riva NIM. We'll walk through:\n",
    "\n",
    "### Deployment Workflow\n",
    "1. **Model Conversion**: Convert NeMo models to Riva-compatible format\n",
    "2. **Pipeline Building**: Create an end-to-end ASR pipeline with Riva ServiceMaker\n",
    "3. **Model Deployment**: Deploy the complete pipeline to NVIDIA NIM\n",
    "4. **Inference Testing**: Test the deployed model with real audio samples\n",
    "\n",
    "### Key Components\n",
    "- **Acoustic Model**: Parakeet RNNT 1.1B (converted to `.riva` format)\n",
    "- **Language Model**: Our trained 6-gram model (`ngpu_6g.nemo`)\n",
    "- **Deployment Platform**: NVIDIA Riva NIM with optimized inference\n",
    "- **Integration**: Seamless combination of acoustic and language models\n",
    "\n",
    "This deployment process ensures optimal performance and scalability for production ASR applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FubCSAin_mMC"
   },
   "source": [
    "### Model Conversion with nemo2riva\n",
    "\n",
    "To deploy our NeMo models with NVIDIA Riva NIM, we need to convert them from the `.nemo` format to the `.riva` format using the `nemo2riva` tool.\n",
    "\n",
    "#### What is nemo2riva?\n",
    "**nemo2riva** is a command-line tool that:\n",
    "- **Converts Models**: Transforms `.nemo` models to Riva-compatible `.riva` format\n",
    "- **Optimizes Performance**: Applies optimizations for production deployment\n",
    "- **Handles Dependencies**: Manages model dependencies and configurations\n",
    "- **Ensures Compatibility**: Guarantees compatibility with Riva NIM deployment\n",
    "\n",
    "#### Conversion Process\n",
    "The conversion process involves:\n",
    "1. **Model Analysis**: Examining the NeMo model structure and dependencies\n",
    "2. **Format Translation**: Converting to Riva's internal representation\n",
    "3. **Optimization**: Applying deployment-specific optimizations\n",
    "4. **Validation**: Ensuring the converted model is ready for deployment\n",
    "\n",
    "#### Installation\n",
    "The `nemo2riva` tool is available as a Python package:\n",
    "- **PyPI Package**: Available on [PyPI](https://pypi.org/project/nemo2riva/)\n",
    "- **Installation**: Simple `pip install` command\n",
    "\n",
    "#### Next Steps\n",
    "After conversion, we'll use the Riva ServiceMaker framework to:\n",
    "- **Build Pipeline**: Create an end-to-end ASR pipeline (`.rmir` format)\n",
    "- **Deploy Model**: Deploy to NVIDIA NIM for production inference\n",
    "- **Test Integration**: Verify the complete pipeline works correctly\n",
    "\n",
    "This workflow ensures seamless transition from research models to production deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing nemo2riva\n",
    "\n",
    "Let's install the `nemo2riva` tool to convert our downloaded Parakeet RNNT model from `.nemo` format to `.riva` format for Riva NIM deployment.\n",
    "\n",
    "**Installation Process:**\n",
    "- Downloads the latest `nemo2riva` package from NVIDIA's PyPI repository\n",
    "- Includes all necessary dependencies for model conversion\n",
    "- Provides command-line interface for easy model conversion\n",
    "\n",
    "**Authentication Required:**\n",
    "- The installation requires access to NVIDIA's private PyPI repository\n",
    "- Ensure you have valid NGC credentials configured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install nemo2riva \n",
    "! pip3 install --extra-index-url https://pypi.nvidia.com  nemo2riva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nemo2riva --key tlt_encode --format nemo $MODELS_DIR/parakeet-rnnt-1.1b.nemo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Riva ServiceMaker Framework\n",
    "\n",
    "**Riva ServiceMaker** is a comprehensive toolkit that streamlines the deployment of custom models to NVIDIA Riva NIM. It handles the complex process of aggregating all necessary artifacts for production deployment.\n",
    "\n",
    "### ServiceMaker Components\n",
    "\n",
    "Riva ServiceMaker consists of two main tools that work together to create a complete deployment pipeline:\n",
    "\n",
    "#### 1. Riva-Build\n",
    "- **Purpose**: Creates deployment-ready model pipelines\n",
    "- **Input**: `.riva` model files and configuration\n",
    "- **Output**: `.rmir` (Riva Model Intermediate Representation) files\n",
    "- **Function**: Combines models, configurations, and optimizations into a single deployable package\n",
    "\n",
    "#### 2. Riva-Deploy\n",
    "- **Purpose**: Deploys the pipeline to target environments\n",
    "- **Input**: `.rmir` files and deployment configuration\n",
    "- **Output**: Complete model repository ready for NIM deployment\n",
    "- **Function**: Creates the final deployment package with all necessary artifacts\n",
    "\n",
    "### Workflow Overview\n",
    "1. **Model Preparation**: Convert models to `.riva` format\n",
    "2. **Pipeline Building**: Use `riva-build` to create `.rmir` files\n",
    "3. **Deployment**: Use `riva-deploy` to create the final model repository\n",
    "4. **NIM Deployment**: Deploy the repository to NVIDIA NIM\n",
    "\n",
    "This framework ensures consistent, optimized deployment across different environments and use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riva-Build: Creating the Model Pipeline\n",
    "\n",
    "**Riva-Build** is the first step in the ServiceMaker workflow, responsible for creating a deployment-ready model pipeline.\n",
    "\n",
    "#### What Riva-Build Does\n",
    "- **Model Integration**: Combines multiple `.riva` model files into a unified pipeline\n",
    "- **Configuration Management**: Applies deployment-specific configurations and optimizations\n",
    "- **Asset Aggregation**: Collects all necessary files, dependencies, and metadata\n",
    "- **Pipeline Creation**: Generates a complete end-to-end inference pipeline\n",
    "\n",
    "#### Output: RMIR Files\n",
    "The primary output is an **RMIR (Riva Model Intermediate Representation)** file that contains:\n",
    "- **Pipeline Specification**: Complete end-to-end inference workflow\n",
    "- **Model Assets**: All model files, weights, and configurations\n",
    "- **Deployment Metadata**: Hardware requirements, optimization settings\n",
    "- **Service Configuration**: API endpoints, input/output specifications\n",
    "\n",
    "#### For ASR with N-gram Language Models\n",
    "In our case, `riva-build` will:\n",
    "1. **Combine Models**: Integrate the Parakeet RNNT acoustic model with our n-gram language model\n",
    "2. **Configure Pipeline**: Set up the complete ASR pipeline with language model integration\n",
    "3. **Optimize Performance**: Apply optimizations for the target deployment environment\n",
    "4. **Generate RMIR**: Create the intermediate representation ready for deployment\n",
    "\n",
    "#### Key Benefits\n",
    "- **Deployment Agnostic**: RMIR files work across different deployment environments\n",
    "- **Optimized Performance**: Includes hardware-specific optimizations\n",
    "- **Complete Pipeline**: Contains everything needed for end-to-end inference\n",
    "- **Easy Deployment**: Simplifies the final deployment step\n",
    "\n",
    "For detailed configuration options, refer to the [Riva ASR NIM Pipeline Configuration documentation](https://docs.nvidia.com/nim/riva/asr/latest/custom-deployment.html#deploying-custom-models-as-nim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: UPDATE THESE PATHS \n",
    "\n",
    "# Riva NIM Docker\n",
    "\n",
    "# Refer to this table to get the CONTAINER_ID for the model architecture you want to deploy.\n",
    "# https://docs.nvidia.com/nim/riva/asr/latest/support-matrix.html#supported-models\n",
    "# Since this is RNNT model, we should use following CONTAINER_ID\n",
    "CONTAINER_ID = \"parakeet-1-1b-rnnt-multilingual\"\n",
    "\n",
    "# Name of the acoustic model .riva file\n",
    "ACOUSTIC_MODEL_NAME = f\"{MODELS_DIR}/parakeet-rnnt-1.1b.riva\"\n",
    "\n",
    "# Name of the language model .nemo file\n",
    "LANGUAGE_MODEL_NAME = f\"{MODELS_DIR}/ngpu_6g.nemo\"\n",
    "\n",
    "# Path to store NIM model repository, Make sure that this directory is empty\n",
    "NIM_EXPORT_PATH=\"~/nim_cache\" \n",
    "\n",
    "! mkdir -p $NIM_EXPORT_PATH\n",
    "! chmod 777 $NIM_EXPORT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the `.rmir` file\n",
    "\n",
    "Refer to the [Riva ASR NIM Pipeline Configuration documentation](https://docs.nvidia.com/nim/riva/asr/latest/pipeline-configuration.html) to obtain the proper `riva-build` parameters for your particular application, select the acoustic model, language, and pipeline type (offline for the purposes of this tutorial) from the interactive web menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the appropriate value\n",
    "! docker run --gpus all --rm \\\n",
    "     -v $MODEL_DIR:/servicemaker-dev \\\n",
    "     --name riva-servicemaker \\\n",
    "     --entrypoint=\"\" \\\n",
    "     nvcr.io/nim/nvidia/$CONTAINER_ID \\\n",
    "     riva-build speech_recognition \\\n",
    "        /servicemaker-dev/asr_offline_riva_ngram_lm.rmir:tlt_encode \\\n",
    "        /servicemaker-dev/$ACOUSTIC_MODEL_NAME:tlt_encode \\\n",
    "        --offline --name=parakeet-rnnt-1.1b-unified-ml-cs-universal-multi-asr-offline \\\n",
    "        --return_separate_utterances=True --featurizer.use_utterance_norm_params=False \\\n",
    "        --featurizer.precalc_norm_time_steps=0 --featurizer.precalc_norm_params=False \\\n",
    "        --ms_per_timestep=80 --language_code=en-US \\\n",
    "        --nn.fp16_needs_obey_precision_pass --unified_acoustic_model \\\n",
    "        --chunk_size=8.0 --left_padding_size=0 --right_padding_size=0 \\\n",
    "        --featurizer.max_batch_size=256 --featurizer.max_execution_batch_size=256 \\\n",
    "        --max_batch_size=128 --nn.opt_batch_size=128 \\\n",
    "        --endpointing_type=niva --endpointing.stop_history=0  \\\n",
    "        --decoder_type=nemo --nemo_decoder.language_model_alpha=0.5 \\\n",
    "        --nemo_decoder.language_model_file=/servicemaker-dev/ngpu_6g.nemo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riva-Deploy: Final Deployment Preparation\n",
    "\n",
    "**Riva-Deploy** is the second step in the ServiceMaker workflow, responsible for creating the final deployment package for NVIDIA NIM.\n",
    "\n",
    "#### What Riva-Deploy Does\n",
    "- **RMIR Processing**: Takes one or more RMIR files as input\n",
    "- **Repository Creation**: Generates a complete model repository structure\n",
    "- **Configuration Generation**: Creates ensemble configurations for pipeline execution\n",
    "- **Asset Organization**: Organizes all files in the proper directory structure\n",
    "- **Deployment Package**: Produces a ready-to-deploy model repository\n",
    "\n",
    "#### Input and Output\n",
    "- **Input**: RMIR files (from `riva-build`) and target directory path\n",
    "- **Output**: Complete model repository with all necessary artifacts\n",
    "- **Format**: Standardized directory structure compatible with NVIDIA NIM\n",
    "\n",
    "#### Security Considerations\n",
    "**Encryption Support**: If you used encryption during the `riva-build` step:\n",
    "- **Key Format**: Append `:your_encryption_key` to the model name in the deploy command\n",
    "- **Security**: Ensures model protection during deployment\n",
    "- **Example**: `model.rmir:my_secret_key`\n",
    "\n",
    "#### Repository Structure\n",
    "The generated repository includes:\n",
    "- **Model Files**: All model weights and configurations\n",
    "- **Pipeline Config**: Ensemble configuration for execution\n",
    "- **Metadata**: Deployment information and requirements\n",
    "- **Dependencies**: All necessary runtime dependencies\n",
    "\n",
    "#### Next Steps\n",
    "After `riva-deploy` completes:\n",
    "1. **Model Repository**: Ready for NVIDIA NIM deployment\n",
    "2. **Container Deployment**: Can be deployed using Docker containers\n",
    "3. **Service Activation**: Ready to serve inference requests\n",
    "\n",
    "This step finalizes the deployment preparation, making your custom models ready for production use with NVIDIA NIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax: riva-deploy -f dir-for-rmir/model.rmir[:key] output-dir-for-repository\n",
    "! docker run --gpus all --rm \\\n",
    "     -v $MODEL_LOC:/servicemaker-dev \\\n",
    "     -v $NIM_EXPORT_PATH:/model_tar \\\n",
    "     --name riva-servicemaker \\\n",
    "     --entrypoint=\"\" \\\n",
    "     nvcr.io/nim/nvidia/$CONTAINER_ID \\\n",
    "     bash -c \"riva-deploy -f /servicemaker-dev/asr_offline_riva_ngram_lm.rmir /data/models/ && tar -czf /model_tar/custom_models.tar.gz -C /data/models .\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Starting the NVIDIA Riva ASR NIM Server\n",
    "\n",
    "Now that we have successfully generated the model repository, we can start the NVIDIA Riva NIM server with our custom models.\n",
    "\n",
    "### Server Configuration\n",
    "The Riva NIM server will be configured with:\n",
    "- **Custom Models**: Our trained n-gram language model integrated with Parakeet RNNT\n",
    "- **GPU Acceleration**: Optimized for NVIDIA GPU inference\n",
    "- **API Endpoints**: Both REST and gRPC interfaces for client connections\n",
    "- **Container Deployment**: Running in a Docker container for easy management\n",
    "\n",
    "### Server Features\n",
    "- **High Performance**: Optimized inference with TensorRT acceleration\n",
    "- **Scalability**: Support for concurrent requests and load balancing\n",
    "- **Monitoring**: Built-in health checks and performance metrics\n",
    "- **Security**: Secure API endpoints with authentication support\n",
    "\n",
    "### Port Configuration\n",
    "- **REST API**: Port 9000 for HTTP-based requests\n",
    "- **gRPC API**: Port 50051 for high-performance gRPC requests\n",
    "- **Health Checks**: Available at `/v1/health/live` endpoint\n",
    "\n",
    "### Environment Variables\n",
    "The server uses several environment variables for configuration:\n",
    "- **NGC_API_KEY**: For accessing NVIDIA NGC resources\n",
    "- **NIM_EXPORT_PATH**: Path to our custom model repository\n",
    "\n",
    "Once started, the server will load our models and be ready to serve inference requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run the container with the cache directory mounted in the appropriate location:\n",
    "! docker run -it --rm -d --name=$CONTAINER_ID \\\n",
    "   --runtime=nvidia \\\n",
    "   --gpus '\"device=0\"' \\\n",
    "   --shm-size=8GB \\\n",
    "   -e NGC_API_KEY \\\n",
    "   -e NIM_TAGS_SELECTOR \\\n",
    "   -e NIM_DISABLE_MODEL_DOWNLOAD=true \\\n",
    "   -e NIM_HTTP_API_PORT=9000 \\\n",
    "   -e NIM_GRPC_API_PORT=50051 \\\n",
    "   -p 9000:9000 \\\n",
    "   -p 50051:50051 \\\n",
    "   -v $NIM_EXPORT_PATH:/opt/nim/export \\\n",
    "   -e NIM_EXPORT_PATH=/opt/nim/export \\\n",
    "   nvcr.io/nim/nvidia/$CONTAINER_ID:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Running Inference with the Deployed Model\n",
    "\n",
    "Now that our NVIDIA Riva NIM server is running with our custom n-gram language model, we can perform inference to test the complete ASR pipeline.\n",
    "\n",
    "### Inference Capabilities\n",
    "Our deployed system provides:\n",
    "- **Enhanced ASR**: Automatic Speech Recognition with n-gram language model integration\n",
    "- **Improved Accuracy**: Language model helps correct and improve transcription quality\n",
    "- **Real-time Processing**: Fast inference suitable for streaming applications\n",
    "- **Multiple APIs**: Both REST and gRPC interfaces for different use cases\n",
    "\n",
    "### Client Setup\n",
    "To interact with the server, we'll use the **Riva Python Client**:\n",
    "- **Package**: Available on [PyPI](https://pypi.org/project/nvidia-riva-client/)\n",
    "- **Features**: Easy-to-use Python API for both REST and gRPC requests\n",
    "- **Documentation**: Comprehensive API documentation and examples\n",
    "- **Authentication**: Built-in support for secure connections\n",
    "\n",
    "### Inference Process\n",
    "The inference workflow includes:\n",
    "1. **Audio Input**: Load and prepare audio files for processing\n",
    "2. **Server Connection**: Establish connection to the Riva NIM server\n",
    "3. **Request Configuration**: Set up recognition parameters and options\n",
    "4. **Inference Execution**: Send requests and receive transcriptions\n",
    "5. **Result Processing**: Handle and display the recognition results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Client API Bindings\n",
    "! pip install nvidia-riva-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import riva.client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the Riva Server\n",
    "\n",
    "Before we can run inference, we need to ensure the Riva NIM server is fully loaded and ready to serve requests.\n",
    "\n",
    "#### Server Loading Process\n",
    "The NIM server goes through several initialization steps:\n",
    "1. **Container Startup**: Docker container initialization\n",
    "2. **Model Loading**: Loading our custom Parakeet RNNT model with n-gram language model\n",
    "3. **GPU Initialization**: Setting up CUDA and TensorRT optimizations\n",
    "4. **Service Activation**: Starting the REST and gRPC API endpoints\n",
    "5. **Health Check**: Verifying all components are operational\n",
    "\n",
    "#### Loading Time Considerations\n",
    "- **Model Size**: Larger models take longer to load into memory\n",
    "- **GPU Memory**: Initial GPU memory allocation and optimization\n",
    "- **First Request**: May take additional time for model warm-up\n",
    "- **Typical Duration**: 2-5 minutes depending on hardware and model size\n",
    "\n",
    "#### Health Monitoring\n",
    "We'll monitor the server status using the health check endpoint:\n",
    "- **Endpoint**: `http://localhost:9000/v1/health/live`\n",
    "- **Response**: Returns \"live\" when server is ready\n",
    "- **Retry Logic**: Automatic retry with 5-second intervals\n",
    "- **Timeout**: Maximum 30 attempts (2.5 minutes) before giving up\n",
    "\n",
    "**Please wait** while the server completes its initialization process. The next cell will automatically detect when the server is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time\n",
    "\n",
    "for i in range(30):\n",
    "    try:\n",
    "        print(f\"Waiting for NIM server to load, retrying in 5 seconds...\")\n",
    "        r = requests.get(\"http://0.0.0.0:9000/v1/health/live\", timeout=2)\n",
    "        if \"live\" in r.text:\n",
    "            print(\"NIM server is ready!\")\n",
    "            break\n",
    "    except requests.RequestException as e:\n",
    "        pass\n",
    "    time.sleep(5)\n",
    "else:\n",
    "    print(\"Server did not become ready after 30 attempts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Function\n",
    "\n",
    "Once the server is ready, we can use this inference function to query the Riva NIM server and transcribe audio files.\n",
    "\n",
    "#### Function Overview\n",
    "The `run_inference()` function provides a simple interface to:\n",
    "- **Load Audio**: Read audio files from disk\n",
    "- **Connect to Server**: Establish gRPC connection to the Riva NIM server\n",
    "- **Configure Recognition**: Set up recognition parameters (language, punctuation, etc.)\n",
    "- **Process Audio**: Send audio data for transcription\n",
    "- **Return Results**: Receive and display the transcription results\n",
    "\n",
    "#### Key Features\n",
    "- **gRPC Protocol**: Uses high-performance gRPC for fast communication\n",
    "- **Flexible Configuration**: Supports various recognition settings\n",
    "- **Error Handling**: Robust error handling and connection management\n",
    "- **Result Formatting**: Clean output with optional detailed response display\n",
    "\n",
    "#### Recognition Configuration\n",
    "The function uses the following default settings:\n",
    "- **Language**: English (en-US)\n",
    "- **Alternatives**: Single best transcription result\n",
    "- **Punctuation**: Automatic punctuation disabled (can be enabled)\n",
    "- **Format**: Plain text output\n",
    "\n",
    "#### Usage\n",
    "Simply call `run_inference(audio_file_path)` with the path to your audio file to get the transcription. The function handles all the complexity of server communication and result processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(audio_file, server='localhost:50051', print_full_response=False):\n",
    "    with open(audio_file, 'rb') as fh:\n",
    "        data = fh.read()\n",
    "    \n",
    "    auth = riva.client.Auth(uri=server)\n",
    "    client = riva.client.ASRService(auth)\n",
    "    config = riva.client.RecognitionConfig(\n",
    "        language_code=\"en-US\",\n",
    "        max_alternatives=1,\n",
    "        enable_automatic_punctuation=False,\n",
    "    )\n",
    "    \n",
    "    response = client.offline_recognize(data, config)\n",
    "    if print_full_response: \n",
    "        print(response)\n",
    "    else:\n",
    "        print(response.results[0].alternatives[0].transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"audio_samples/en-US_sample.wav\"\n",
    "run_inference(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup: Stopping the Riva NIM Server\n",
    "\n",
    "When you're finished with the tutorial, it's important to properly stop the Riva NIM server to free up system resources.\n",
    "\n",
    "#### Why Stop the Server?\n",
    "- **Resource Management**: Frees up GPU memory and CPU resources\n",
    "- **Container Cleanup**: Properly terminates the Docker container\n",
    "- **System Performance**: Prevents resource conflicts with other applications\n",
    "- **Best Practices**: Ensures clean shutdown of all services\n",
    "\n",
    "#### Cleanup Process\n",
    "The cleanup commands will:\n",
    "1. **Stop Container**: Gracefully stop the running Riva NIM container\n",
    "2. **Remove Container**: Clean up the container instance\n",
    "3. **Free Resources**: Release GPU memory and system resources\n",
    "\n",
    "#### Alternative: Keep Server Running\n",
    "If you want to continue using the server for additional testing:\n",
    "- **Skip Cleanup**: Don't run the cleanup commands\n",
    "- **Server Persistence**: The server will continue running until manually stopped\n",
    "- **Resource Usage**: Monitor system resources if keeping the server active\n",
    "\n",
    "#### Next Steps\n",
    "After stopping the server, you can:\n",
    "- **Review Results**: Analyze the transcription results and model performance\n",
    "- **Experiment Further**: Try different audio files or configuration settings\n",
    "- **Deploy Production**: Use the same workflow for production deployments\n",
    "- **Scale Up**: Deploy multiple instances for higher throughput\n",
    "\n",
    "**Congratulations!** You've successfully completed the full pipeline from training an n-gram language model to deploying it with NVIDIA Riva NIM for production ASR inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker stop $CONTAINER_ID\n",
    "! docker rm $CONTAINER_ID"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
