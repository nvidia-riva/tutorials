{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Acoustic Model with Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPuyTHGTm8Q-"
   },
   "source": [
    "In this notebook, we train an ASR model for German, using the Citrinet model with cross language transfer learning. The workflow is demonstrated in the figure below.\n",
    "\n",
    "![png](./imgs/german-transfer-learning.PNG)\n",
    "\n",
    "We first demonstrate the training process with NeMo on 1 GPU in this notebook. To speed up training, multiple GPUs should be leveraged using the more efficient DDP (distributed data parallel) protocol, which must run in a seperate [training script](./train.py).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jALgpGLjmaCw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n"
     ]
    }
   ],
   "source": [
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr\n",
    "\n",
    "print(nemo.__version__)\n",
    "\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDTC4fXZ5QnT"
   },
   "source": [
    "## Cross-Language Transfer Learning\n",
    "\n",
    "Transfer learning is an important machine learning technique that uses a model’s knowledge of one task to perform better on another. Fine-tuning is one of the techniques to perform transfer learning. It is an essential part of the recipe for many state-of-the-art results where a base model is first pretrained on a task with abundant training data and then fine-tuned on different tasks of interest where the training data is less abundant or even scarce.\n",
    "\n",
    "Transfer learning with NeMo is simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IN0LbDbY5YR1"
   },
   "source": [
    "\n",
    "First, let's load the pretrained Nemo Citrinet model, which was trained on ~6000 hours of English data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-07-12 02:00:48 cloud:56] Found existing object /root/.cache/torch/NeMo/NeMo_1.7.1/stt_en_citrinet_1024/86acfaf495a53383369fb6c9c547b8dd/stt_en_citrinet_1024.nemo.\n",
      "[NeMo I 2022-07-12 02:00:48 cloud:62] Re-using file from: /root/.cache/torch/NeMo/NeMo_1.7.1/stt_en_citrinet_1024/86acfaf495a53383369fb6c9c547b8dd/stt_en_citrinet_1024.nemo\n",
      "[NeMo I 2022-07-12 02:00:48 common:704] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2022-07-12 02:00:53 mixins:146] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-07-12 02:00:53 modelPT:148] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    trim_silence: true\n",
      "    max_duration: 16.7\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    use_start_end_token: false\n",
      "    \n",
      "[NeMo W 2022-07-12 02:00:53 modelPT:155] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    use_start_end_token: false\n",
      "    \n",
      "[NeMo W 2022-07-12 02:00:53 modelPT:161] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-07-12 02:00:53 features:255] PADDING: 16\n",
      "[NeMo I 2022-07-12 02:00:53 features:272] STFT using torch\n",
      "[NeMo I 2022-07-12 02:00:59 save_restore_connector:157] Model EncDecCTCModelBPE was successfully restored from /root/.cache/torch/NeMo/NeMo_1.7.1/stt_en_citrinet_1024/86acfaf495a53383369fb6c9c547b8dd/stt_en_citrinet_1024.nemo.\n"
     ]
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "asr_model = nemo_asr.models.EncDecCTCModelBPE.from_pretrained(model_name=\"stt_en_citrinet_1024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update vocabulary\n",
    "Next, check what kind of vocabulary/alphabet the model has right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', 's', '▁the', 't', '▁a', '▁i', \"'\", '▁and', '▁to', 'ed', 'd', '▁of', 'e', '▁in', 'ing', '.', '▁it', '▁you', 'n', '▁that', 'm', 'y', 'er', '▁he', 're', 'r', '▁was', '▁is', '▁for', '▁know', 'a', 'p', 'c', ',', '▁be', 'o', '▁but', '▁they', 'g', '▁so', 'ly', 'b', '▁s', '▁yeah', '▁we', '▁have', '▁re', '▁like', 'l', '▁on', 'll', 'u', '▁with', '▁do', 'al', '▁not', '▁are', 'or', 'ar', 'le', '▁this', '▁as', 'es', '▁c', '▁de', 'f', 'in', 'i', 've', '▁uh', 'ent', '▁or', '▁what', '▁me', '▁t', '▁at', '▁my', '▁his', '▁there', 'w', '▁all', '▁just', 'h', '▁can', 'ri', 'il', 'k', 'ic', '▁e', '▁', '▁um', '▁don', '▁b', '▁had', 'ch', 'ation', 'en', 'th', '▁no', '▁she', 'it', '▁one', '▁think', '▁st', '▁if', '▁from', 'ter', '▁an', 'an', 'ur', '▁out', 'on', '▁go', 'ck', '▁would', '▁were', '▁w', '▁will', '▁about', '▁right', 'ment', '▁her', 'te', 'ion', '▁well', '▁by', 'ce', '▁g', '▁oh', '▁up', 'ro', 'ra', '▁when', '▁some', '▁also', '▁their', 'ers', 'ow', '▁more', '▁time', 'ate', '▁has', '▁people', '▁see', '▁pa', 'el', '▁get', '▁ex', '▁mean', 'li', '▁really', 'v', '▁ra', '▁been', '▁said', '-', 'la', 'ge', '▁how', '▁po', 'ir', '▁mo', '▁who', '▁because', '▁co', '▁other', '▁f', 'id', 'ol', '▁un', '▁now', '▁work', 'ist', 'us', '▁your', '▁them', 'ver', 'as', 'ne', '▁ca', 'lo', '▁fa', '▁him', 'ng', '▁good', '▁could', '▁pro', 'ive', '▁con', 'de', 'un', 'age', '▁ma', '?', 'at', '▁ro', '▁ba', '▁then', '▁com', 'est', 'vi', '▁dis', 'ies', 'ance', '▁su', '▁even', '▁any', 'ut', 'ad', 'ul', '▁se', '▁two', '▁bu', '▁lo', '▁say', '▁la', '▁fi', 'is', '▁li', '▁over', '▁new', '▁man', '▁sp', 'ity', '▁did', '▁bo', '▁very', 'x', 'end', '▁which', '▁our', '▁after', '▁o', 'ke', '▁p', 'im', '▁want', '▁ha', '▁v', 'z', '▁where', 'ard', 'um', '▁into', 'ru', '▁di', '▁lot', '▁dr', 'mp', '▁day', 'ated', 'ci', '▁these', '▁than', '▁take', '▁kind', '▁got', 'ight', '▁make', 'ence', '▁pre', '▁going', 'ish', '▁k', 'able', '▁look', 'ti', 'per', '▁here', '▁en', '▁ah', 'ry', '▁too', '▁part', 'ant', 'one', '▁ho', '▁much', '▁way', '▁sa', '▁something', 'mo', '▁us', '▁th', '▁mhm', '▁mi', '▁off', 'pe', '▁back', 'les', '▁cr', '▁ri', '▁fe', 'und', '▁fl', 'port', '▁school', '▁ch', '▁should', '▁first', '▁only', '▁le', 'ot', 'tion', '▁little', '▁da', '▁hu', '▁d', 'me', 'ta', '▁down', '▁okay', '▁come', 'ain', 'ff', '▁car', 'co', '▁need', 'ture', '▁many', '▁things', '▁ta', 'qu', 'man', 'ty', 'iv', '▁year', 'he', '▁thing', 'ho', '▁singapore', 'po', '▁vi', '▁sc', '▁still', 'der', '▁hi', '▁never', '▁qu', 'ia', '▁fr', '▁min', '▁most', 'om', 'ful', '▁bi', '▁long', 'ig', '▁years', 'ous', '▁three', '▁play', '▁before', '▁pi', 'ical', '▁those', '▁comp', 'huh', '▁live', 'tor', 'ise', '▁old', 'am', 'rr', '▁sta', '▁n', 'ick', 'di', 'ma', 'ary', 'ction', '▁friend', 'ition', '▁gu', '▁through', 'pp', 'for', 'ie', 'ious', '▁sh', '▁home', 'lu', '▁high', 'ian', 'cu', '▁help', '▁give', '▁talk', '▁sha', '▁such', '▁didn', 'em', '▁may', '▁ga', \"▁'\", '▁gra', '▁guess', '▁every', '▁app', 'tic', '▁tra', '▁\"', 'op', '▁made', '\"', '▁op', '▁own', '▁mar', 'no', '▁ph', '▁life', '▁y', 'ak', 'ine', '▁pu', '▁place', '▁always', '▁start', '▁jo', '▁pe', '▁let', '▁name', 'ni', '▁same', '▁last', '▁cl', 'ph', '▁both', '▁pri', 'ities', '▁another', 'and', '▁al', '▁boy', 'ving', '▁actually', '▁person', '▁went', '▁yes', 'ca', 'ally', '▁h', '▁great', '▁thought', '▁used', 'act', '▁feel', 'ward', '▁different', '▁cons', '▁show', '▁watch', '▁being', '▁money', 'ay', '▁try', '▁why', '▁big', 'ens', '▁cha', '▁find', '▁hand', '▁real', '▁four', 'ial', '▁ne', '▁che', '▁read', '▁five', '▁family', 'ag', '▁change', '▁add', 'ha', '▁put', 'par', 'lic', 'side', '▁came', '▁under', 'ness', '▁per', 'j', '▁around', '▁end', '▁house', 'if', '▁while', 'vo', '▁act', '▁happen', '▁plan', 'mit', '▁far', '▁tri', '▁ten', '▁du', '▁win', '▁tea', 'ze', '▁better', '▁sure', '▁mu', '▁use', '▁anything', '▁love', '▁world', '▁hard', 'ure', '▁does', '▁war', '▁stuff', '▁ja', '▁must', 'min', 'gg', '▁ru', '▁care', '▁tell', '▁pl', '▁doing', '▁probably', '▁found', 'ative', '▁point', 'ach', '▁ju', 'ip', '▁again', '▁interest', '▁state', '▁week', 'na', '▁might', '▁pretty', '▁ki', '▁fo', 'ber', '▁am', 'line', 'led', '▁six', '▁acc', '▁bri', '▁call', '▁sw', '▁each', '▁business', '▁keep', '▁away', 'cause', '▁pass', '▁va', '▁children', '▁pay', '▁count', '▁public', '▁everything', 'land', '▁though', '▁men', 'bo', '▁young', '▁na', '▁move', 'ough', 'ating', 'com', '▁month', 'ton', '▁close', '▁few', '!', '▁maybe', '▁imp', 'son', '▁grow', '▁u', '▁turn', 'ible', '▁em', '▁air', '▁ever', 'our', '▁sea', '▁fun', '▁government', '▁miss', '▁done', '▁next', '▁kids', '▁cor', '▁set', '▁run', 'way', '▁wa', '▁getting', '▁eight', '▁open', '▁job', '▁problem', 'ook', '▁night', '▁learn', '▁book', 'ual', '▁ti', '▁best', 'cept', '▁during', '▁small', 'ex', '▁without', '▁water', '▁trans', '▁course', '▁once', '▁sit', '▁area', '▁country', '▁mister', '▁nothing', '▁whole', '▁believe', '▁service', '▁took', '▁face', '▁bad', '▁later', '▁head', '▁called', '▁seven', '▁art', '▁since', '▁er', '▁fact', '▁city', '▁market', '▁hour', '▁continue', 'ship', '▁invest', '▁exactly', '▁large', '▁true', '▁nine', '▁sub', '▁having', '▁game', 'va', '▁lu', '▁conf', '▁case', '▁doesn', '▁certain', '▁wi', '▁law', '▁else', 'fi', '▁left', '▁enough', '▁second', '▁gonna', '▁food', '▁hope', '▁saw', '▁between', '▁je', 'bi', '▁girl', '▁company', '▁able', '▁expect', '▁told', '▁stand', '▁group', '▁main', '▁walk', '▁cause', '▁however', '▁number', '▁follow', '▁near', '▁yet', '▁sometimes', '▁train', '▁lead', '▁system', '▁remain', '▁develop', 'gra', '▁word', '▁exc', '▁together', '▁consider', '▁town', '▁less', 'ator', '▁important', '▁remember', '▁free', '▁quite', '▁understand', '▁bra', '▁support', '▁idea', '▁stop', '▁reason', '▁nice', '▁mm', '▁agree', '▁low', '▁against', '▁issue', '▁become', '▁today', '▁side', '▁student', '▁matter', '▁question', '▁mother', '▁father', '▁hundred', '▁sort', '▁eat', '▁already', '▁rest', '▁line', '▁asked', '▁include', '▁upon', '▁office', '▁won', '▁class', '▁wait', '▁twenty', '▁half', '▁light', '▁price', '▁almost', 'ash', '▁child', '▁sign', '▁least', '▁several', 'press', '▁either', '▁minute', '▁himself', '▁parents', '▁room', '▁whatever', '▁general', '▁cost', '▁among', '▁direct', '▁computer', '▁appear', '▁meet', '▁ski', '▁return', '▁couple', '▁product', '▁suppose', '▁definitely', '▁america', '▁term', '▁usually', '▁strong', '▁current', '▁arm', '▁speak', '▁local', '▁south', '▁experience', '▁full', '▁north', '▁elect', '▁leave', '▁provide', 'qui', '▁power', '▁movie', '▁everyone', '▁making', '▁member', '▁woman', '▁somebody', '▁wonder', '▁short', '▁health', '▁police', '▁bank', '▁until', '▁companies', '▁everybody', '▁knew', '▁program', '▁music', '▁york', '▁land', '▁doctor', '▁answer', '▁building', '▁employ', '▁travel', '▁major', '▁seems', '▁safe', 'gue', '▁college', '▁along', '▁clear', '▁especially', '▁umhu', '▁result', '▁type', '▁court', '▁black', '▁hold', '▁myself', '▁education', '▁social', '▁enjoy', '▁became', '▁whether', '▁morning', '▁difficult', '▁shi', '▁felt', '▁husband', '▁white', '▁taking', '▁million', '▁require', '▁early', 'ency', '▁visit', '▁level', '▁brother', '▁married', '▁further', '▁affect', '▁serve', '▁present', '▁park', '▁effect', '▁wife', '▁teacher', '▁cannot', '▁community', '▁street', '▁period', '▁national', '▁view', '▁future', '▁daughter', '▁situation', '▁grand', '▁success', '▁perform', '▁concern', '▁complete', '▁example', 'ized', '▁thousand', '▁increase', '▁began', '▁final', '▁east', '▁sense', '▁charge', '▁record', '▁born', '▁instead', '▁receive', '▁women', '▁across', '▁information', '▁although', '▁process', '▁condition', '▁security', '▁treat', '▁funny', '▁custom', '▁cold', '▁behind', 'ified', '▁ground', 'cycl', '▁depend', '▁themselves', '▁design', '▁slow', '▁third', '▁smoke', '▁wrong', '▁project', '▁space', '▁drink', '▁particular', '▁listen', '▁thirty', '▁special', 'ability', '▁improve', '▁attack', '▁happy', '▁strange', '▁english', '▁value', '▁brought', '▁private', '▁account', '▁china', '▁spoke', '▁foreign', '▁possible', '▁author', '▁circ', '▁voice', '▁figure', '▁control', '▁according', '▁green', '▁university', '▁language', '▁please', '▁animal', '▁church', '▁society', '▁dream', '’', 'q', ':', ';', '—', '‘', '”', '_', '3', '8', '<', '>', '1', '–', '7', '(', ')', '0', '2', '4', '+', '&', '5', '9', 'ü', 'é', '/', 'á', 'ó', 'ō', 'ú', ']', 'â', 'í', 'ã', 'ð', 'ā', 'ć', 'č', 'š', 'è', 'ë', '`', 'ç', 'ū', 'ạ', 'ø', '=', 'à', 'ł', 'α', 'ô', 'к', '}', 'å', 'ă', 'и', 'ī', 'π', 'œ', '\\\\', '[', 'ñ', 'ß', 'ö', 'ä', '6', 'з', 'н', 'û', '%', '{', '¡', 'æ', 'ê', 'þ', 'ę', 'ě', 'ğ', 'ń', 'ő', 'ř', 'ž', 'ʻ', 'в', 'е', 'й', 'л', 'ь', 'χ', '“']\n"
     ]
    }
   ],
   "source": [
    "print(asr_model.decoder.vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BBtk30g5sHJ"
   },
   "source": [
    "Now let's update the vocabulary in this model, using the German tokenizer that we have trained in the data preparation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4Ey9CUkJ5o56"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-07-12 02:00:59 modelPT:215] You tried to register an artifact under config key=tokenizer.model_path but an artifact for it has already been registered.\n",
      "[NeMo W 2022-07-12 02:00:59 modelPT:215] You tried to register an artifact under config key=tokenizer.vocab_path but an artifact for it has already been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-07-12 02:00:59 mixins:146] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2022-07-12 02:00:59 ctc_bpe_models:361] \n",
      "    Replacing old number of classes (1024) with new number of classes - 1024\n",
      "[NeMo I 2022-07-12 02:01:00 ctc_bpe_models:390] Changed tokenizer to ['<unk>', 'en', 'er', 'ch', '▁d', 'ei', 'ie', '▁s', 'un', '▁a', '▁w', '▁i', 'st', 'ein', 'ge', '▁die', '▁m', 'ich', '▁b', '▁un', 'an', 'te', '▁v', 'sch', '▁h', '▁da', '▁n', 'on', 'es', '▁z', '▁k', '▁f', '▁der', 'in', '▁ein', '▁au', '▁und', 'gen', 'it', 'll', 'or', 'ur', '▁in', 'ss', 'ar', 'at', '▁ge', 'ir', 'hr', 'ung', 'ten', '▁g', '▁er', 'em', 'den', 'al', '▁zu', 'au', 'der', '▁l', '▁p', '▁wir', 'icht', 'de', '▁r', '▁ver', 'lich', 'eit', '▁be', '▁an', '▁das', 'ig', 'ber', 'ier', 'ür', '▁e', 'isch', '▁t', 'ben', '▁ist', 'mm', 'ach', '▁den', 'ter', 'se', 'ion', '▁sie', '▁mit', '▁sch', 'tz', '▁nicht', '▁j', '▁auf', '▁es', 'ent', '▁st', 'el', 'ol', 'ra', 'um', 'ro', '▁auch', '▁ich', '▁von', '▁so', 'ck', 'ft', 're', '▁sich', '▁al', '▁für', 'and', 'ann', '▁dass', 'am', '▁eine', '▁dies', 'op', '▁aus', '▁des', 'eu', 'kt', 'is', '▁ko', 'eich', '▁vor', '▁im', '▁wer', '▁ha', '▁sein', 'hn', 'le', '▁re', 'ehr', 'über', 'ischen', 'och', '▁o', 'sp', '▁als', 'be', '▁dem', '▁eur', '▁war', 'än', 'ind', 'ste', 'ssen', 'il', 'hm', 'as', '▁wie', '▁bei', 'lle', 'ff', 'ungen', '▁europ', '▁hat', 'nen', '▁werden', '▁um', '▁über', 'wei', '▁ihr', '▁nach', 'rau', '▁sind', 'rei', 'hen', 'hl', 'iel', '▁haben', 'llen', 'ahr', '▁her', 'ne', 'ik', 'mmen', 'scha', '▁uns', '▁aber', '▁unter', 'cht', 'ag', 'chen', 'sten', 'lie', 'ort', '▁wur', '▁ab', '▁ent', 'anz', 'ien', '▁ber', 'aus', '▁sp', 'ische', 'ut', 'eil', '▁wird', 'ger', 'hal', '▁je', 'rä', 'ri', 'end', '▁wen', 'zu', 'ation', 'iss', 'uss', '▁europä', 'schaft', 'ün', '▁ar', 'ver', 'ige', 'igen', '▁gr', 'ön', '▁diese', 'gt', '▁le', 'ern', '▁wo', 'urch', 'tzt', 'la', '▁komm', 'ungs', '▁all', '▁noch', '▁kon', '▁wurde', '▁man', 'sen', 'rie', '▁hier', '▁mehr', '▁pro', 'ör', 'ät', '▁ste', '▁zur', '▁einen', 'und', 'tw', '▁zwei', '▁nur', '▁durch', '▁gl', '▁kön', 'us', 'iv', 'är', '▁wenn', 'ht', 'ere', 'eine', 'ru', 'bt', 'ück', 'heit', '▁kl', 'he', '▁mü', 'hmen', 'dern', 'ul', 'ken', 'om', 'schen', 'fen', 'her', 'tet', 'iert', '▁par', '▁herr', '▁einer', 'ab', 'ang', '▁was', 'ing', 'aat', 'chte', 'lichen', '▁se', '▁dar', '▁zum', 'mit', 'keit', 'schl', '▁dieser', 'ieren', '▁weit', 'rit', '▁kann', '▁hin', '▁kommiss', 'ko', '▁we', '▁bes', 'ster', 'ühr', '▁mö', '▁gew', '▁müssen', '▁lie', 'liche', '▁jahr', '▁gem', '▁ander', '▁am', 'ick', 'olit', 'mal', 'beit', 'ad', 'gel', 'ot', 'bei', '▁zw', '▁sehr', 'wer', 'tel', '▁ihn', '▁dann', 'reich', 'sam', 'si', 'ffen', 'eut', '▁wi', '▁c', '▁können', 'mmer', 'we', '▁gen', 'bl', '▁fra', 'os', 'ßen', '▁soll', '▁sa', '▁gegen', '▁europäischen', '▁oder', '▁men', '▁en', '▁einem', 'ahl', 'all', 'geb', 'pf', 'bar', 'sa', '▁gro', 'acht', 'del', '▁fin', 'for', 'dent', '▁wieder', '▁muss', '▁ne', '▁wel', '▁ganz', 'zi', '▁ei', '▁gi', '▁ger', 'räsi', 'det', '▁bet', '▁neu', 'eute', 'na', '▁verb', 'che', 'gr', 'ze', 'glie', '▁mein', 'ass', 'setz', '▁kommission', '▁seine', 'imm', 'lam', 'ord', 'ran', '▁alle', '▁deut', '▁gel', '▁bl', 'ess', 'rin', '▁bis', 'äch', 'dem', 'glich', 'ähr', 'ierung', 'og', '▁menschen', 'ktion', '▁jetzt', 'aaten', 'kommen', '▁viel', 'ke', '▁zeit', '▁frau', '▁mitglie', '▁ihre', 'pt', '▁reg', '▁weiter', 'ner', 'innen', 'ichtig', '▁hatte', '▁europa', 'räsident', 'zen', '▁ja', '▁gemein', '▁diesem', '▁son', '▁gibt', '▁kolle', 'stell', '▁sel', '▁parlam', 'achen', 'eiten', '▁heute', 'rat', 'ist', '▁gef', '▁mich', 'äu', 'tschaft', '▁recht', '▁inter', 'andel', '▁parlament', 'ität', 'ok', '▁ange', '▁mitglied', 'utz', '▁land', '▁daß', 'pp', '▁vert', '▁geb', '▁ch', 'gl', '▁ihm', '▁arbeit', '▁immer', '▁dieses', 'hör', 'rü', 'est', '▁erf', 'igkeit', '▁gesch', 'bst', 'alt', '▁groß', '▁eu', '▁bür', 'kte', '▁eigen', '▁ma', '▁denn', 'olitik', '▁schon', 'du', '▁keine', 'dert', '▁fl', '▁vie', '▁la', '▁damit', '▁for', '▁ges', 'et', '▁ho', '▁eben', '▁zusa', 'qu', '▁drei', 'halt', '▁unser', 'ha', '▁europäische', 'mp', 'schie', 'iz', 'halb', 'ale', '▁präsident', '▁ö', '▁zusammen', '▁kein', 'fl', '▁mir', 'alen', 'änd', 'ionen', 'men', 'staaten', '▁union', '▁habe', '▁wei', 'ers', '▁gleich', '▁bürger', '▁dan', 'ant', 'fe', '▁de', 'äl', 'atz', 'ert', '▁doch', 'art', 'rag', '▁etw', 'ill', 'lin', '▁leben', '▁du', '▁brau', 'tes', 'gend', 'zeit', 'halten', '▁wurden', '▁seiner', '▁möchte', 'onder', 'oll', 'für', 'ma', '▁selbst', '▁wirk', 'führ', '▁sicher', '▁ins', '▁diesen', 'iger', 'wick', '▁lei', '▁verh', 'ßer', '▁teil', 'rechen', '▁wollen', '▁schw', 'nung', '▁te', '▁bericht', '▁mitgliedstaaten', 'spiel', 'nehmen', 'hne', '▁rat', '▁geht', 'teil', '▁haus', '▁sondern', '▁erw', '▁grund', '▁sol', 'angen', '▁schl', '▁waren', 'änder', '▁wor', 'vor', 'rach', 'ierte', '▁beg', 'fin', 'wir', 'kl', 'ob', 'stand', '▁verf', 'att', 'ließ', '▁dort', '▁ob', 'ga', '▁nat', 'adt', '▁ausge', '▁nun', '▁glau', 'ütz', 'land', 'eid', '▁ihnen', '▁weil', '▁pl', 'ud', 'ischer', 'ekt', 'stimm', '▁wichtig', 'annt', 'enz', '▁jed', '▁besch', '▁sei', '▁machen', '▁nie', 'hem', '▁entsch', 'me', 'ätz', 'tt', '▁welt', 'ho', 'fer', 'sel', '▁bek', 'aupt', '▁möglich', '▁ro', '▁frei', 'ften', '▁gu', '▁mar', '▁gemeinsam', '▁wirtschaft', '▁dafür', 'uch', '▁sagen', '▁unsere', '▁fol', '▁vier', 'auf', '▁dazu', '▁fa', 'ade', '▁entwick', '▁darau', 'ahren', '▁frage', '▁fün', '▁eines', '▁dabei', '▁führ', 'sk', '▁stra', '▁anderen', 'itz', '▁bez', '▁verl', '▁aller', 'blem', '▁bereit', '▁viele', 'hin', '▁finanz', '▁einge', '▁brauchen', '▁klar', 'nis', '▁hei', '▁etwas', '▁fünf', 'nte', 'weise', 'tete', '▁wür', '▁meine', '▁spiel', 'lung', '▁verw', '▁zurück', '▁letz', '▁richt', 'okrat', '▁sag', 'ild', '▁kam', '▁unterst', '▁gehör', '▁ohne', '▁wirklich', 'sicht', 'elt', '▁sozi', 'wo', '▁or', 'unden', '▁einz', 'ding', '▁zwischen', 'gte', 'ständ', 'ritt', '▁will', '▁abge', 'ap', 'str', 'sche', 'ive', 'ug', '▁einmal', '▁ihrer', '▁bin', 'igt', 'arbeit', '▁erfol', '▁ex', 'net', '▁darauf', '▁problem', '▁jedoch', 'geben', '▁kollegen', 'wie', 'ander', '▁mittel', '▁ta', 'fall', 'ler', 'äre', 'ragen', '▁deshalb', '▁handel', 'uß', 'ser', '▁vom', 'inn', 'id', 'unkt', '▁gut', 'äm', 'form', 'ange', 'gung', 'ausend', 'erst', 'fach', '▁em', '▁bede', '▁wissen', 'ker', '▁vers', '▁besonder', '▁alles', 'ult', '▁me', '▁also', '▁seinen', 'setzt', 'legen', 'kun', '▁min', '▁nation', 'anden', 'öl', '▁gerade', 'gie', 'twort', 'je', 'ähl', '▁hal', '▁na', '▁tun', 'spro', 'eicht', '▁fre', '▁gest', 'ffent', 'agt', '▁ra', '▁natür', '▁erst', 'pl', 'ahn', 'bau', '▁außer', 'kehr', 'äter', '▁polit', 'gem', 'stem', 'cher', 'ös', 'ition', 'stellt', '▁regel', '▁ihren', '▁sollte', '▁einige', '▁mal', '▁sollten', '▁zahl', 'mus', 'ater', '▁tausend', '▁di', '▁sy', '▁stadt', '▁kur', 'tern', '▁andere', '▁nichts', 'urg', 'isk', 'esen', '▁jahren', 'mar', 'igung', 'ald', 'richt', 'kunft', '▁allen', '▁fest', '▁weg', '▁liegt', 'gehen', '▁währ', '▁komp', '▁ersten', '▁natürlich', '▁star', 'dr', 'gra', '▁gar', 'äft', '▁seit', 'ommen', '▁kin', '▁beste', '▁klein', 'ossen', 'erk', 'zeu', '▁bau', '▁sagte', '▁unterstütz', 'schieden', '▁veran', 'eist', 'ält', '▁hö', '▁ern', '▁tat', '▁bereich', '▁kommissar', '▁fr', '▁bed', '▁kommen', '▁fe', '▁bereits', 'sehen', '▁allem', 'oren', '▁unternehmen', 'cken', '▁qu', 'ße', 'entr', 'nahmen', 'ard', '▁them', '▁dir', '▁neue', 'mittel', 'essen', '▁verein', 'od', '▁ben', 'lagen', 'uer', '▁später', 'min', 'recht', 'wegen', '▁grö', 'ßt', 'eln', 'olog', '▁seinem', '▁scha', '▁glaube', '▁rei', 'ichte', 'undert', 'orgen', '▁beim', '▁part', '▁nam', '▁ur', 'ffe', 'licher', '▁mus', 'satz', '▁beispiel', 'sst', '▁no', '▁daher', '▁darüber', 'sse', '▁kont', '▁wäre', 'gang', '▁hät', '▁worden', '▁vorschl', '▁fort', 'gesch', '▁lassen', '▁erreich', '▁hand', '▁kolleg', 'fä', 'zie', 'zig', '▁br', 'aten', 'tlich', 'im', '▁end', '▁ort', 'chn', 'sprechen', '▁demokrat', '▁disk', 'isse', 'stellen', 'tens', '▁haupt', 'rieg', '▁stand', '▁bit', 'anken', 'hol', 'ismus', 'ringen', 'ruck', '▁sehen', 'vers', 'gleich', '▁würde', '▁arbei', '▁su', 'bes', 'ow', 'lage', '▁get', '▁heraus', '▁län', 'gar', 'sell', '▁gesagt', '▁sü', '▁mon', 'ell', 'eiden', 'reten', '▁pers', '▁dien', 'so', 'aut', 'schutz', '▁', 'e', 'n', 'i', 'r', 's', 't', 'a', 'd', 'h', 'u', 'l', 'g', 'c', 'm', 'o', 'b', 'w', 'f', 'k', 'z', 'p', 'v', 'ü', 'ä', 'ö', 'j', 'ß', 'y', 'x', 'q', \"'\", '2', '0', '1', '3', '5', '7', 'š', '8', '9', '4', '6', 'č', 'å', 'ş', 'İ', 'ł'] vocabulary.\n"
     ]
    }
   ],
   "source": [
    "# Lets change the tokenizer vocabulary by passing the path to the new directory,\n",
    "asr_model.change_vocabulary(\n",
    "    new_tokenizer_dir=\"../data_preparation/data/processed/tokenizer/tokenizer_spe_bpe_v1024/\",\n",
    "    new_tokenizer_type=\"bpe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZ3sf2P26SiA"
   },
   "source": [
    "After this, our decoder has completely changed, but our encoder (where most of the weights are) remained intact.\n",
    "\n",
    "### Update Config\n",
    "\n",
    "Each NeMo model has a config embedded in it, which can be accessed via model.cfg. In general, this is the config that was used to construct the model.\n",
    "\n",
    "For pre-trained models, this config generally represents the config used to construct the model when it was trained. A nice benefit to this embedded config is that we can repurpose it to set up new data loaders, optimizers, schedulers, and even data augmentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-07-12 02:01:00 audio_to_text_dataset:171] dataset does not have explicitly defined labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-07-12 02:01:01 collections:173] Dataset loaded with 16009 files totalling 59.60 hours\n",
      "[NeMo I 2022-07-12 02:01:01 collections:174] 6647 files were filtered totalling 33.73 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-07-12 02:01:01 ctc_models:469] Model Trainer was not set before constructing the dataset, incorrect number of training batches will be used. Please set the trainer and rebuild the dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-07-12 02:01:03 collections:173] Dataset loaded with 41620 files totalling 88.22 hours\n",
      "[NeMo I 2022-07-12 02:01:03 collections:174] 0 files were filtered totalling 0.00 hours\n"
     ]
    }
   ],
   "source": [
    "USE_TARRED_DATASET = True\n",
    "\n",
    "if USE_TARRED_DATASET:\n",
    "    # Setup train, validation, test configs\n",
    "    with open_dict(asr_model.cfg):    \n",
    "      # Train dataset  (Concatenate train manifest cleaned and dev manifest cleaned)\n",
    "      asr_model.cfg.train_ds.manifest_filepath = './data/processed/tar/train/tarred_audio_manifest.json'\n",
    "      asr_model.cfg.train_ds.is_tarred = True\n",
    "      asr_model.cfg.train_ds.tarred_audio_filepaths='./data/processed/tar/train/audio_{0..127}.tar'\n",
    "\n",
    "      asr_model.cfg.train_ds.batch_size = 32\n",
    "      asr_model.cfg.train_ds.num_workers = 32\n",
    "      asr_model.cfg.train_ds.pin_memory = True\n",
    "      asr_model.cfg.train_ds.trim_silence = True\n",
    "\n",
    "      # Validation dataset  (Use test dataset as validation, since we train using train + dev)\n",
    "      asr_model.cfg.validation_ds.manifest_filepath = ['./data/processed/test_manifest_merged.json', './data/processed/dev_manifest_merged.json']\n",
    "      asr_model.cfg.validation_ds.batch_size = 32\n",
    "      asr_model.cfg.validation_ds.num_workers = 32\n",
    "      asr_model.cfg.validation_ds.pin_memory = True\n",
    "      asr_model.cfg.validation_ds.trim_silence = True\n",
    "else:\n",
    "    # Setup train, validation, test configs\n",
    "    with open_dict(asr_model.cfg):    \n",
    "      # Train dataset  (Concatenate train manifest cleaned and dev manifest cleaned)\n",
    "      asr_model.cfg.train_ds.manifest_filepath = './data/processed/train_manifest_merged.json'\n",
    "      asr_model.cfg.train_ds.batch_size = 32\n",
    "      asr_model.cfg.train_ds.num_workers = 32\n",
    "      asr_model.cfg.train_ds.pin_memory = True\n",
    "      asr_model.cfg.train_ds.trim_silence = True\n",
    "\n",
    "      # Validation dataset  (Use test dataset as validation, since we train using train + dev)\n",
    "      asr_model.cfg.validation_ds.manifest_filepath = ['./data/processed/test_manifest_merged.json', './data/processed/dev_manifest_merged.json']\n",
    "      asr_model.cfg.validation_ds.batch_size = 32\n",
    "      asr_model.cfg.validation_ds.num_workers = 32\n",
    "      asr_model.cfg.validation_ds.pin_memory = True\n",
    "      asr_model.cfg.validation_ds.trim_silence = True\n",
    "\n",
    "# Point to the new train and validation data for fine-tuning\n",
    "asr_model.setup_training_data(train_data_config=asr_model.cfg.train_ds)\n",
    "asr_model.setup_validation_data(val_data_config=asr_model.cfg.validation_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up optimizer and scheduler\n",
    "\n",
    "When fine-tuning character models, it is generally advised to use a lower learning rate and reduced warmup. A reduced learning rate helps preserve the pre-trained weights of the encoder. Since the fine-tuning dataset is generally smaller than the original training dataset, the warmup steps would be far too much for the smaller fine-tuning dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7m_CRtH46BjO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: novograd\n",
      "lr: 0.05\n",
      "betas:\n",
      "- 0.8\n",
      "- 0.25\n",
      "weight_decay: 0.001\n",
      "sched:\n",
      "  name: CosineAnnealing\n",
      "  warmup_steps: 1000\n",
      "  warmup_ratio: null\n",
      "  min_lr: 1.0e-05\n",
      "  last_epoch: -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Original optimizer + scheduler\n",
    "print(OmegaConf.to_yaml(asr_model.cfg.optim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the smaller learning rate we set before\n",
    "with open_dict(asr_model.cfg.optim):\n",
    "  asr_model.cfg.optim.name=\"adamw\"\n",
    "  asr_model.cfg.optim.lr = 0.01\n",
    "  asr_model.cfg.optim.betas = [0.8, 0.25]  # from paper\n",
    "  asr_model.cfg.optim.weight_decay = 0.001  # Original weight decay\n",
    "  asr_model.cfg.optim.sched.warmup_steps = None  # Remove default number of steps of warmup\n",
    "  asr_model.cfg.optim.sched.warmup_ratio = 0.05  # 5 % warmup\n",
    "  asr_model.cfg.optim.sched.min_lr = 1e-5\n",
    "  asr_model.cfg.optim.sched.max_steps = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "And now we can create a PyTorch Lightning trainer and call `fit`. To increase training speed, we can leverage the mixed precision training mode. In this notebook, we demonstrate training with 1 GPUs. To train with 8 GPUs, execute the [train.py](train.py) script in a shell terminal.\n",
    "\n",
    "Notes:\n",
    "- Even with cross-language transfer learning, the model will still take a few hundreds epochs to train to convergence. \n",
    "- To stabilize training and avoid NAN loss issues, increase the global batch size to the range of [256, 2048]. On devices with small memory, this can be achieved by setting an appropriate number of the `accumulate_grad_batches`.\n",
    "- `asr_model.cfg.train_ds.batch_size` denotes the per-device batchsize. The global batch size will be `batch_size* #nodes * GPUs per node * accumulate_grad_batches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fs2aK7xB6pAd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[NeMo W 2022-07-12 02:01:03 modelPT:475] Trainer wasn't specified in model constructor. Make sure that you really wanted it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-07-12 02:01:03 modelPT:587] Optimizer config = AdamW (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.8, 0.25]\n",
      "        eps: 1e-08\n",
      "        lr: 0.01\n",
      "        weight_decay: 0.001\n",
      "    )\n",
      "[NeMo I 2022-07-12 02:01:03 lr_scheduler:833] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f0277d11ee0>\" \n",
      "    will be used during training (effective maximum steps = 50000) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.05\n",
      "    min_lr: 1.0e-05\n",
      "    last_epoch: -1\n",
      "    max_steps: 50000\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name              | Type                              | Params\n",
      "------------------------------------------------------------------------\n",
      "0 | preprocessor      | AudioToMelSpectrogramPreprocessor | 0     \n",
      "1 | encoder           | ConvASREncoder                    | 142 M \n",
      "2 | spec_augmentation | SpectrogramAugmentation           | 0     \n",
      "3 | _wer              | WERBPE                            | 0     \n",
      "4 | decoder           | ConvASRDecoder                    | 1.1 M \n",
      "5 | loss              | CTCLoss                           | 0     \n",
      "------------------------------------------------------------------------\n",
      "143 M     Trainable params\n",
      "0         Non-trainable params\n",
      "143 M     Total params\n",
      "286.237   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5490b06bd740bbba90c98f6e1f1f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-07-12 02:01:06 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:92: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n",
      "      rank_zero_warn(\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d02e034a3f14b31bafaa11a0aac5738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-07-12 02:01:27 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "      warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "    \n",
      "[NeMo W 2022-07-12 02:03:01 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "      rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=10,\n",
    "    monitor=\"val_wer\",\n",
    "    mode=\"min\",\n",
    "    dirpath=\"./checkpoint-dir\",\n",
    "    filename=\"citrinet-DE-{epoch:02d}\",\n",
    "    save_on_train_epoch_end=True,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(precision=16, \n",
    "                     devices=1, \n",
    "                     accelerator='gpu',                        \n",
    "                     max_epochs=500,                      \n",
    "                     default_root_dir=\"./checkpoint/\",\n",
    "                     accumulate_grad_batches=32, # For a global batch size of 32*1*32 = 1024\n",
    "                     callbacks=[checkpoint_callback])\n",
    "    \n",
    "trainer.fit(asr_model)\n",
    "trainer.save_to('de-asr-model.nemo')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ASR_with_Subword_Tokenization.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
