{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0038f5b9",
   "metadata": {},
   "source": [
    "# Filter, Mix, Train/Test and Split\n",
    "\n",
    "In this tutorial, we apply a simple filter to filter outlying data points. Then, we combine the three datasets into a single one and perform a train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e77b0be",
   "metadata": {},
   "source": [
    "# Filter\n",
    "\n",
    "We implement a simple filter here, to filter out samples that are too long (>20s), too short (<0.1s) or empty. \n",
    "We also replace special characters (other than those in the German alphabet, punctuation marks and numbers) with space.\n",
    "\n",
    "Advanced filter to weed out out samples that are considered 'noisy', that is, samples having very high WER (word error rate) or CER (character error rate) w.r.t. a previously trained German models are left as an advanced exercice for interested readers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "acb0deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "\n",
    "def load_jsonl(filepath):\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf_8') as fp:\n",
    "        inlines = fp.readlines()\n",
    "        for line in inlines:\n",
    "            if line.startswith(\"//\") or line.strip() == '':\n",
    "                continue\n",
    "            row = json.loads(line)\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "def dump_jsonl(filepath, data):\n",
    "    with open(filepath, 'w') as fp:\n",
    "        for datum in data:\n",
    "            row = json.dumps(datum, ensure_ascii=False)\n",
    "            fp.write(row)\n",
    "            fp.write('\\n')\n",
    "\n",
    "german_alphabet = set(\" abcdefghijklmnopqrstuvwxyzäöüß\"+ string.punctuation + \"0123456789\")\n",
    " \n",
    "def filter_manifest(input_manifest, output_manifest, min_duration=0.1, max_duration=20):\n",
    "    utterances = load_jsonl(input_manifest)\n",
    "    filtered_utterances = []\n",
    "    for i in tqdm.tqdm(range(len(utterances))):\n",
    "        if (utterances[i]['duration'] > max_duration) and (utterances[i]['duration'] < min_duration):\n",
    "            continue\n",
    "        \n",
    "        invalid_chars = set(utterances[i]['text'].lower())-german_alphabet\n",
    "        for c in invalid_chars: \n",
    "            utterances[i]['text']= re.sub(c, \" \", utterances[i]['text'])\n",
    "        \n",
    "        # Remove punctuation\n",
    "        utterances[i]['text'] = utterances[i]['text'].translate(str.maketrans('', '', ''.join(set(string.punctuation)-{\"'\"})))\n",
    "            \n",
    "        filtered_utterances.append(utterances[i])\n",
    "    \n",
    "    print(\"Number of utterances filtered out: \", len(utterances) - len(filtered_utterances))    \n",
    "    dump_jsonl(output_manifest, filtered_utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aece2710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  ./data/processed/mls/mls_train_manifest_normalized.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 23497/23497 [00:00<00:00, 37217.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances filtered out:  0\n",
      "Processing  ./data/processed/mls/mls_dev_manifest_normalized.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 3469/3469 [00:00<00:00, 34964.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances filtered out:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  ./data/processed/mls/mls_test_manifest_normalized.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 3394/3394 [00:00<00:00, 35854.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances filtered out:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  ./data/processed/voxpopuli/voxpopuli_train_manifest_normalized.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 108473/108473 [00:02<00:00, 47112.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances filtered out:  0\n",
      "Processing  ./data/processed/voxpopuli/voxpopuli_dev_manifest_normalized.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 2109/2109 [00:00<00:00, 46509.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances filtered out:  0\n",
      "Processing  ./data/processed/voxpopuli/voxpopuli_test_manifest_normalized.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 1968/1968 [00:00<00:00, 45736.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances filtered out:  0\n",
      "Processing  ./data/processed/mcv/mcv_train_manifest_normalized.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 196403/196403 [00:02<00:00, 73612.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances filtered out:  0\n",
      "Processing  ./data/processed/mcv/mcv_dev_manifest_normalized.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 15340/15340 [00:00<00:00, 75184.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances filtered out:  0\n",
      "Processing  ./data/processed/mcv/mcv_test_manifest_normalized.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 15340/15340 [00:00<00:00, 77212.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances filtered out:  0\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['mls', 'voxpopuli', 'mcv']:\n",
    "    for subset in ['train', 'dev', 'test']:        \n",
    "        input_manifest = os.path.join('./data/processed/', dataset, f\"{dataset}_{subset}_manifest_normalized.json\")\n",
    "        output_manifest = os.path.join('./data/processed/', dataset, f\"{dataset}_{subset}_manifest_normalized_filtered.json\")\n",
    "        print(\"Processing \", input_manifest)\n",
    "        filter_manifest(input_manifest, output_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa548517",
   "metadata": {},
   "source": [
    "## Mix and Train/Test Split\n",
    "\n",
    "We keep the train/dev/test structure of the original datasets, and simply merge them together. \n",
    "For other application where certain dataset is over- or under-represented, one might want to apply over sampling or undersampling instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce64e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  mls train\n",
      "Processing  voxpopuli train\n",
      "Processing  mcv train\n"
     ]
    }
   ],
   "source": [
    "for subset in ['train', 'dev', 'test']:\n",
    "    merged_manifest = []\n",
    "    for dataset in ['mls', 'voxpopuli', 'mcv']:    \n",
    "        print(\"Processing \", dataset, subset)\n",
    "        merged_manifest.extend(load_jsonl(os.path.join('./data/processed/', dataset, f\"{dataset}_{subset}_manifest_normalized_filtered.json\")))\n",
    "    output_manifest = os.path.join('./data/processed/', f\"{subset}_manifest_merged.json\")\n",
    "    dump_jsonl(output_manifest, merged_manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b8212f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
