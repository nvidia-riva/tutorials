{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAO - TTS FastPitch/HiFi-GAN Riva Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Train Adapt Optimize (TAO) Toolkit](https://developer.nvidia.com/tao-toolkit) provides the capability to export your model in a format that can be deployed using [NVIDIA Riva](https://developer.nvidia.com/riva), a highly performant application framework for multi-modal conversational AI services using GPUs.\n",
    "\n",
    "This tutorial explores taking 2 `.riva models`, the result of `tao spectro_gen` and `tao vocoder` commands, and leveraging the Riva ServiceMaker framework to aggregate all the necessary artifacts for the Riva deployment to a target environment. Once the models are deployed in Riva, you can issue inference requests to the server. We will demonstrate how quick and straightforward this whole process is.\n",
    "\n",
    "---\n",
    "## Learning Objectives\n",
    "In this tutorial, you will learn how to:  \n",
    "- use Riva ServiceMaker to take a TAO exported .riva and convert it to .rmir.\n",
    "- deploy the model(s) locally on the Riva server.\n",
    "- send inference requests from a demo client using Riva API bindings.\n",
    "\n",
    "---\n",
    "## Prerequisites\n",
    "\n",
    "To follow along, ensure you:\n",
    "- have access to NVIDIA NGC and are able to download the Riva Quick Start [resources](https://ngc.nvidia.com/catalog/resources/nvidia:riva:riva_quickstart)\n",
    "- have a `.riva` model file that you want to deploy. You can obtain this from `tao <task> export` (with `export_format=RIVA`). Refer to the [Text to Speech tutorial](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/resources/texttospeech_notebook) on Speech Synthesis using Train Adapt Optimize (TAO) Toolkit for more details on training and exporting a `.riva` model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Riva ServiceMaker\n",
    "ServiceMaker is a set of tools that aggregates all the necessary artifacts (models, files, configurations, and user settings) for Riva deployment to a target environment. It has two main components:\n",
    "\n",
    "* `riva-build`\n",
    "* `riva-deploy`\n",
    "\n",
    "### Riva-build\n",
    "\n",
    "This step helps build a Riva-ready version of the model. It’s only output is an intermediate format (called a Riva Model Intermediate Representation (.rmir)) of an end-to-end pipeline for the supported services within Riva. Let’s consider two TTS models:\n",
    "\n",
    "* [FastPitch](https://ngc.nvidia.com/catalog/models/nvidia:tao:speechsynthesis_english_fastpitch) (spectrogram generator)\n",
    "* [HiFi-GAN](https://ngc.nvidia.com/catalog/models/nvidia:tao:speechsynthesis_hifigan) (vocoder).<br>\n",
    "\n",
    "`riva-build` is responsible for the combination of one or more exported models (`.riva` files) into a single file\n",
    "containing an intermediate format called `.rmir`. This file contains a\n",
    "deployment-agnostic specification of the whole end-to-end pipeline along with all the assets required for the\n",
    "final deployment and inference. Refer to the [Riva documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tts/tts-custom.html#fastpitch-and-hifi-gan) for more information.\n",
    "\n",
    "### Riva-deploy\n",
    "\n",
    "The deployment tool takes as input one or more `.rmir` files and a target model repository directory. It creates an ensemble configuration specifying the pipeline for\n",
    "the execution and finally writes all those assets to the output model repository directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this tutorial, we will only be using the `riva-build` component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: UPDATE THESE PATHS \n",
    "\n",
    "# ServiceMaker Docker\n",
    "RIVA_SM_CONTAINER = \"<add container name>\"\n",
    "\n",
    "# Directory where the .riva models are stored $MODEL_LOC/*.riva\n",
    "# Both the FastPitch_22k_LJS.riva and HifiGAN_22k_LJS.riva models should be present\n",
    "MODEL_LOC = \"<add path to model location>\"\n",
    "\n",
    "# Name of the .riva file\n",
    "SPECTRO_GEN_MODEL_NAME = \"<add model name>\"\n",
    "VOCODER_MODEL_NAME = \"<add model name>\"\n",
    "\n",
    "# Key that model is encrypted with, while exporting with TAO\n",
    "KEY = \"<add encryption key used for trained model>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the auxillary files for RIVA to help enhance the quality of the audio output.\n",
    "!ngc registry model download-version \"nvidia/tao/speechsynthesis_en_us_auxiliary_files:deployable_v1.0\" --dest $MODEL_LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ServiceMaker docker\n",
    "! docker pull $RIVA_SM_CONTAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a multi-speaker model, please un-comment the command below and run the following command.\n",
    "! mkdir -p $MODEL_LOC/rmir\n",
    "! docker run --rm --gpus 0 -v $MODEL_LOC:/data $RIVA_SM_CONTAINER \\\n",
    "             riva-build speech_synthesis /data/rmir/new_speaker.rmir:$KEY \\\n",
    "             /data/$SPECTRO_GEN_MODEL_NAME:$KEY \\\n",
    "             /data/$VOCODER_MODEL_NAME:$KEY \\\n",
    "             --voice_name=new_speaker \\\n",
    "             --subvoices=ljspeech:0,new_voice:1 \\\n",
    "             --abbreviations_file=/data/speechsynthesis_en_us_auxiliary_files_vdeployable_v1.0/abbr.txt \\\n",
    "             --arpabet_file=/data/speechsynthesis_en_us_auxiliary_files_vdeployable_v1.0/cmudict-0.7b-nv0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Start Riva Server\n",
    "Once the model repository is generated, we are ready to start the Riva server. From this step onwards you need to download the Riva QuickStart Resource from NGC. \n",
    "Set the path to the directory here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Riva QuickStart directory\n",
    "RIVA_DIR = \"<Path to the uncompressed folder downloaded from quickstart(include the folder name)>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we modify the `config.sh` file to enable the relevant Riva services (TTS in this case for FastPitch and HiFi-GAN), and provide the encryption key and path to the model repository (riva_model_loc) generated in the previous step.\n",
    "\n",
    "For example, if the above model repository is generated at `$MODEL_LOC/models`, then you can specify `riva_model_loc` as the same directory as `MODEL_LOC`\n",
    "\n",
    "Pretrained versions of models specified in `models_asr/nlp/tts` are fetched from NGC. Since we are using our custom model, we can comment it out in `models_tts` (and any others that are not relevant to the use case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### config.sh snippet\n",
    "```\n",
    "# Enable or Disable Riva Services \n",
    "service_enabled_asr=false                                                      ## MAKE CHANGES HERE\n",
    "service_enabled_nlp=false                                                      ## MAKE CHANGES HERE\n",
    "service_enabled_tts=true                                                     ## MAKE CHANGES HERE\n",
    "\n",
    "# Specify one or more GPUs to use\n",
    "# specifying more than one GPU is currently an experimental feature, and may result in undefined behaviours.\n",
    "gpus_to_use=\"device=0\"\n",
    "\n",
    "# Specify the encryption key to use to deploy models\n",
    "MODEL_DEPLOY_KEY=\"tlt_encode\"                                                  ## MAKE CHANGES HERE\n",
    "\n",
    "# Locations to use for storing models artifacts\n",
    "#\n",
    "# If an absolute path is specified, the data will be written to that location\n",
    "# Otherwise, a docker volume will be used (default).\n",
    "#\n",
    "# riva_init.sh will create a `rmir` and `models` directory in the volume or\n",
    "# path specified. \n",
    "#\n",
    "# RMIR ($riva_model_loc/rmir)\n",
    "# Riva uses an intermediate representation (RMIR) for models\n",
    "# that are ready to deploy but not yet fully optimized for deployment. Pretrained\n",
    "# versions can be obtained from NGC (by specifying NGC models below) and will be\n",
    "# downloaded to $riva_model_loc/rmir by `riva_init.sh`\n",
    "# \n",
    "# Custom models produced by NeMo or TAO and prepared using riva-build\n",
    "# may also be copied manually to this location $(riva_model_loc/rmir).\n",
    "#\n",
    "# Models ($riva_model_loc/models)\n",
    "# During the riva_init process, the RMIR files in $riva_model_loc/rmir\n",
    "# are inspected and optimized for deployment. The optimized versions are\n",
    "# stored in $riva_model_loc/models. The riva server exclusively uses these\n",
    "# optimized versions.\n",
    "riva_model_loc=\"<add path>\"                              ## MAKE CHANGES HERE (Replace with MODEL_LOC)    \n",
    "\n",
    "# The default RMIRs are downloaded from NGC by default in the above $riva_rmir_loc directory\n",
    "# If you'd like to skip the download from NGC and use the existing RMIRs in the $riva_rmir_loc\n",
    "# then set the below $use_existing_rmirs flag to true. You can also deploy your set of custom\n",
    "# RMIRs by keeping them in the riva_rmir_loc dir and use this quickstart script with the\n",
    "# below flag to deploy them all together.\n",
    "use_existing_rmirs=false                                ## MAKE CHANGES HERE (Set to true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have permission to execute these scripts\n",
    "! cd $RIVA_DIR && chmod +x ./riva_init.sh && chmod +x ./riva_start.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Riva Init. This will fetch the containers/models\n",
    "# YOU CAN SKIP THIS STEP IF YOU DID RIVA DEPLOY\n",
    "! cd $RIVA_DIR && ./riva_init.sh config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Riva Start. This will deploy your model(s).\n",
    "! cd $RIVA_DIR && ./riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Inference\n",
    "Once the Riva server is up and running with your models, you can send inference requests querying the server. \n",
    "\n",
    "To send gRPC requests, install the Riva Python API bindings for the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install client API bindings\n",
    "! pip install nvidia-riva-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the Riva server and run inference\n",
    "Now, we can query the Riva server; let’s get started. The following cell queries the Riva server (using gRPC) to yield a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import soundfile\n",
    "import riva.client\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "\n",
    "server = \"localhost:50051\"                # location of riva server\n",
    "auth = riva.client.Auth(uri=server)\n",
    "tts_service = riva.client.SpeechSynthesisService(auth)\n",
    "\n",
    "\n",
    "text = \"Is it recognize speech or wreck a nice beach?\"\n",
    "language_code = \"en-US\"                   # currently required to be \"en-US\"\n",
    "sample_rate_hz = 22050                    # the desired sample rate\n",
    "voice_name = \"new_speaker.new_voice\"      # subvoice to generate the audio output.\n",
    "data_type = np.int16                      # For RIVA version < 1.10.0 please set this to np.float32\n",
    "\n",
    "resp = tts_service.synthesize(text, voice_name=voice_name, language_code=language_code, sample_rate_hz=sample_rate_hz)\n",
    "audio = resp.audio\n",
    "meta = resp.meta\n",
    "processed_text = meta.processed_text\n",
    "predicted_durations = meta.predicted_durations\n",
    "\n",
    "audio_samples = np.frombuffer(resp.audio, dtype=data_type)\n",
    "print(processed_text)\n",
    "ipd.Audio(audio_samples, rate=sample_rate_hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can stop the Riva Servicemaker container (and thus shut down Riva Server) before shutting down the Jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker container stop riva-speech"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
