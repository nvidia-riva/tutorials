{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_asr_asr-python-advanced-tao-ngram-pretrain/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to pretrain a Riva ASR Language Modeling (n-gram) with TAO Toolkit\n",
    "This tutorial walks you through the pretraining of Riva ASR language modeling (n-gram) with Train Adapt Optimize (TAO) Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVIDIA Riva Overview\n",
    "\n",
    "NVIDIA Riva is a GPU-accelerated SDK for building speech AI applications that are customized for your use case and deliver real-time performance. <br/>\n",
    "Riva offers a rich set of speech and natural language understanding services such as:\n",
    "\n",
    "- Automated speech recognition (ASR)\n",
    "- Text-to-Speech synthesis (TTS)\n",
    "- A collection of natural language processing (NLP) services, such as named entity recognition (NER), punctuation, and intent classification.\n",
    "\n",
    "In this tutorial, we will pretrain Riva ASR language modeling (n-gram) with TAO Toolkit. <br> \n",
    "To understand the basics of Riva ASR APIs, refer to [Getting started with Riva ASR in Python](https://github.com/nvidia-riva/tutorials/blob/dev/22.04/asr-python-basics.ipynb). <br>\n",
    "\n",
    "For more information about Riva, refer to the [Riva developer documentation](https://developer.nvidia.com/riva)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAO Toolkit\n",
    "Train Adapt Optimize (TAO) Toolkit is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data. Developers, researchers, and software partners building intelligent vision AI applications and services can bring their own data to fine-tune pre-trained models instead of going through the hassle of training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Train Adapt Optimize (TAO) Toolkit](https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning extracts learned features from an existing neural network into a new one. Transfer learning is often used when creating a large training dataset is not feasible. The goal of this toolkit is to reduce that 80 hour workload to an 8 hour workload, which can enable data scientists to have considerably more train-test iterations in the same time frame.\n",
    "\n",
    "Let's see this in action with a use case for Automatic Speech Recognition Language Modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='isc-task-description'></a>\n",
    "## Language Modeling\n",
    "\n",
    "### Task Description\n",
    "\n",
    "Language modeling returns a probability distribution over a sequence of words. Besides assigning a probability to a sequence of words, the language models also assign a probability for the likelihood of a given word (or a sequence of words) that follows a sequence of words. <br>\n",
    "\n",
    "> The sentence:  **all of a sudden I notice three guys standing on the sidewalk**\n",
    "> would be scored higher than \n",
    "> the sentence: **on guys all I of notice sidewalk three a sudden standing the** by the language model. <br>\n",
    "\n",
    "A language model trained on large corpus can significantly improve the accuracy of an ASR system as suggested in recent research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram Language Model\n",
    "There are primarily two types of language models:\n",
    "\n",
    "- **N-gram language models**: These models use frequency of n-grams to learn the probability distribution over words. Two benefits of N-gram language models are simplicity and scalability – with a larger `n`, a model can store more context with a well-understood space–time tradeoff, enabling small experiments to scale up efficiently.\n",
    "- **Neural language models**: These models use different kinds of neural networks to model the probability distribution over words, and have surpassed the N-gram language models in the ability to model language, but are generally slower to evaluate.\n",
    "\n",
    "In this tutorial, we will show how to train, evaluate, and optionally fine-tune an [N-gram language model](https://web.stanford.edu/~jurafsky/slp3/3.pdf) leveraging TAO Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Let's Dig in: Riva Language Modeling using TAO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing and setting up TAO\n",
    "Install TAO Toolkit inside a Python virtual environment. We recommend performing this step first and then launching the tutorial from the virtual environment.\n",
    "\n",
    "It's a simple `pip` install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nvidia-pyindex\n",
    "! pip install nvidia-tao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the Docker image versions and the tasks that TAO can perform, use the `tao info` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao info --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to installing the TAO Toolkit package, ensure you meet the following software requirements:\n",
    "\n",
    "1. Python 3.6.9\n",
    "2. `docker-ce` > 19.03.5\n",
    "3. `docker-API` 1.40\n",
    "4. `nvidia-container-toolkit` > 1.3.0-1\n",
    "5. `nvidia-container-runtime` > 3.4.0-1\n",
    "6. `nvidia-docker2` > 2.5.0-1\n",
    "7. `nvidia-driver` >= 455.23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if the GPU device(s) is visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='isc-prepare-data'></a>\n",
    "### Preparing the dataset\n",
    "#### Librispeech LM Normalized dataset\n",
    "For this tutorial, we use the normalized version of the LibriSpeech LM dataset to train our N-gram language model. The normalized version of the LibriSpeech LM dataset is available [here](https://www.openslr.org/11/).\n",
    "\n",
    "#### LibriSpeech dev-clean dataset\n",
    "For this tutorial, we also use the clean version of the LibriSpeech development set to evaluate our N-gram language model. The clean version of the LibriSpeech development set is available [here](https://www.openslr.org/12/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LibriSpeech LM Normalized dataset\n",
    "The training data is publicly available [here](https://www.openslr.org/resources/11/librispeech-lm-corpus.tgz) and can be downloaded directly.#### Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# IMPORTANT NOTE: Set the path to a folder where you want your data and results to be saved.\n",
    "# TODO\n",
    "DATA_DOWNLOAD_DIR = \"<YOUR_PATH_TO_DATA_DIR>\"\n",
    "assert os.path.exists(DATA_DOWNLOAD_DIR), \"Provided DATA_DOWNLOAD_DIR does not exist.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Ensure that wget and unzip utilities are available. If not, install them.\n",
    "!wget 'https://www.openslr.org/resources/11/librispeech-lm-norm.txt.gz' -P $DATA_DOWNLOAD_DIR\n",
    "\n",
    "# Extract the data\n",
    "!gzip -dk $DATA_DOWNLOAD_DIR/librispeech-lm-norm.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LibriSpeech dev-clean dataset\n",
    "The evaluation data is publicly available [here](https://www.openslr.org/resources/12/dev-clean.tar.gz) and can be downloaded directly. We provided a Python script to download and preprocess the dataset for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scripts to download and preprocess LibriSpeech dev-clean\n",
    "\"\"\"\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy\n",
    "\n",
    "LOG_STR = \" To regenerate this file, please, remove it.\"\n",
    "\n",
    "def find_transcript_files(dir):\n",
    "    files = []\n",
    "    for dirpath, _, filenames in os.walk(dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".trans.txt\"):\n",
    "                files.append(os.path.join(dirpath, filename))\n",
    "    return files\n",
    "\n",
    "def transcript_to_list(file):\n",
    "    audio_path = os.path.dirname(file)\n",
    "    ret = []\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            file_id, trans = line.strip().split(\" \", 1)\n",
    "            audio_file = os.path.abspath(os.path.join(audio_path, file_id + \".flac\"))\n",
    "            duration = 0  # We are not using the audio\n",
    "            ret.append([file_id, audio_file, str(duration), trans.lower()])\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    name = \"dev-clean\"\n",
    "    data_path = os.path.join(DATA_DOWNLOAD_DIR, \"eval_data\")\n",
    "    text_path = os.path.join(DATA_DOWNLOAD_DIR, \"text\")\n",
    "    lists_path = os.path.join(DATA_DOWNLOAD_DIR, \"lists\")\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    os.makedirs(text_path, exist_ok=True)\n",
    "    os.makedirs(lists_path, exist_ok=True)\n",
    "    data_http = \"http://www.openslr.org/resources/12/\"\n",
    "\n",
    "    # Download the audio data\n",
    "    print(\"Downloading the evaluation data.\", flush=True)\n",
    "    if not os.path.exists(os.path.join(data_path, \"LibriSpeech\", name)):\n",
    "        print(\"Downloading and unpacking {}...\".format(name))\n",
    "        cmd = \"\"\"wget -c {http}{name}.tar.gz -P {path};\n",
    "                 yes n 2>/dev/null | gunzip {path}/{name}.tar.gz;\n",
    "                 tar -C {path} -xf {path}/{name}.tar\"\"\"\n",
    "        os.system(cmd.format(path=data_path, http=data_http, name=name))\n",
    "    else:\n",
    "        log_str = \"{} part of data exists, skip its downloading and unpacking\"\n",
    "        print(log_str.format(name) + LOG_STR, flush=True)\n",
    "\n",
    "    # Prepare the audio data\n",
    "    print(\"Converting data into necessary format.\", flush=True)\n",
    "    word_dict = {}\n",
    "    word_dict[name] = set()\n",
    "    src = os.path.join(data_path, \"LibriSpeech\", name)\n",
    "    assert os.path.exists(src), \"Unable to find the directory - '{src}'\".format(\n",
    "        src=src\n",
    "    )\n",
    "\n",
    "    dst_list = os.path.join(lists_path, name + \".lst\")\n",
    "    if os.path.exists(dst_list):\n",
    "        print(\n",
    "            \"Path {} exists, skip its generation.\".format(dst_list) + LOG_STR,\n",
    "            flush=True,\n",
    "        )\n",
    "        \n",
    "\n",
    "    print(\"Analyzing {src}...\".format(src=src), flush=True)\n",
    "    transcript_files = find_transcript_files(src)\n",
    "    transcript_files.sort()\n",
    "\n",
    "    print(\"Writing to {dst}...\".format(dst=dst_list), flush=True)\n",
    "    with Pool(processes=8) as p:\n",
    "        samples = list(p.imap(transcript_to_list, transcript_files))\n",
    "\n",
    "    with open(dst_list, \"w\") as fout:\n",
    "        for sp in samples:\n",
    "            for s in sp:\n",
    "                word_dict[name].update(s[-1].split(\" \"))\n",
    "                s[0] = name + \"-\" + s[0]\n",
    "                fout.write(\" \".join(s) + \"\\n\")\n",
    "\n",
    "    current_path = os.path.join(text_path, name + \".txt\")\n",
    "    if not os.path.exists(current_path):\n",
    "        with open(os.path.join(lists_path, name + \".lst\"), \"r\") as flist, open(\n",
    "            os.path.join(text_path, name + \".txt\"), \"w\"\n",
    "        ) as fout:\n",
    "            for line in flist:\n",
    "                fout.write(\" \".join(line.strip().split(\" \")[3:]) + \"\\n\")\n",
    "    else:\n",
    "        print(\n",
    "            \"Path {} exists, skip its generation.\".format(current_path) + LOG_STR,\n",
    "            flush=True,\n",
    "        )\n",
    "\n",
    "print(\"Done!\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of reducing the time this demo takes, we reduce the number of lines of the training dataset. Feel free to modify the number of used lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a random 100,000 lines for training\n",
    "!shuf -n 100000 $DATA_DOWNLOAD_DIR/librispeech-lm-norm.txt  > $DATA_DOWNLOAD_DIR/reduced_training.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TAO Toolkit workflow\n",
    "The rest of the tutorial demonstrates what a sample TAO Toolkit workflow looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting TAO Toolkit Mounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataset has been downloaded, an important step in using TAO Toolkit is to setup the directory mounts. The TAO Toolkit launcher uses Docker containers under the hood, and **for our data and results directory to be visible to Docker, they need to be mapped**. The launcher can be configured using the config file `~/.tao_mounts.json`. Apart from the mounts, you can also configure additional options like the environment variables and the amount of shared memory available to the TAO Toolkit launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The following code creates a sample `~/.tao_mounts.json`  file. Here, we can map directories in which we save the data, specs, results, and cache. You should configure it for your specific use case.  These directories are correctly visible to the Docker container. **Ensure that the source directories exist on your machine.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tee ~/.tao_mounts.json <<'EOF'\n",
    "{\n",
    "   \"Mounts\":[\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_DATA_DIR>\",\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_SPECS_DIR>\",\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_RESULTS_DIR>\",\n",
    "           \"destination\": \"/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_CACHE_DIR eg. /home/user/.cache>\",\n",
    "           \"destination\": \"/root/.cache\"\n",
    "       }\n",
    "   ]\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the source directories exist, if not, create them, Provide aboslute Paths\n",
    "SPECS_DIR_LOCAL = \"<YOUR_PATH_TO_SPECS_DIR>\"\"\n",
    "RESULT_DIR_LOCAL = \"<YOUR_PATH_TO_RESULTS_DIR>\"\n",
    "CACHE_DIR_LOCAL = \"<YOUR_PATH_TO_CACHE_DIR>\"\n",
    "! mkdir $SPECS_DIR_LOCAL\n",
    "! mkdir $RESULT_DIR_LOCAL\n",
    "! mkdir $CACHE_DIR_LOCAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users with basic knowledge of deep learning can get started building their own custom models using a simple specification file. It's essentially just one command each to run data preprocessing, training, fine-tuning, evaluation, inference, and export. All configurations happen through `.yaml` spec files. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Configuration/Specification Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The essence of all commands in TAO Toolkit lies within `.yaml` spec files. There are sample spec files already available for you to use directly or as a reference to create your own.  Through these spec files, you can tune many knobs like the model, dataset, hyperparameters, etc. Each command (like train, fine-tune, evaluate, etc.) should have a dedicated spec file with configurations pertinent to it. <br>\n",
    "\n",
    "Here is an example of the training spec file:\n",
    "\n",
    "---\n",
    "```\n",
    "model:\n",
    "  intermediate: True\n",
    "  order: 2\n",
    "  pruning:\n",
    "    - 0\n",
    "training_ds:\n",
    "  is_tarred: false\n",
    "  is_file: true\n",
    "  data_file: ???\n",
    "\n",
    "vocab_file: \"\"\n",
    "encryption_key: \"tlt_encode\"\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Downloading Specs\n",
    "Let's download the spec files. You may choose to modify/rewrite these specs, or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command. <br>\n",
    "\n",
    "The `-o` argument indicates the folder where the default specification files will be downloaded. The `-r` argument instructs the script on where to save the logs. **Ensure the `-o` argument points to an empty folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao n_gram download_specs \\\n",
    "    -r /results \\\n",
    "    -o /specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Data Convert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation for training/fine-tuning, we need to preprocess the dataset. The `tao n_gram dataset_convert` command can be used in conjunction with the appropriate configuration in the spec file. Here is the sample `dataset_convert.yaml` spec file we use:\n",
    "```\n",
    "# Dataset. Available options: [assistant]\n",
    "dataset_name: assistant\n",
    "\n",
    "# Extension of the files containing in dataset\n",
    "extension: ???\n",
    "\n",
    "# Path to the folder containing the dataset source files.\n",
    "source_data_dir: ???\n",
    "\n",
    "# Path to the output folder.\n",
    "target_data_file: ???\n",
    "\n",
    "```\n",
    " Take a look at the `.yaml` spec files we provide.\n",
    "As we show below, you can override the `source_data_dir` and `target_data_dir` options with the appropriate paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess training data (LibriSpeech LM Normalized)\n",
    "!tao n_gram dataset_convert \\\n",
    "            -e /specs/dataset_convert.yaml \\\n",
    "            -r /results/dataset_convert \\\n",
    "            extension=*.txt \\\n",
    "            source_data_dir=/data/reduced_training.txt \\\n",
    "            target_data_file=/data/preprocessed.txt\n",
    "\n",
    "# Preprocess evaluation data (LibriSpeech dev-clean)\n",
    "!tao n_gram dataset_convert \\\n",
    "            -e /specs/dataset_convert.yaml \\\n",
    "            -r /results/dataset_convert \\\n",
    "            extension=*.txt \\\n",
    "            source_data_dir=/data/text/dev-clean.txt \\\n",
    "            target_data_file=/data/preprocessed_dev_clean.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command preprocess the training and evaluation dataset using basic text preprocessings including converting lowercase, normalization, removing punctuation, and write the results into files named `preprocessed.txt` and `preprocessed_dev_clean.txt` for training and evaluation correspondingly. In both `preprocessed.txt` and `preprocessed_dev_clean.txt` files, each preprocessed sentence corresponds to a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='isc-training'></a>\n",
    "### Training / Fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model using TAO Toolkit is as simple as configuring your spec file and running the train command. The following code uses the `train.yaml` spec file available to you as reference. The spec file configurations can easily be overridden using the `tao-launcher` CLI. For example, below we override `model.order`, `model.pruning` and `training_ds.data_file` configurations to suit our needs. <br>\n",
    "\n",
    "For training an N-gram language model in TAO Toolkit, we use the `tao n_gram train` command with the following arguments:\n",
    "- `-e`: Path to the spec file\n",
    "- `-k`: User specified encryption key to use while saving/loading the model\n",
    "- `-r`: Path to a folder where the outputs should be written. Ensure this is mapped in the `tlt_mounts.json` file.\n",
    "- Any overrides to the spec file. For example, `model.order`.\n",
    "<br>\n",
    "\n",
    "\n",
    "For more information about these arguments, refer to the [TAO Toolkit Getting Started Guide](https://docs.nvidia.com/tao/tao-toolkit/text/overview.html). <br>\n",
    "`Note:` All file paths correspond to the destination mounted directory that is visible in the TAO Toolkit docker container used in backend.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao n_gram train \\\n",
    "            -e /specs/train.yaml \\\n",
    "            -r /results/base \\\n",
    "            training_ds.data_file=/data/preprocessed.txt \\\n",
    "            model.order=4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train command produces three files called `train_n_gram.arpa`, `train_n_gram.vocab` and `train_n_gram.kenlm_intermediate` saved at `$RESULTS_DIR_LOCAL/train/checkpoints`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='evaluation'></a>\n",
    "### Evaluation\n",
    "The evaluation spec `.yaml` is as simple as:\n",
    "\n",
    "```\n",
    "# Name of the `.arpa` or `.binary` file where the trained model will be restored from.\n",
    "restore_from: ???\n",
    "\n",
    "test_ds:\n",
    "  data_file: ???\n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao n_gram evaluate \\\n",
    "     -e /specs/evaluate.yaml \\\n",
    "     -r /results/evaluate \\\n",
    "     restore_from=/results/base/checkpoints/train_n_gram.arpa \\\n",
    "     test_ds.data_file=/data/preprocessed_dev_clean.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the evaluation gives us the perplexity of the N-gram language model on the evaluation (LibriSpeech dev-clean) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='isc-inference'></a>\n",
    "### Inference\n",
    "Inference using a trained `.arpa` or `.binary` model uses the `tao n_gram infer` command.  <br>\n",
    "The `infer.yaml` is also very simple, and we can directly give inputs for the model to run inference.\n",
    "```\n",
    "# \"Simulate\" user input:\n",
    "input_batch:\n",
    "  - 'set alarm for seven thirty am'\n",
    "  - 'lower volume by fifty percent'\n",
    "  - 'what is my schedule for tomorrow'\n",
    "\n",
    "restore_from: ???\n",
    "\n",
    "```\n",
    "\n",
    "Try out your own inputs as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao n_gram infer \\\n",
    "            -e /specs/infer.yaml \\\n",
    "            -r /results/infer \\\n",
    "            restore_from=/results/base/checkpoints/train_n_gram.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command returns the log likelihood, perplexity, and all n-grams for each of the input sequences that users provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='isc-export-riva'></a>\n",
    "### Export to Riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With TAO Toolkit, you can also export your model in a format that can deployed using [NVIDIA Riva](https://developer.nvidia.com/riva), a highly performant application framework for multi-modal conversational AI services using GPUs. The export command will convert the trained language model from `.arpa` to `.binary` with the option of quantizing the model binary. We will set `export_format` in the spec file to `RIVA` to create a `.riva` file which will contain the language model binary and its corresponding vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao n_gram export \\\n",
    "            -e /specs/export.yaml \\\n",
    "            -r /results/base \\\n",
    "            export_format=RIVA \\\n",
    "            export_to=exported-base.riva \\\n",
    "            restore_from=/results/base/checkpoints/train_n_gram.arpa \\\n",
    "            binary_type=trie \\\n",
    "            binary_q_bits=8 \\\n",
    "            binary_b_bits=7 \\\n",
    "            binary_a_bits=256         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is exported as `exported-model.binary` which is in a format suited for deployment in Riva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='isc-deploy'></a>\n",
    "## RIVA deployment with ASR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riva ServiceMaker\n",
    "Servicemaker is the set of tools that aggregates all the necessary artifacts (models, files, configurations, and user settings) for Riva deployment to a target environment. It has two main components as shown below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Riva-build\n",
    "\n",
    "This step helps build a Riva-ready version of the model. It’s only output is an intermediate format (called a RMIR) of an end to end pipeline for the supported services within Riva. We are taking a ASR Citrinet Model in consideration. Although same setup can be used for Conformer models too.<br>\n",
    "\n",
    "`riva-build` is responsible for the combination of one or more exported models (.riva files) into a single file containing an intermediate format called Riva Model Intermediate Representation (.rmir). This file contains a deployment-agnostic specification of the whole end-to-end pipeline along with all the assets required for the final deployment and inference. Please checkout the [documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/service-asr.html#pipeline-configuration) to find out more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Riva skills version\n",
    "RIVA_VERSION=\"2.6.0\"\n",
    "\n",
    "# ServiceMaker Docker\n",
    "RIVA_SM_CONTAINER = f\"nvcr.io/nvidia/riva/riva-speech:{RIVA_VERSION}-servicemaker\"\n",
    "\n",
    "# Riva API Docker\n",
    "RIVA_API_CONTAINER =f\"nvcr.io/nvidia/riva/riva-speech:{RIVA_VERSION}\"\n",
    "\n",
    "# Directory where the create model repo\n",
    "MODEL_LOC = \"<YOUR_PATH_TO_MODEL_REPO>\"\n",
    "\n",
    "# Name of the .riva file\n",
    "MODEL_NAME = \"nvidia/tao/speechtotext_en_us_citrinet:deployable_v3.0\"\n",
    "\n",
    "# Key that model is encrypted with, while exporting with TAO\n",
    "KEY = \"tlt_encode\"\n",
    "\n",
    "# NGC API KEY, can be generated from ngc.nvidia.com/setup\n",
    "NGC_API_KEY=\"<YOUR_NGC_API_KEY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ServiceMaker docker and latest riva ASR model\n",
    "! mkdir $MODEL_LOC\n",
    "! docker pull $RIVA_SM_CONTAINER\n",
    "! ngc registry model download-version $MODEL_NAME\n",
    "! mv speechtotext_en_us_citrinet_vdeployable_v3.0/citrinet-1024-Jarvis-asrset-3_0-encrypted.riva $MODEL_LOC/Citrinet.riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup Flashligh decoder\n",
    "The Flashlight decoder, deployed by default in Riva, is a lexicon-based decoder and only emits words that are present in the provided lexicon file.\n",
    "Vocabulary file: The vocabulary file is a flat text file containing a list of vocabulary words, each on its own line. For example:\n",
    "```\n",
    "the\n",
    "i\n",
    "to\n",
    "and\n",
    "a\n",
    "you\n",
    "of\n",
    "that\n",
    "```\n",
    "This file is used by the riva-build process to generate the lexicon file.\n",
    "\n",
    "Lexicon file: The lexicon file is a flat text file that contains the mapping of each vocabulary word to its tokenized form, e.g, sentencepiece tokens, separated by a tab. Below is an example:\n",
    "```\n",
    "with    ▁with\n",
    "not     ▁not\n",
    "this    ▁this\n",
    "just    ▁just\n",
    "my      ▁my\n",
    "as      ▁as\n",
    "don't   ▁don ' t\n",
    "```\n",
    "Note: Ultimately, the Riva decoder makes use only of the lexicon file directly at run time (but not the vocabulary file).\n",
    "\n",
    "Riva ServiceMaker automatically tokenizes the words in the vocabulary file to generate the lexicon file. It uses the correct tokenizer model that is packaged together with the acoustic model in the .riva file. By default, Riva generates 1 tokenized form for each word in the vocabulary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate vocabulary using base LM training data\n",
    "! cat $DATA_DOWNLOAD_DIR/preprocessed.txt | sed \"s/ /\\n/g\" | sort -u > $RESULT_DIR_LOCAL/base/dict_vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the RMIR file with trained Base Language Model\n",
    "! docker run -it --rm --gpus 0 -v $MODEL_LOC:/data \\\n",
    "            -v $RESULT_DIR_LOCAL/base:/lm \\\n",
    "            --name riva-service-maker-lm \\\n",
    "            $RIVA_SM_CONTAINER  -- \\\n",
    "            riva-build speech_recognition /data/base_asr.rmir:$KEY \\\n",
    "            /data/Citrinet.riva:$KEY \\\n",
    "            --ms_per_timestep=80 \\\n",
    "            --chunk_size=0.16 \\\n",
    "            --left_padding_size=1.92 \\\n",
    "            --right_padding_size=1.92 \\\n",
    "            --decoder_type=flashlight \\\n",
    "            --decoding_language_model_binary=/lm/exported-base.binary \\\n",
    "            --decoding_vocab=/lm/dict_vocab.txt \\\n",
    "            --flashlight_decoder.lm_weight=0.2 \\\n",
    "            --flashlight_decoder.word_insertion_score=0.2 \\\n",
    "            --flashlight_decoder.beam_threshold=20. \\\n",
    "            --featurizer.dither=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Riva-deploy\n",
    "\n",
    "The deployment tool takes as input one or more Riva Model Intermediate Representation (RMIR) files and a target model repository directory. It creates an ensemble configuration specifying the pipeline for the execution and finally writes all those assets to the output model repository directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax: riva-deploy -f dir-for-rmir/model.rmir:key output-dir-for-repository\n",
    "! docker run --rm --gpus 0 -v $MODEL_LOC:/data $RIVA_SM_CONTAINER -- \\\n",
    "            riva-deploy -f  /data/base_asr.rmir:$KEY /data/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Start Riva Server\n",
    "Once the model repository is generated, we are ready to start the Riva server. From this step onwards you need to download the Riva QuickStart Resource from NGC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Riva Quickstart\n",
    "! ngc registry resource download-version nvidia/riva/riva_quickstart:$RIVA_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### config.sh snippet\n",
    "```\n",
    "service_enabled_asr=true                                                              ## MAKE CHANGES HERE\n",
    "service_enabled_nlp=false                                                             ## MAKE CHANGES HERE\n",
    "service_enabled_tts=false                                                             ## MAKE CHANGES HERE\n",
    "\n",
    "# Enable Riva Enterprise\n",
    "# If enrolled in Enterprise, enable Riva Enterprise by setting configuration\n",
    "# here. You must explicitly acknowledge you have read and agree to the EULA.\n",
    "# RIVA_API_KEY=<ngc api key>                                                               \n",
    "# RIVA_API_NGC_ORG=<ngc organization>                                                             \n",
    "# RIVA_EULA=accept\n",
    "\n",
    "# Language code to fetch models of a specify language\n",
    "# Currently only ASR supports languages other than English\n",
    "# Supported language codes: en-US, de-DE, es-US, ru-RU, zh-CN, hi-IN, fr-FR\n",
    "# for any language other than English, set service_enabled_nlp and service_enabled_tts to False\n",
    "# for multiple languages enter space separated language codes.\n",
    "language_code=(\"en-US\")\n",
    "\n",
    "# ASR acoustic model architecture\n",
    "# Supported values are: conformer, citrinet_1024, citrinet_256 (en-US + arm64 only), jasper (en-US + amd64 only), quartznet (en-US + amd64 only)\n",
    "asr_acoustic_model=(\"conformer\")\n",
    "\n",
    "# Specify one or more GPUs to use\n",
    "# specifying more than one GPU is currently an experimental feature, and may result in undefined behaviours.\n",
    "gpus_to_use=\"device=0\"\n",
    "\n",
    "# Specify the encryption key to use to deploy models\n",
    "MODEL_DEPLOY_KEY=\"tlt_encode\"                                                        ## MAKE CHANGES HERE\n",
    "\n",
    "# Locations to use for storing models artifacts\n",
    "#\n",
    "# If an absolute path is specified, the data will be written to that location\n",
    "# Otherwise, a docker volume will be used (default).\n",
    "#\n",
    "# riva_init.sh will create a `rmir` and `models` directory in the volume or\n",
    "# path specified.\n",
    "#\n",
    "# RMIR ($riva_model_loc/rmir)\n",
    "# Riva uses an intermediate representation (RMIR) for models\n",
    "# that are ready to deploy but not yet fully optimized for deployment. Pretrained\n",
    "# versions can be obtained from NGC (by specifying NGC models below) and will be\n",
    "# downloaded to $riva_model_loc/rmir by `riva_init.sh`\n",
    "#\n",
    "# Custom models produced by NeMo or TLT and prepared using riva-build\n",
    "# may also be copied manually to this location $(riva_model_loc/rmir).\n",
    "#\n",
    "# Models ($riva_model_loc/models)\n",
    "# During the riva_init process, the RMIR files in $riva_model_loc/rmir\n",
    "# are inspected and optimized for deployment. The optimized versions are\n",
    "# stored in $riva_model_loc/models. The riva server exclusively uses these\n",
    "# optimized versions.\n",
    "riva_model_loc=\"riva-model-repo\"                                                  ## MAKE CHANGES HERE (Replace with MODEL_LOC)            \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have permission to execute these scripts\n",
    "! cd riva_quickstart_v$RIVA_VERSION && chmod +x ./riva_init.sh && chmod +x ./riva_start.sh && chmod +x ./riva_stop.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Riva Start. This will deploy your model(s).\n",
    "! cd riva_quickstart_v$RIVA_VERSION && ./riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Evaluation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: This data can be used only with NVIDIA’s products or services for evaluation and benchmarking purposes.\n",
    "! ngc registry resource  download-version --dest $DATA_DOWNLOAD_DIR nvstaging/tao/healthcare_eval_dataset:1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Inference\n",
    "Once the Riva server is up and running with your models, you can send inference requests querying the server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run --rm -v $DATA_DOWNLOAD_DIR/healthcare_eval_dataset_v1.0:/data  \\\n",
    "    --net=host $RIVA_API_CONTAINER -- \\\n",
    "     riva_streaming_asr_client \\\n",
    "        --automatic_punctuation=false \\\n",
    "        --interim_results=false \\\n",
    "        --word_time_offsets=false \\\n",
    "        --audio_file /data/general.json \\\n",
    "        --output_filename=/data/base_asr_on_base_output.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run --rm -v $DATA_DOWNLOAD_DIR/healthcare_eval_dataset_v1.0/:/data  \\\n",
    "    --net=host $RIVA_API_CONTAINER -- \\\n",
    "     riva_streaming_asr_client \\\n",
    "        --automatic_punctuation=false \\\n",
    "        --interim_results=false \\\n",
    "        --word_time_offsets=false \\\n",
    "        --audio_file /data/healthcare.json \\\n",
    "        --output_filename=/data/base_asr_on_domain_output.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate word error rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install jiwer\n",
    "from jiwer import wer\n",
    "import json\n",
    "\n",
    "def calculate_wer(ground_truth_manifest, asr_transcript):\n",
    "    data ={}\n",
    "    ground_truths = []\n",
    "    predictions = []\n",
    "    with open(ground_truth_manifest) as file:\n",
    "        for line in file:\n",
    "            dt = json.loads(line)\n",
    "            data[dt['audio_filepath']] = dt['text']\n",
    "    with open(asr_transcript) as file:\n",
    "        for line in file:\n",
    "            dt = json.loads(line)\n",
    "            if dt['audio_filepath'] in data:\n",
    "                ground_truths.append(data[dt['audio_filepath']])\n",
    "                predictions.append(dt['text'])\n",
    "    return round(100*wer(ground_truths, predictions), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"WER of base model on generic domain data\", calculate_wer(f\"{DATA_DOWNLOAD_DIR}/healthcare_eval_dataset_v1.0/general.json\", f\"{DATA_DOWNLOAD_DIR}/healthcare_eval_dataset_v1.0/base_asr_on_base_output.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WER of base model on Healthcare domain data\", calculate_wer(f\"{DATA_DOWNLOAD_DIR}/healthcare_eval_dataset_v1.0/healthcare.json\", f\"{DATA_DOWNLOAD_DIR}/healthcare_eval_dataset_v1.0/base_asr_on_domain_output.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above results show the model performace is well on general data but not on healthcare specific domain data. We can finetune the Language model on healthcare domain data to boost ASR performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='isc-Finetuning'></a>\n",
    "### Finetuning/Interpolation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuning process will continue training using a previously trained model by training a second model on new domain data and interpolating it with the original model. Finetuning requires the original model have intermediate enabled during training. A finetuned model cannot be used for finetuning again. <br>\n",
    "\n",
    "\n",
    "### Downloading and procesing domain data (healthcare) for LM finetuning\n",
    "We can make use of reddit data for this purpose. It is a collection of Corpuses of Reddit data built from Pushshift.io Reddit Corpus. Each Corpus contains posts and comments from an individual subreddit from its inception until Oct 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install convokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, download\n",
    "corpus = Corpus(download('subreddit-healthcare'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform basic text cleaning and generate domain data\n",
    "import string,re\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-z' ]+\", \"\", text.lower().strip())\n",
    "    text = ' '.join(text.split())\n",
    "    if len(text.split())> 5:\n",
    "        return text.strip()\n",
    "    \n",
    "with open(f'{DATA_DOWNLOAD_DIR}/domain_data_all.txt', 'w') as file:\n",
    "    for utt in corpus.iter_utterances():\n",
    "        text = clean_text(utt.text)\n",
    "        if text:\n",
    "            file.write(text+'\\n')\n",
    "            \n",
    "# Picking top 10000 lines from dataset\n",
    "! head -10000 $DATA_DOWNLOAD_DIR/domain_data_all.txt > $DATA_DOWNLOAD_DIR/domain_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuning process will continue training using a previously trained model by training a second model on new domain data and interpolating it with the original model. Finetuning requires the original model have intermediate enabled during training. A finetuned model cannot be used for finetuning again. <br>\n",
    "\n",
    "\n",
    "For Finetuning a N-gram language model in TAO Toolkit, we use the `tao n_gram finetune` command with the following args:\n",
    "- `-e`: Path to the spec file\n",
    "- `-k`: User specified encryption key to use while saving/loading the model\n",
    "- `-r`: Path to a folder where the outputs should be written. Make sure this is mapped in tlt_mounts.json\n",
    "- Any overrides to the spec file eg. `model.order`, `weight` etc\n",
    "<br>\n",
    "\n",
    "\n",
    "More details about these arguments are present in the [TAO Toolkit Getting Started Guide](https://docs.nvidia.com/tao/tao-toolkit/text/overview.html) <br>\n",
    "`Note:` All file paths correspond to the destination mounted directory that is visible in the TAO Toolkit docker container used in backend.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate the domain LM with base LM\n",
    "!tao n_gram finetune \\\n",
    "            -e /specs/finetune.yaml \\\n",
    "            -r /results \\\n",
    "            restore_from=/results/base/checkpoints/train_n_gram.kenlm_intermediate \\\n",
    "            tuning_ds.data_file=/data/domain_data.txt \\\n",
    "            model.order=4 \\\n",
    "            weight=0.6      # weight of domain specific model \\\n",
    "            -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export interpolated LM to Riva compatible format\n",
    "!tao n_gram export \\\n",
    "            -e /specs/export.yaml \\\n",
    "            -r /results/interpolated \\\n",
    "            export_format=RIVA \\\n",
    "            export_to=exported-model.riva \\\n",
    "            restore_from=/results/checkpoints/finetune_n_gram.arpa \\\n",
    "            binary_type=trie \\\n",
    "            binary_q_bits=8 \\\n",
    "            binary_b_bits=7 \\\n",
    "            binary_a_bits=256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolated LM is not generated at /results/interpolated/exported-model.binary. <br>\n",
    "We can now use this LM along with new vocabulary file to generate model repo for Domain specific ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add domain specific words to vocabulary file\n",
    "! cat $DATA_DOWNLOAD_DIR/domain_data.txt | sed \"s/ /\\n/g\" | sort -u > $RESULT_DIR_LOCAL/interpolated/dict_vocab_domain.txt\n",
    "! cat $RESULT_DIR_LOCAL/base/dict_vocab.txt $RESULT_DIR_LOCAL/interpolated/dict_vocab_domain.txt | sort -u > $RESULT_DIR_LOCAL/interpolated/dict_vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new model repo with interpolated LM. Set absolute path to create MODEL_LOC_DOMAIN\n",
    "MODEL_LOC_DOMAIN = \"<YOUR_PATH_TO_DOMAIN_MODEL_REPO>\"\n",
    "! mkdir $MODEL_LOC_DOMAIN\n",
    "! cp $MODEL_LOC/Citrinet.riva $MODEL_LOC_DOMAIN/\n",
    "! docker run -it --rm --gpus 0 -v $MODEL_LOC_DOMAIN:/data \\\n",
    "            -v $RESULT_DIR_LOCAL/interpolated:/lm \\\n",
    "            --name riva-service-maker-lm \\\n",
    "            $RIVA_SM_CONTAINER  -- \\\n",
    "            riva-build speech_recognition /data/interpolated_asr.rmir:$KEY \\\n",
    "            /data/Citrinet.riva:$KEY \\\n",
    "            --ms_per_timestep=80 \\\n",
    "            --chunk_size=0.16 \\\n",
    "            --left_padding_size=1.92 \\\n",
    "            --right_padding_size=1.92 \\\n",
    "            --decoder_type=flashlight \\\n",
    "            --decoding_language_model_binary=/lm/exported-model.binary \\\n",
    "            --decoding_vocab=/lm/dict_vocab.txt \\\n",
    "            --flashlight_decoder.lm_weight=0.2 \\\n",
    "            --flashlight_decoder.word_insertion_score=0.2 \\\n",
    "            --flashlight_decoder.beam_threshold=20. \\\n",
    "            --force --featurizer.dither=0.0\n",
    "! docker run --rm --gpus 0 -v $MODEL_LOC_DOMAIN:/data $RIVA_SM_CONTAINER -- \\\n",
    "            riva-deploy -f  /data/interpolated_asr.rmir:$KEY /data/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update riva_model_loc in riva_quickstart config file to MODEL_LOC_DOMAIN\n",
    "! cd riva_quickstart_v$RIVA_VERSION && ./riva_stop.sh && ./riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model transcripts on base data\n",
    "! docker run --rm -v $DATA_DOWNLOAD_DIR/healthcare_eval_dataset_v1.0:/data  \\\n",
    "    --net=host $RIVA_API_CONTAINER -- \\\n",
    "     riva_streaming_asr_client \\\n",
    "        --automatic_punctuation=false \\\n",
    "        --interim_results=false \\\n",
    "        --word_time_offsets=false \\\n",
    "        --audio_file /data/general.json \\\n",
    "        --output_filename=/data/interpolated_asr_on_base_output.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model transcripts on Healthcare domain data\n",
    "! docker run --rm -v $DATA_DOWNLOAD_DIR/healthcare_eval_dataset_v1.0:/data  \\\n",
    "    --net=host $RIVA_API_CONTAINER -- \\\n",
    "     riva_streaming_asr_client \\\n",
    "        --automatic_punctuation=false \\\n",
    "        --interim_results=false \\\n",
    "        --word_time_offsets=false \\\n",
    "        --audio_file /data/healthcare.json \\\n",
    "        --output_filename=/data/interpolated_asr_on_domain_output.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check WER on base data\n",
    "print(\"WER of base model on generic data: \", calculate_wer(f\"{DATA_DOWNLOAD_DIR}/healthcare_eval_dataset_v1.0/general.json\", f\"{DATA_DOWNLOAD_DIR}/healthcare_eval_dataset_v1.0/base_asr_on_base_output.json\"))\n",
    "print(\"WER of Domain model on generic data: \", calculate_wer(f\"{DATA_DOWNLOAD_DIR}/healthcare_eval_dataset_v1.0/general.json\", f\"{DATA_DOWNLOAD_DIR}/healthcare_eval_dataset_v1.0/interpolated_asr_on_base_output.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check WER on Healtcare domain data\n",
    "print(\"WER of base model on Healtcare domain data: \", calculate_wer(f\"{DATA_DOWNLOAD_DIR}/healthcare_eval_dataset_v1.0/healthcare.json\", f\"{DATA_DOWNLOAD_DIR}/healthcare_eval_dataset_v1.0/base_asr_on_domain_output.json\"))\n",
    "print(\"WER of Domain model on Healtcare domain data: \", calculate_wer(f\"{DATA_DOWNLOAD_DIR}/healthcare_eval_dataset_v1.0/healthcare.json\", f\"{DATA_DOWNLOAD_DIR}/healthcare_eval_dataset_v1.0/interpolated_asr_on_domain_output.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With help of interpolation we were able to improve the performance of our ASR model on Healthcare domain as well as generic domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LM generated by simply passing the text corpus to TAO toolkit contains some n-grams which are less frequent(in corpus) and thus have very low probabilities. Such n-grams can be removed by `pruning`.<br>\n",
    "Pruning requires some thresholds which can be defined in the `train.yaml` file as follows (for 4-gram):\n",
    "```\n",
    "pruning:\n",
    "    - 0\n",
    "    - 1\n",
    "    - 7\n",
    "    - 9\n",
    "```\n",
    "or can be passed as command line argument as follows:<br>\n",
    "`model.pruning=[0,1,7,9]`\n",
    "\n",
    "All the n-gram with frequncy less than or equal to specified threshold will get eliminated.<br>\n",
    "Here, 2-grams with freq. <= 1, 3-gram with freq.<=7 & 4-gram with freq.<=9 will get eliminated.<br>\n",
    "There's a tradeoff between degree of pruning and accuracy. High pruning parameters will reduce the size of language model but at the cost of model accuracy!  \n",
    "\n",
    "#### *Note:\n",
    "Pruning of 1-gram is not supported, threshold for 1-gram should always be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao n_gram train \\\n",
    "    -e /specs/train.yaml \\\n",
    "    -r /results/pruned \\\n",
    "    training_ds.data_file=/data/preprocessed.txt \\\n",
    "    model.order=4 \\\n",
    "    model.pruning=[0,1,7,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check the size of original LM and Pruned LM\n",
    "!echo \"Size of unpruned ARPA: $(du -h $RESULT_DIR_LOCAL/base/checkpoints/train_n_gram.arpa | cut -f 1)\"\n",
    "!echo \"Size of pruned ARPA: $(du -h $RESULT_DIR_LOCAL/pruned/checkpoints/train_n_gram.arpa | cut -f 1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets deploy ASR server with Pruned LM, Set absolute path to create MODEL_LOC_PRUNED\n",
    "MODEL_LOC_PRUNED = \"<YOUR_PATH_TO_PRUNING_MODEL_REPO>\"\n",
    "\n",
    "#export to Riva format \n",
    "!tao n_gram export \\\n",
    "            -e /specs/export.yaml \\\n",
    "            -r /results/base \\\n",
    "            export_format=RIVA \\\n",
    "            export_to=pruned-base.riva \\\n",
    "            restore_from=/results/pruned/checkpoints/train_n_gram.arpa \\\n",
    "            binary_type=trie \\\n",
    "            binary_q_bits=8 \\\n",
    "            binary_b_bits=7 \\\n",
    "            binary_a_bits=256\n",
    "\n",
    "# Generate RMIR\n",
    "! mkdir $MODEL_LOC_PRUNED\n",
    "! cp $MODEL_LOC/Citrinet.riva $MODEL_LOC_PRUNED/\n",
    "! docker run -it --rm --gpus 0 -v $MODEL_LOC_PRUNED:/data \\\n",
    "            -v $RESULT_DIR_LOCAL/base:/lm \\\n",
    "            --name riva-service-maker-lm \\\n",
    "            $RIVA_SM_CONTAINER  -- \\\n",
    "            riva-build speech_recognition /data/pruned_asr.rmir:$KEY \\\n",
    "            /data/Citrinet.riva:$KEY \\\n",
    "            --ms_per_timestep=80 \\\n",
    "            --chunk_size=0.16 \\\n",
    "            --left_padding_size=1.92 \\\n",
    "            --right_padding_size=1.92 \\\n",
    "            --decoder_type=flashlight \\\n",
    "            --decoding_language_model_binary=/lm/pruned-base.binary \\\n",
    "            --decoding_vocab=/lm/dict_vocab.txt \\\n",
    "            --flashlight_decoder.lm_weight=0.2 \\\n",
    "            --flashlight_decoder.word_insertion_score=0.2 \\\n",
    "            --flashlight_decoder.beam_threshold=20. \\\n",
    "            --force --featurizer.dither=0.0\n",
    "                \n",
    "# Deploy RMIR with Pruned LM\n",
    "! docker run --rm --gpus 0 -v $MODEL_LOC_PRUNED:/data $RIVA_SM_CONTAINER -- \\\n",
    "            riva-deploy -f  /data/pruned_asr.rmir:$KEY /data/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update riva_model_loc in riva_quickstart config file to MODEL_LOC_PRUNED and then start server\n",
    "! cd riva_quickstart_v$RIVA_VERSION && ./riva_stop.sh && ./riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model and calculate WERs\n",
    "! docker run --rm -v $DATA_DOWNLOAD_DIR/healthcare_eval_dataset_v1.0/:/data  \\\n",
    "    --net=host $RIVA_API_CONTAINER -- \\\n",
    "     riva_streaming_asr_client \\\n",
    "        --automatic_punctuation=false \\\n",
    "        --interim_results=false \\\n",
    "        --word_time_offsets=false \\\n",
    "        --audio_file /data/general.json \\\n",
    "        --output_filename=/data/pruned_asr_on_base_output.json\n",
    "\n",
    "! docker run --rm -v $DATA_DOWNLOAD_DIR/healthcare_eval_dataset_v1.0/:/data  \\\n",
    "    --net=host $RIVA_API_CONTAINER -- \\\n",
    "     riva_streaming_asr_client \\\n",
    "        --automatic_punctuation=false \\\n",
    "        --interim_results=false \\\n",
    "        --word_time_offsets=false \\\n",
    "        --audio_file /data/healthcare.json \\\n",
    "        --output_filename=/data/pruned_asr_on_domain_output.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check WER on base data\n",
    "print(\"WER of base model on generic data: \", calculate_wer(f\"{DATA_DOWNLOAD_DIR}/healthcare_eval_dataset_v1.0/general.json\", f\"{DATA_DOWNLOAD_DIR}/healthcare_eval_dataset_v1.0/base_asr_on_base_output.json\"))\n",
    "print(\"WER of Pruned base model on generic data: \", calculate_wer(f\"{DATA_DOWNLOAD_DIR}/healthcare_eval_dataset_v1.0/general.json\", f\"{DATA_DOWNLOAD_DIR}/healthcare_eval_dataset_v1.0/pruned_asr_on_base_output.json\"))\n",
    "\n",
    "# Check WER on Healtcare domain data\n",
    "print(\"WER of base model on Healtcare domain data: \", calculate_wer(f\"{DATA_DOWNLOAD_DIR}/healthcare_eval_dataset_v1.0/healthcare.json\", f\"{DATA_DOWNLOAD_DIR}/healthcare_eval_dataset_v1.0/base_asr_on_domain_output.json\"))\n",
    "print(\"WER of Pruned base model on Healtcare domain data: \", calculate_wer(f\"{DATA_DOWNLOAD_DIR}/healthcare_eval_dataset_v1.0/healthcare.json\", f\"{DATA_DOWNLOAD_DIR}/healthcare_eval_dataset_v1.0/pruned_asr_on_domain_output.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning drops some of low probabiliy N-grams from Lnaguage model. This can affect models in both ways.\n",
    "For our case, we were able to improve model performance by reducing the perplexity of Language model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
