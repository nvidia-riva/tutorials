{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJz6FDU1lRzc"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "5. Restart the runtime (Runtime -> Restart Runtime) for any upgraded packages to take effect\n",
    "\"\"\"\n",
    "# If you're using Google Colab and not running locally, run this cell.\n",
    "\n",
    "## Install dependencies\n",
    "!pip install wget\n",
    "!apt-get install sox libsndfile1 ffmpeg\n",
    "!pip install unidecode\n",
    "!pip install matplotlib>=3.3.2\n",
    "\n",
    "## Install NeMo\n",
    "BRANCH = 'main'\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n",
    "\n",
    "## Grab the config we'll use in this example\n",
    "!mkdir configs\n",
    "!wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/asr/conf/config.yaml\n",
    "\n",
    "\"\"\"\n",
    "Remember to restart the runtime for the kernel to pick up any upgraded packages (e.g. matplotlib)!\n",
    "Alternatively, you can uncomment the exit() below to crash and restart the kernel, in the case\n",
    "that you want to use the \"Run All Cells\" (or similar) option.\n",
    "\"\"\"\n",
    "# exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where the an4/ directory will be placed.\n",
    "# Change this if you don't want the data to be extracted in the current directory.\n",
    "data_dir = '.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******\n",
      "Dataset downloaded at: /home/jbalam/nemo/clone3/NeMo/tutorials/asr/an4_sphere.tar.gz\n",
      "Converting .sph to .wav...\n",
      "Finished conversion.\n",
      "******\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "import tarfile\n",
    "import wget\n",
    "data_dir = os.path.abspath(data_dir)\n",
    "# Download the dataset. This will take a few moments...\n",
    "print(\"******\")\n",
    "if not os.path.exists(data_dir + '/an4_sphere.tar.gz'):\n",
    "    an4_url = 'https://dldata-public.s3.us-east-2.amazonaws.com/an4_sphere.tar.gz'\n",
    "    an4_path = wget.download(an4_url, data_dir)\n",
    "    print(f\"Dataset downloaded at: {an4_path}\")\n",
    "else:\n",
    "    print(\"Tarfile already exists.\")\n",
    "    an4_path = data_dir + '/an4_sphere.tar.gz'\n",
    "\n",
    "    \n",
    "\n",
    "if not os.path.exists(data_dir + '/an4/'):\n",
    "    # Untar and convert .sph to .wav (using sox)\n",
    "    tar = tarfile.open(an4_path)\n",
    "    tar.extractall(path=data_dir)\n",
    "\n",
    "    print(\"Converting .sph to .wav...\")\n",
    "    sph_list = glob.glob(data_dir + '/an4/**/*.sph', recursive=True)\n",
    "    for sph_path in sph_list:\n",
    "        wav_path = sph_path[:-4] + '.wav'\n",
    "        #converting to 16kHz wav\n",
    "        cmd = [\"sox\", sph_path, \"-r\", \"16000\", wav_path]\n",
    "        subprocess.run(cmd)\n",
    "print(\"Finished conversion.\\n******\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******\n",
      "***Done***\n"
     ]
    }
   ],
   "source": [
    "# --- Building Manifest Files --- #\n",
    "import json\n",
    "import librosa\n",
    "\n",
    "# Function to build a manifest\n",
    "def build_manifest(transcripts_path, manifest_path, wav_path):\n",
    "    with open(transcripts_path, 'r') as fin:\n",
    "        with open(manifest_path, 'w') as fout:\n",
    "            for line in fin:\n",
    "                # Lines look like this:\n",
    "                # <s> transcript </s> (fileID)\n",
    "                transcript = line[: line.find('(')-1].lower()\n",
    "                transcript = transcript.replace('<s>', '').replace('</s>', '')\n",
    "                transcript = transcript.strip()\n",
    "\n",
    "                file_id = line[line.find('(')+1 : -2]  # e.g. \"cen4-fash-b\"\n",
    "                audio_path = os.path.join(\n",
    "                    data_dir, wav_path,\n",
    "                    file_id[file_id.find('-')+1 : file_id.rfind('-')],\n",
    "                    file_id + '.wav')\n",
    "\n",
    "                duration = librosa.core.get_duration(filename=audio_path)\n",
    "\n",
    "                # Write the metadata to the manifest\n",
    "                metadata = {\n",
    "                    \"audio_filepath\": audio_path,\n",
    "                    \"duration\": duration,\n",
    "                    \"text\": transcript\n",
    "                }\n",
    "                json.dump(metadata, fout)\n",
    "                fout.write('\\n')\n",
    "                \n",
    "# Building Manifests\n",
    "print(\"******\")\n",
    "train_transcripts = data_dir + '/an4/etc/an4_train.transcription'\n",
    "train_manifest = data_dir + '/an4/train_manifest.json'\n",
    "if not os.path.isfile(train_manifest):\n",
    "    build_manifest(train_transcripts, train_manifest, 'an4/wav/an4_clstk')\n",
    "    print(\"Training manifest created.\")\n",
    "\n",
    "test_transcripts = data_dir + '/an4/etc/an4_test.transcription'\n",
    "test_manifest = data_dir + '/an4/test_manifest.json'\n",
    "if not os.path.isfile(test_manifest):\n",
    "    build_manifest(test_transcripts, test_manifest, 'an4/wav/an4test_clstk')\n",
    "    print(\"Test manifest created.\")\n",
    "print(\"***Done***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will look at how to finetune a model for situations where we can sample background noises for an application. We will use the background noise samples from \"Room Impulse Response and Noise Database\" from the openslr database. For each 30 second isotropic noise sample in the dataset we use the first 15 seconds for training and the last 15 seconds for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******\n",
      "Dataset downloaded at: https://www.openslr.org/resources/28/rirs_noises.zip\n",
      "Finished downloading noises.\n",
      "******\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset. This will take a few moments...\n",
    "print(\"******\")\n",
    "if not os.path.exists(data_dir + '/rirs_noises.zip'):\n",
    "    slr28_url = 'https://www.openslr.org/resources/28/rirs_noises.zip'\n",
    "    noise_path = wget.download(slr28_url, data_dir)\n",
    "    print(f\"Dataset downloaded at: {slr28_url}\")\n",
    "else:\n",
    "    print(\"Zip file already exists.\")\n",
    "    noise_path = data_dir + '/rirs_noises.zip'\n",
    "print(\"Finished downloading noises.\\n******\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract noise data\n",
    "from zipfile import ZipFile\n",
    "try:\n",
    "    with ZipFile(noise_path, \"r\") as zipObj:\n",
    "        zipObj.extractall(data_dir)\n",
    "except Exception:\n",
    "    logging.info(\"Not extracting. Maybe already there?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "iso_path = os.path.join(data_dir,\"RIRS_NOISES/real_rirs_isotropic_noises\")\n",
    "iso_noise_list = os.path.join(iso_path, \"noise_list\")\n",
    "# create manifest from noise files\n",
    "def process_row(row, offset, duration):\n",
    "  try:\n",
    "    entry = {}\n",
    "    wav_f = row['wav_filename']\n",
    "    newfile = wav_f\n",
    "    duration = subprocess.check_output(\n",
    "      'soxi -D {0}'.format(newfile), shell=True)\n",
    "    entry['audio_filepath'] = newfile\n",
    "    entry['duration'] = float(duration)\n",
    "    entry['offset'] = offset\n",
    "    entry['text'] = row['transcript']\n",
    "    return entry\n",
    "  except Exception as e:\n",
    "    wav_f = row['wav_filename']\n",
    "    newfile = wav_f\n",
    "    print(f\"Error processing {newfile} file!!!\")\n",
    "    \n",
    "train_rows = []\n",
    "test_rows = []\n",
    "\n",
    "with open(iso_noise_list,\"r\") as in_f:\n",
    "    for line in in_f:\n",
    "        row = {}\n",
    "        data = line.rstrip().split()\n",
    "        row['wav_filename']=os.path.join(data_dir,data[-1])\n",
    "        row['transcript'] = \"-\"\n",
    "        train_rows.append(process_row(row, 0 , 15))\n",
    "        test_rows.append(process_row(row, 15 , 15))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_manifest(manifest_file, manifest_lines):\n",
    "    with open(manifest_file, 'w') as fout:\n",
    "      for m in manifest_lines:\n",
    "        fout.write(json.dumps(m) + '\\n')\n",
    "\n",
    "test_noise_manifest = os.path.join(data_dir, \"test_noise.json\")\n",
    "train_noise_manifest = os.path.join(data_dir, \"train_noise.json\")\n",
    "write_manifest(test_noise_manifest, test_rows)\n",
    "write_manifest(train_noise_manifest, train_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No let's create an evaluation set using the an4 test set by adding noise at 0 dB using a script in NeMo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to test set\n",
    "run = f\"python ../../scripts/dataset_processing/add_noise.py \\\n",
    "    --input_manifest={test_manifest} \\\n",
    "    --noise_manifest={test_noise_manifest} \\\n",
    "    --snrs=0 \\\n",
    "    --out_dir={data_dir}/noise_data\"\n",
    "!{run}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-04-17 23:54:56 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
      "[NeMo I 2022-04-17 23:54:57 transcribe_speech:100] Hydra config: model_path: null\n",
      "    pretrained_name: stt_en_conformer_ctc_medium\n",
      "    audio_dir: null\n",
      "    dataset_manifest: /home/jbalam/nemo/clone3/NeMo/tutorials/asr/noise_data/manifests/test_manifest_test_noise_0db.json\n",
      "    output_filename: evaluation_transcripts.json\n",
      "    batch_size: 32\n",
      "    num_workers: 0\n",
      "    cuda: null\n",
      "    amp: false\n",
      "    audio_type: wav\n",
      "    overwrite_transcripts: true\n",
      "    rnnt_decoding:\n",
      "      strategy: greedy_batch\n",
      "      compute_hypothesis_token_set: false\n",
      "      preserve_alignments: null\n",
      "      fused_batch_size: -1\n",
      "      greedy:\n",
      "        max_symbols_per_step: 10\n",
      "        preserve_alignments: false\n",
      "      beam:\n",
      "        beam_size: 4\n",
      "        search_type: default\n",
      "        score_norm: true\n",
      "        return_best_hypothesis: true\n",
      "        tsd_max_sym_exp_per_step: 50\n",
      "        alsd_max_target_len: 1.0\n",
      "        nsc_max_timesteps_expansion: 1\n",
      "        nsc_prefix_alpha: 1\n",
      "        maes_num_steps: 2\n",
      "        maes_prefix_alpha: 1\n",
      "        maes_expansion_gamma: 2.3\n",
      "        maes_expansion_beta: 2\n",
      "        language_model: null\n",
      "        softmax_temperature: 1.0\n",
      "        preserve_alignments: false\n",
      "    use_cer: false\n",
      "    tolerance: null\n",
      "    only_score_manifest: false\n",
      "    \n",
      "[NeMo I 2022-04-17 23:54:57 cloud:56] Found existing object /home/jbalam/.cache/torch/NeMo/NeMo_1.9.0rc0/stt_en_conformer_ctc_medium/54700ca38836d6aff9aa8504c6dc49d3/stt_en_conformer_ctc_medium.nemo.\n",
      "[NeMo I 2022-04-17 23:54:57 cloud:62] Re-using file from: /home/jbalam/.cache/torch/NeMo/NeMo_1.9.0rc0/stt_en_conformer_ctc_medium/54700ca38836d6aff9aa8504c6dc49d3/stt_en_conformer_ctc_medium.nemo\n",
      "[NeMo I 2022-04-17 23:54:57 common:705] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2022-04-17 23:54:58 mixins:165] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "Created a temporary directory at /tmp/tmpvla9d6an\n",
      "Writing /tmp/tmpvla9d6an/_remote_module_non_sriptable.py\n",
      "[NeMo W 2022-04-17 23:54:58 nemo_logging:349] /home/jbalam/miniconda3/envs/main0310/lib/python3.8/site-packages/torch/cuda/__init__.py:122: UserWarning: \n",
      "        Found GPU2 GeForce GT 710 which is of cuda capability 3.5.\n",
      "        PyTorch no longer supports this GPU because it is too old.\n",
      "        The minimum cuda capability supported by this library is 3.7.\n",
      "        \n",
      "      warnings.warn(old_gpu_warn % (d, name, major, minor, min_arch // 10, min_arch % 10))\n",
      "    \n",
      "[NeMo W 2022-04-17 23:54:58 modelPT:148] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /data/NeMo_ASR_SET/English/v2.0/train/tarred_audio_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20.0\n",
      "    min_duration: 0.1\n",
      "    shuffle_n: 2048\n",
      "    is_tarred: true\n",
      "    tarred_audio_filepaths: /data/NeMo_ASR_SET/English/v2.0/train/audio__OP_0..4095_CL_.tar\n",
      "    \n",
      "[NeMo W 2022-04-17 23:54:58 modelPT:155] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n",
      "[NeMo W 2022-04-17 23:54:58 modelPT:161] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n",
      "[NeMo I 2022-04-17 23:54:58 features:259] PADDING: 0\n",
      "[NeMo I 2022-04-17 23:54:58 features:276] STFT using torch\n",
      "[NeMo I 2022-04-17 23:54:59 audio_preprocessing:496] Numba CUDA SpecAugment kernel is being used\n",
      "[NeMo I 2022-04-17 23:55:01 save_restore_connector:158] Model EncDecCTCModelBPE was successfully restored from /home/jbalam/.cache/torch/NeMo/NeMo_1.9.0rc0/stt_en_conformer_ctc_medium/54700ca38836d6aff9aa8504c6dc49d3/stt_en_conformer_ctc_medium.nemo.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[NeMo I 2022-04-17 23:55:01 transcribe_speech:172] \n",
      "    Transcribing 130 files...\n",
      "    \n",
      "Transcribing: 100%|███████████████████████████████| 5/5 [00:00<00:00,  9.42it/s]\n",
      "[NeMo I 2022-04-17 23:55:02 transcribe_speech:224] Finished transcribing 130 files !\n",
      "[NeMo I 2022-04-17 23:55:02 transcribe_speech:226] Writing transcriptions into file: evaluation_transcripts.json\n",
      "[NeMo I 2022-04-17 23:55:02 transcribe_speech:244] Finished writing predictions !\n",
      "[NeMo I 2022-04-17 23:55:02 speech_to_text_eval:108] Finished transcribing speech dataset. Computing ASR metrics..\n",
      "[NeMo I 2022-04-17 23:55:02 speech_to_text_eval:145] Got WER of 0.0724450194049159\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "run=f\"python ../../examples/asr/speech_to_text_eval.py \\\n",
    "    pretrained_name=stt_en_conformer_ctc_medium \\\n",
    "    dataset_manifest={data_dir}/noise_data/manifests/test_manifest_test_noise_0db.json\"\n",
    "!{run}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finetune the pretrained model wtih noise augmentation we need to add an augmentor to the trainer. In this case we will Noise_perturbation augmentation to add noise to our training data. To achieve this we will ned to add the following lines to our trainer:\n",
    "```yaml\n",
    "\n",
    "trainer:\n",
    "    train_ds:\n",
    "        augmentor:\n",
    "            noise:\n",
    "              prob: 0.1\n",
    "              manifest_path: \"/path/to/train_noise.json\"\n",
    "              min_snr_db: 0\n",
    "              max_snr_db: 30\n",
    "```\n",
    "With the above lines, we added a noise agumentor to our training dataset. With a low augmentation probability (0.1) we make sure that the model has good accuracy on clean data while improving accuracy on noisy data. If you expect your test data to be mostly noisy, increasing the probability will tweak the model's performance to be better for noisy data with a degaradation in clean speech accuracy. Alsso, with the above settings we are adding noise at an SNR that is randomly chosen between 0 to 30 db, these values can be adjusted according to the expected SNR for a given application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ASR_with_NeMo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
