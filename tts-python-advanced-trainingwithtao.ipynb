{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to Train a FastPitch and HiFi-GAN Model using NVIDIA TAO Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial walks you through how to train a FastPitch and HiFi-GAN model from scratch with the LJSpeech dataset using TAO Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVIDIA TAO Toolkit Overview\n",
    "\n",
    "Train Adapt Optimize (TAO) Toolkit is a Python-based AI toolkit for taking purpose-built pre-trained AI models and customizing them with your own data. \n",
    "\n",
    "Transfer learning extracts learned features from an existing neural network into a new one. Transfer learning is often used when creating a large training dataset is not feasible. \n",
    "\n",
    "Developers, researchers, and software partners building intelligent vision AI applications and services, can bring their own data to fine-tune pre-trained models instead of going through the hassle of training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Train Adapt Optimize (TAO) Toolkit](https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this toolkit is to reduce that 80 hour workload to an 8 hour workload, which can enable data scientists to have considerably more train-test iterations in the same time frame.\n",
    "\n",
    "Let's see this in action with a use case for Text-to-Speech (TTS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text to Speech (TTS) is often the last step in building a conversational AI model. A TTS model converts text into audible speech. The main objective is to synthesize reasonable and natural speech for given text. Since there are no universal standards to measure quality of synthesized speech, you will need to listen to some inferred speech to tell whether a TTS model is well trained.\n",
    "\n",
    "In TAO Toolkit, TTS consists of two models: [FastPitch](https://arxiv.org/pdf/2006.06873.pdf) for spectrogram generation and [HiFi-GAN](https://arxiv.org/pdf/2010.05646.pdf) for vocoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TTS using TAO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing and setting up TAO\n",
    "\n",
    "Install TAO inside a Python virtual environment. We recommend performing this step first and then launching the tutorial from the virtual environment.\n",
    "\n",
    "In addition to installing the TAO Python package, ensure you meet the following software requirements:\n",
    "\n",
    "1. `python` 3.8.13\n",
    "2. `docker-ce` > 19.03.5\n",
    "3. `docker-API` 1.40\n",
    "4. `nvidia-container-toolkit` > 1.3.0-1\n",
    "5. `nvidia-container-runtime` > 3.4.0-1\n",
    "6. `nvidia-docker2` > 2.5.0-1\n",
    "7. `nvidia-driver` >= 470.57"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing TAO is a simple `pip` install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nvidia-tao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing TAO, the next step is to setup the mounts for TAO. The TAO launcher uses Docker containers under the hood, and **for our data and results directory to be visible to Docker, they need to be mapped**. The launcher can be configured using the config file `~/.tao_mounts.json`. Apart from the mounts, you can also configure additional options like the environment variables and the amount of shared memory available to the TAO launcher. <br>\n",
    "\n",
    "Replace the `FIXME` variables with the required paths enclosed in `\"\"` as a string.\n",
    "\n",
    "`IMPORTANT NOTE:` The following code creates a sample `~/.tao_mounts.json`  file. Here, we can map directories in which we save the data, specs, results, and cache. You should configure it for your specific use case so these directories are correctly visible to the Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please define these paths on your local host machine\n",
    "import os\n",
    "\n",
    "os.environ[\"HOST_DATA_DIR\"] = FIXME\n",
    "os.environ[\"HOST_SPECS_DIR\"] = FIXME\n",
    "os.environ[\"HOST_RESULTS_DIR\"] = FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p $HOST_DATA_DIR\n",
    "! mkdir -p $HOST_SPECS_DIR\n",
    "! mkdir -p $HOST_RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "import os\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "tao_configs = {\n",
    "   \"Mounts\":[\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_DATA_DIR\"],\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_SPECS_DIR\"],\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_RESULTS_DIR\"],\n",
    "           \"destination\": \"/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.path.expanduser(\"~/.cache\"),\n",
    "           \"destination\": \"/root/.cache\"\n",
    "       }\n",
    "   ],\n",
    "   \"DockerOptions\": {\n",
    "        \"shm_size\": \"16G\",\n",
    "        \"ulimits\": {\n",
    "            \"memlock\": -1,\n",
    "            \"stack\": 67108864\n",
    "         }\n",
    "   }\n",
    "}\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(tao_configs, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the Docker image versions and the tasks that it performs. You can also check by issuing `tao --help` or:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tao info --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Relevant Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TAO Docker.\n",
    "\n",
    "# The data is saved here:\n",
    "DATA_DIR = \"/data\"\n",
    "SPECS_DIR = \"/specs\"\n",
    "RESULTS_DIR = \"/results\"\n",
    "\n",
    "# Set your encryption key and use the same key for all commands:\n",
    "KEY = 'tlt_encode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command structure for the TAO interface can be broken down as follows: `tao <task name> <subcommand>` <br> \n",
    "\n",
    "Let's see this in further detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Downloading Specs\n",
    "TAO's conversational AI toolkit works off of spec files which make it easy to edit hyperparameters on the fly. We can proceed to downloading the spec files. You may choose to modify/rewrite these specs or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command. <br>\n",
    "\n",
    "The `-o` argument indicates the folder where the default specification files will be downloaded. The `-r` argument instructs the script on where to save the logs. **Ensure the `-o` points to an empty folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download spec files for FastPitch\n",
    "! tao spectro_gen download_specs \\\n",
    "    -r $RESULTS_DIR/spectro_gen \\\n",
    "    -o $SPECS_DIR/spectro_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download spec files for HiFiGAN\n",
    "! tao vocoder download_specs \\\n",
    "    -r $RESULTS_DIR/vocoder \\\n",
    "    -o $SPECS_DIR/vocoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will use the popular LJSpeech dataset. Let's download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -O $HOST_DATA_DIR/ljspeech.tar.bz2 https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading, untar the dataset and move it to the correct directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -xvf $HOST_DATA_DIR/ljspeech.tar.bz2\n",
    "! rm -rf $HOST_DATA_DIR/ljspeech\n",
    "! mv LJSpeech-1.1 $HOST_DATA_DIR/ljspeech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using your own dataset\n",
    "\n",
    "If you want to use your own dataset, you'll have to organize your own dataset following the LJSpeech format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step downloads audio to text file lists from NVIDIA for LJSpeech and generates the manifest files. If you use your own dataset, you'll have to generate three files: `ljs_audio_text_train_filelist.txt`, `ljs_audio_text_val_filelist.txt`, and `ljs_audio_text_test_filelist.txt` yourself. Those files correspond to your train `/ val /` test split. For each text file, the number of rows should be equal to the number of samples in this split. Each row should look similar to:\n",
    "\n",
    "```\n",
    "DUMMY/<file_name>.wav|<text_of_the_audio>\n",
    "```\n",
    "\n",
    "An example row is:\n",
    "\n",
    "```\n",
    "DUMMY/LJ045-0096.wav|Mrs. De Mohrenschildt thought that Oswald,\n",
    "```\n",
    "\n",
    "After having those three files in your `data_dir`, run the following command as you would do for the LJSpeech dataset:\n",
    "\n",
    "Be patient! This step can take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tao spectro_gen dataset_convert \\\n",
    "    -e $SPECS_DIR/spectro_gen/dataset_convert_ljs.yaml \\\n",
    "    -r $RESULTS_DIR/spectro_gen/dataset_convert \\\n",
    "    data_dir=$DATA_DIR/ljspeech \\\n",
    "    dataset_name=ljspeech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TAO interface enables you to configure the training parameters from the command-line interface. <br>\n",
    "\n",
    "The process of opening the training script, finding the parameters of interest (which might be spread across multiple files), and making the changes needed, is being replaced by a simple command-line interface.\n",
    "\n",
    "For example, if the number of epochs are needed to be modified along with a change in the learning rate, you can add `trainer.max_epochs=10` and `optim.lr=0.02` and train the model. Sample commands are given below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training TTS models in TAO, we use the `tao spectro_gen train` and `tao vocoder train` commands with the following arguments:\n",
    "<ul>\n",
    "    <li>`-e`: Path to the spec file </li>\n",
    "    <li>`-g`: Number of GPUs to use </li>\n",
    "    <li>`-r`: Path to the results folder </li>\n",
    "    <li>`-k`: User specified encryption key to use while saving/loading the model </li>\n",
    "    <li>Any overrides to the spec file. For example, `trainer.max_epochs`. </li>\n",
    "</ul>\n",
    "\n",
    "NOTE: In order to get a TTS pipeline, you need to train **BOTH** FastPitch (`spectro_gen`) and HiFi-GAN (vocoder). For HiFi-GAN, since it's universal for a specific language, you might just download the pretrained weights from NGC and it will give you good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training FastPitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior is needed for FastPitch training. If an empty folder is provided, prior will generate on-the-fly.\n",
    "! mkdir -p $RESULTS_DIR/spectro_gen/train/prior_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you provided an empty prior folder, this may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao spectro_gen train \\\n",
    "     -e $SPECS_DIR/spectro_gen/train.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -r $RESULTS_DIR/spectro_gen/train \\\n",
    "     train_dataset=$DATA_DIR/ljspeech/ljspeech_train.json \\\n",
    "     validation_dataset=$DATA_DIR/ljspeech/ljspeech_val.json \\\n",
    "     prior_folder=$RESULTS_DIR/spectro_gen/train/prior_folder \\\n",
    "     trainer.max_epochs=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training HiFi-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of passing `trainer.max_epochs`, HiFi-GAN requires the definition of `trainer.max_steps`. Defining `trainer.max_epochs` for HiFi-GAN has no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao vocoder train \\\n",
    "     -e $SPECS_DIR/vocoder/train.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -r $RESULTS_DIR/vocoder/train \\\n",
    "     train_dataset=$DATA_DIR/ljspeech/ljspeech_train.json \\\n",
    "     validation_dataset=$DATA_DIR/ljspeech/ljspeech_val.json \\\n",
    "     trainer.max_steps=10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTS model export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With TAO, you can also export your model in a format that can deployed using NVIDIA Riva; a highly performant application framework for multi-modal conversational AI services using GPUs. The same command for exporting to ONNX can be used here. The only small variation is the configuration for `export_format` in the spec file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao spectro_gen export \\\n",
    "     -e $SPECS_DIR/spectro_gen/export.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/spectro_gen/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/spectro_gen/export \\\n",
    "     export_format=ONNX \\\n",
    "     export_to=spectro_gen.eonnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao vocoder export \\\n",
    "     -e $SPECS_DIR/vocoder/export.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/vocoder/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/vocoder/export \\\n",
    "     export_format=ONNX \\\n",
    "     export_to=vocoder.eonnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to Riva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao spectro_gen export \\\n",
    "     -e $SPECS_DIR/spectro_gen/export.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/spectro_gen/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/spectro_gen/export \\\n",
    "     export_format=RIVA \\\n",
    "     export_to=spectro_gen.riva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao vocoder export \\\n",
    "     -e $SPECS_DIR/vocoder/export.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/vocoder/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/vocoder/export \\\n",
    "     export_format=RIVA \\\n",
    "     export_to=vocoder.riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTS Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, since there are no universal standards to measure quality of synthesized speech, you will need to listen to some inferred speech to tell whether a TTS model is well trained. Therefore, we do not provide `evaluate` functionality in TAO Toolkit for TTS but only provide `infer` functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate spectrogram\n",
    "\n",
    "The first step for inference is generating a spectrogram. That's a NumPy array (saved as `.npy` file) for a sentence which can be converted to voice by a vocoder. We use the FastPitch model we just trained to generate a spectrogram.\n",
    "\n",
    "You may have to work with the `infer.yaml` file to set the texts you want for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao spectro_gen infer \\\n",
    "     -e $SPECS_DIR/spectro_gen/infer.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/spectro_gen/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/spectro_gen/infer \\\n",
    "     output_path=$RESULTS_DIR/spectro_gen/infer/spectro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate sound file\n",
    "\n",
    "The second step for inference is generating a `.wav` sound file based on a spectrogram you generated in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao vocoder infer \\\n",
    "     -e $SPECS_DIR/vocoder/infer.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/vocoder/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/vocoder/infer \\\n",
    "     input_path=$RESULTS_DIR/spectro_gen/infer/spectro \\\n",
    "     output_path=$RESULTS_DIR/vocoder/infer/wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython.display as ipd\n",
    "# change path of the file here\n",
    "ipd.Audio(os.environ[\"HOST_RESULTS_DIR\"] + '/vocoder/infer/wav/0.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug\n",
    "\n",
    "If the above sound file does not have good quality, you probably need to first figure out whether it's a FastPitch or HiFi-GAN problem. Then, retrain or fine-tune the problematic model. For this purpose, you can download pre-trained HiFi-GAN from NVIDIA NGC and (1) generate the spectrogram with your trained FastPitch (2) generate the `.wav` file with NVIDIA pretrained HiFi-GAN. If the `.wav` file generated in this manner is good, you know your HiFi-GAN is not well-trained. Otherwise, the problem is with FastPitch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTS Inference using ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAO provides the capability to use the exported `.eonnx` model for inference. The commands are very similar to the inference command for `.tlt` models. Again, the inputs in the spec file used is just for demo purposes, you may choose to try out your custom input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate spectrogram\n",
    "\n",
    "The first step for inference is generating a spectrogram. That's a NumPy array (saved as a `.npy` file) for a sentence which can be converted to voice by a vocoder. We use the FastPitch model we just trained to generate a spectrogram.\n",
    "\n",
    "You may have to work with the `infer.yaml` file to set the texts you want for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao spectro_gen infer_onnx \\\n",
    "     -e $SPECS_DIR/spectro_gen/infer.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/spectro_gen/export/spectro_gen.eonnx \\\n",
    "     -r $RESULTS_DIR/spectro_gen/infer_onnx \\\n",
    "     output_path=$RESULTS_DIR/spectro_gen/infer_onnx/spectro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the Sound File\n",
    "\n",
    "The second step for inference is generating a `.wav` sound file based on the spectrogram you generated in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao vocoder infer_onnx \\\n",
    "     -e $SPECS_DIR/vocoder/infer.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/vocoder/export/vocoder.eonnx \\\n",
    "     -r $RESULTS_DIR/vocoder/infer_onnx \\\n",
    "     input_path=$RESULTS_DIR/spectro_gen/infer_onnx/spectro \\\n",
    "     output_path=$RESULTS_DIR/vocoder/infer_onnx/wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything works properly, the `.wav` file below should sound exactly the same as the `.wav` file in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython.display as ipd\n",
    "# change path of the file here\n",
    "ipd.Audio(os.environ[\"HOST_RESULTS_DIR\"] + '/vocoder/infer_onnx/wav/0.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use TAO to: \n",
    " 1. Build custom models for your own applications.\n",
    " 2. Deploy the custom model to NVIDIA Riva using the [tts-python-advanced-deploytrainedmodel.ipynb](tts-python-advanced-deploytrainedmodel.ipynb) tutorial."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "741d73fab70d7eb29e7b56260ebaa567f0620f4d2780830ca385f600e5120e14"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
