{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caa47ff5",
   "metadata": {},
   "source": [
    "# TTS Deploy\n",
    "\n",
    "In this tutorial we will explain the process of generating a TTS RMIR from an acoustic model and a vocoder for both data center and embedded machines. The acoustic model and vocoder need to be store as .riva files. RMIR (Riva Model Intermediate Representation) is an intermediate file that has all the necessary artifacts (models, files, configurations, and user settings) required to deploy a Riva service.  \n",
    "\n",
    "In this tutorial we will use pretrained [Fastpitch.riva](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechsynthesis_en_us_fastpitch_ipa) and [HifiGan.riva](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechsynthesis_en_us_hifigan_ipa). These can be replaced with any custom acoutic_model or vocoder riva files. [`nemo2riva`](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/model-overview.html#export-models-with-nemo2riva) can be used to generate .riva files from nemo checkpoints.  \n",
    "\n",
    "We will also deploy the RMIR we generated using [riva_quickstart](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d3303d",
   "metadata": {},
   "source": [
    "### Set configs and params.\n",
    "Set following config parameters:  \n",
    "`acoustic_model`: Full path for acoustic_model.riva file from [ngc](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechsynthesis_en_us_fastpitch_ipa). This can be replaced with a custom acoustic model .riva checkpoint.  \n",
    "`vocoder`: Full path for vocoder.riva file file from [ngc](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechsynthesis_en_us_hifigan_ipa). This can be replaced with a custom vocoder .riva checkpoint.  \n",
    "`out_dir`: Directory to put TTS.rmir file. The RMIR will be placed in ${out_dir}/RMIR/RMIR_NAME.rmir  \n",
    "`voice`: Set the voice name of the model.  \n",
    "`key`: This is the encryption key used in nemo2riva. Same key will be used to deploy the RMIR generated in this tutorial.  \n",
    "`use_ipa`: Set to \"y\" or \"Y\" if the model uses IPA phones, \"no\" if the model uses arpabet.  \n",
    "`lang`: Model language.  \n",
    "`sample_rate`: Sample rate of generated audios.  \n",
    "`machine_type`: type of machine the tutorial is being run on. Acceptable values are `arm` and `amd`.  \n",
    "\n",
    "`target_machine`: type of machine the RMIR will be deployed on. Acceptable values are `arm` and `amd`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579b6420",
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_model = pathlib.Path.cwd() / \"speechsynthesis_en_us_fastpitch_ipa_vdeployable_v1.0/FastPitch_44k_EnglishUS_IPA.riva\" ##acoustic_model .riva location\n",
    "vocoder = pathlib.Path.cwd() / \"speechsynthesis_en_us_hifigan_ipa_vdeployable_v1.0/HifiGAN_44k_EnglishUS_IPA.riva\" ##vocoder .riva location\n",
    "out_dir = pathlib.Path(\"out/\") ##Output directory to store generated RMIR. The RMIR will be placed in ${out_dir}/RMIR/RMIR_NAME.rmir\n",
    "voice = \"test\" ##Voice name     \n",
    "key = \"tlt_encode\" ##Encryption key used during nemo2riva\n",
    "use_ipa = \"no\" ##\"y\" or \"Y\" if the model uses ipa, no otherwise.\n",
    "lang = \"en-US\" ##Language\n",
    "sample_rate = 44100 ##Sample rate of the audios\n",
    "machine_type=\"amd\" #Change this to amd incase of an x86_64 machine.\n",
    "target_machine=\"arm\" #Change this to amd incase of an x86_64 machine.\n",
    "riva_model_files=pathlib.Path.cwd() / \"speechsynthesis_en_us_auxiliary_files_vdeployable_v1.3\" ##Riva model repo path. incase of custom model repo, change this to the full path of the custom riva model repo.\n",
    "\n",
    "rmir_dir = out_dir / \"rmir\"\n",
    "\n",
    "## Riva NGC, servicemaker image config.\n",
    "riva_ngc_org = \"nvidia\"\n",
    "riva_ngc_team = \"riva\"\n",
    "NGC_TARGET = f\"{riva_ngc_org}/{riva_ngc_team}\"\n",
    "riva_ngc_image_version = \"2.8.0\"\n",
    "if machine_type==\"arm\":\n",
    "    riva_init_image = f\"nvcr.io/{NGC_TARGET}/riva-speech:{riva_ngc_image_version}-servicemaker-l4t-aarch64\"\n",
    "elif machine_type==\"amd\":\n",
    "    riva_init_image = f\"nvcr.io/{NGC_TARGET}/riva-speech:{riva_ngc_image_version}-servicemaker\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670aea87",
   "metadata": {},
   "source": [
    "### Download models\n",
    "We will download pretrained [Fastpitch](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechsynthesis_en_us_fastpitch_ipa) and [HifiGan](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechsynthesis_en_us_hifigan_ipa) models from ngc. You can replace these models with the paths of your custom model, incase of custom models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca2a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngc registry model download-version \"nvidia/tao/speechsynthesis_en_us_fastpitch_ipa:deployable_v1.0\"\n",
    "!ngc registry model download-version \"nvidia/tao/speechsynthesis_en_us_hifigan_ipa:deployable_v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d8e550",
   "metadata": {},
   "source": [
    "Download the auxiliary TTS deployable files from ngc. This will include the following files:  \n",
    "- Arpabet dict\n",
    "- IPA dict\n",
    "- abbreviation dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69861f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngc registry model download-version \"nvidia/tao/speechsynthesis_en_us_auxiliary_files:deployable_v1.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f30b159",
   "metadata": {},
   "source": [
    "Get acoustic_model, vocoder directory path and model names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b11ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "synt_dir = acoustic_model.parent\n",
    "voc_dir = vocoder.parent\n",
    "\n",
    "synt_name = acoustic_model.name\n",
    "voc_name = vocoder.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff35df8",
   "metadata": {},
   "source": [
    "Create output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4beba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not out_dir.exists():\n",
    "    out_dir.mkdir()\n",
    "if not rmir_dir.exists():\n",
    "    rmir_dir.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab4ef3",
   "metadata": {},
   "source": [
    "Stop already running docker file and run riva_servicemaker and run again with acoustic_model and vocoder paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72a1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run the riva servicemaker.\n",
    "!docker stop riva_rmir_gen &> /dev/null\n",
    "!set -x && docker run -td --gpus all --rm -v {str(riva_model_files)}:/riva_repo -v {str(synt_dir)}/:/synt -v {str(voc_dir)}:/voc \\\n",
    "            -v {str(rmir_dir.resolve())}:/data --name riva_rmir_gen --entrypoint=\"/bin/bash\" {riva_init_image}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58db5092",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "    Using <b>--force</b> tag in <b>riva-build</b> this will replace any existing RMIR.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9897d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.warn(\"Using --force in riva-build will replace any existing RMIR.\")\n",
    "riva_build=f\"\"\"riva-build speech_synthesis --force --voice_name={voice}  --language_code={lang} \\\n",
    "                --sample_rate={sample_rate} /data/FastPitch_HifiGan.rmir:{key} /synt/{synt_name}:{key} \\\n",
    "                /voc/{voc_name}:{key}  --abbreviations_file=/riva_repo/abbr.txt\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fb897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_machine==\"arm\":\n",
    "    riva_build += \"\"\"--max_batch_size 1 --denoiser.max_batch_size 1 --preprocessor.max_batch_size 1 \\\n",
    "                --encoderFastPitch.max_batch_size 1 --chunkerFastPitch.max_batch_size 1 --hifigan.max_batch_size 1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a992d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ipa == \"Y\" or use_ipa==\"y\":\n",
    "    riva_build+=\" --phone_set=ipa --arpabet_file=/riva_repo/ipa_cmudict-0.7b_nv22.08.txt\"\n",
    "else:\n",
    "    riva_build+=\" --arpabet_file=/riva_repo/cmudict-0.7b_nv22.08\"\n",
    "print(riva_build)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7988d2",
   "metadata": {},
   "source": [
    "Execute the riva build command and stop the riva_servicemaker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec  riva_rmir_gen {riva_build}\n",
    "!docker stop riva_rmir_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e170f",
   "metadata": {},
   "source": [
    "## Deploy.\n",
    "\n",
    "So far in this tutorial, we have learned how to generate RMIR files from .riva files. We would see that a `FastPitch_HifiGan.rmir` has been generated in the `${out_dir}/rmir` location we defined earlier.  \n",
    "\n",
    "The RMIR file generated in this tutorial can be deployed using [riva_quickstart](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html).\n",
    "\n",
    "### Steps to deploy the RMIR\n",
    "- Download the riva_quickstart\n",
    "- Open `config.sh` and update the following params:  \n",
    "    - set `service_enabled_asr` to `false`.  \n",
    "    - set `service_enabled_nlp` to `false`.  \n",
    "    - set `service_enabled_tts` to `true`.  \n",
    "    - `riva_model_loc` to the location of your `out_dir`.  \n",
    "    - set `use_existing_rmirs` to `true`.  \n",
    "- run `riva_init.sh`.  \n",
    "- run `riva_start.sh`.  \n",
    "\n",
    "The RMIR should be deployed after these steps.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55191704",
   "metadata": {},
   "source": [
    "# Run Inference\n",
    "Once the Riva server is up and running with your models, you can send inference requests querying the server.\n",
    "\n",
    "To send gRPC requests, install the Riva Python API bindings for the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802c1368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install client API bindings\n",
    "! pip install nvidia-riva-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1287b6a3",
   "metadata": {},
   "source": [
    "### Connect to the Riva server and run inference\n",
    "Now, we can query the Riva server; letâ€™s get started. The following cell queries the Riva server (using gRPC) to yield a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import soundfile\n",
    "import riva.client\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "\n",
    "server = \"localhost:50051\"                # location of riva server\n",
    "auth = riva.client.Auth(uri=server)\n",
    "tts_service = riva.client.SpeechSynthesisService(auth)\n",
    "\n",
    "\n",
    "text = \"Is it recognize speech or wreck a nice beach?\"\n",
    "language_code = \"en-US\"                   # currently required to be \"en-US\"\n",
    "sample_rate_hz = 22050                    # the desired sample rate\n",
    "voice_name = \"new_speaker.new_voice\"      # subvoice to generate the audio output.\n",
    "data_type = np.int16                      # For RIVA version < 1.10.0 please set this to np.float32\n",
    "\n",
    "resp = tts_service.synthesize(text, voice_name=voice_name, language_code=language_code, sample_rate_hz=sample_rate_hz)\n",
    "audio = resp.audio\n",
    "meta = resp.meta\n",
    "processed_text = meta.processed_text\n",
    "predicted_durations = meta.predicted_durations\n",
    "\n",
    "audio_samples = np.frombuffer(resp.audio, dtype=data_type)\n",
    "print(processed_text)\n",
    "ipd.Audio(audio_samples, rate=sample_rate_hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a329e2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_speech_ml0.6",
   "language": "python",
   "name": "py38_speech_ml0.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
