{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caa47ff5",
   "metadata": {},
   "source": [
    "# TTS Deploy\n",
    "\n",
    "In this tutorial we will explain the process of generating a TTS RMIR from an acoustic model and a vocoder for both data center and embedded machines. The acoustic model and vocoder need to be store as .riva files. RMIR (Riva Model Intermediate Representation) is an intermediate file that has all the necessary artifacts (models, files, configurations, and user settings) required to deploy a Riva service.  \n",
    "\n",
    "In this tutorial we will use pretrained [Fastpitch.riva](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechsynthesis_en_us_fastpitch_ipa) and [HifiGan.riva](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechsynthesis_en_us_hifigan_ipa). These can be replaced with any custom acoutic_model or vocoder riva files. [`nemo2riva`](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/model-overview.html#export-models-with-nemo2riva) can be used to generate .riva files from nemo checkpoints.  \n",
    "\n",
    "We will also deploy the RMIR we generated using [riva_quickstart](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d3303d",
   "metadata": {},
   "source": [
    "### Set configs and params.\n",
    "Set following config parameters:  \n",
    "`acoustic_model`: Full path for acoustic_model.riva file from [ngc](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechsynthesis_en_us_fastpitch_ipa). This can be replaced with a custom acoustic model .riva checkpoint.  \n",
    "`vocoder`: Full path for vocoder.riva file file from [ngc](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechsynthesis_en_us_hifigan_ipa). This can be replaced with a custom vocoder .riva checkpoint.  \n",
    "`out_dir`: Directory to put TTS.rmir file. The RMIR will be placed in ${out_dir}/RMIR/RMIR_NAME.rmir  \n",
    "`voice`: Set the voice name of the model.  \n",
    "`key`: This is the encryption key used in nemo2riva. Same key will be used to deploy the RMIR generated in this tutorial.  \n",
    "`use_ipa`: Set to \"y\" or \"Y\" if the model uses IPA phones, \"no\" if the model uses arpabet.  \n",
    "`lang`: Model language.  \n",
    "`sample_rate`: Sample rate of generated audios.  \n",
    "`machine_type`: type of machine the tutorial is being run on. Acceptable values are `arm` and `amd`.  \n",
    "\n",
    "`target_machine`: type of machine the RMIR will be deployed on. Acceptable values are `arm` and `amd`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579b6420",
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_model = pathlib.Path.cwd() / \"speechsynthesis_en_us_fastpitch_ipa_vdeployable_v1.0/FastPitch_44k_EnglishUS_IPA.riva\" ##acoustic_model .riva location\n",
    "vocoder = pathlib.Path.cwd() / \"speechsynthesis_en_us_hifigan_ipa_vdeployable_v1.0/HifiGAN_44k_EnglishUS_IPA.riva\" ##vocoder .riva location\n",
    "out_dir = pathlib.Path(\"out/\") ##Output directory to store generated RMIR. The RMIR will be placed in ${out_dir}/RMIR/RMIR_NAME.rmir\n",
    "voice = \"test\" ##Voice name     \n",
    "key = \"tlt_encode\" ##Encryption key used during nemo2riva\n",
    "use_ipa = \"no\" ##\"y\" or \"Y\" if the model uses ipa, no otherwise.\n",
    "lang = \"en-US\" ##Language\n",
    "sample_rate = 44100 ##Sample rate of the audios\n",
    "machine_type=\"amd\" #Change this to amd incase of an x86_64 machine.\n",
    "target_machine=\"arm\" #Change this to amd incase of an x86_64 machine.\n",
    "riva_model_files=pathlib.Path.cwd() / \"speechsynthesis_en_us_auxiliary_files_vdeployable_v1.3\" ##Riva model repo path. incase of custom model repo, change this to the full path of the custom riva model repo.\n",
    "\n",
    "rmir_dir = out_dir / \"rmir\"\n",
    "\n",
    "## Riva NGC, servicemaker image config.\n",
    "riva_ngc_org = \"nvidia\"\n",
    "riva_ngc_team = \"riva\"\n",
    "NGC_TARGET = f\"{riva_ngc_org}/{riva_ngc_team}\"\n",
    "riva_ngc_image_version = \"2.8.0\"\n",
    "if machine_type==\"arm\":\n",
    "    riva_init_image = f\"nvcr.io/{NGC_TARGET}/riva-speech:{riva_ngc_image_version}-servicemaker-l4t-aarch64\"\n",
    "elif machine_type==\"amd\":\n",
    "    riva_init_image = f\"nvcr.io/{NGC_TARGET}/riva-speech:{riva_ngc_image_version}-servicemaker\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670aea87",
   "metadata": {},
   "source": [
    "### Download models\n",
    "We will download pretrained [Fastpitch](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechsynthesis_en_us_fastpitch_ipa) and [HifiGan](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechsynthesis_en_us_hifigan_ipa) models from ngc. You can replace these models with the paths of your custom model, incase of custom models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca2a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngc registry model download-version \"nvidia/tao/speechsynthesis_en_us_fastpitch_ipa:deployable_v1.0\"\n",
    "!ngc registry model download-version \"nvidia/tao/speechsynthesis_en_us_hifigan_ipa:deployable_v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d8e550",
   "metadata": {},
   "source": [
    "Download the auxiliary TTS deployable files from ngc. This will include the following files:  \n",
    "- Arpabet dict\n",
    "- IPA dict\n",
    "- abbreviation dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69861f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngc registry model download-version \"nvidia/tao/speechsynthesis_en_us_auxiliary_files:deployable_v1.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f30b159",
   "metadata": {},
   "source": [
    "Get acoustic_model, vocoder directory path and model names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b11ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "synt_dir = acoustic_model.parent\n",
    "voc_dir = vocoder.parent\n",
    "\n",
    "synt_name = acoustic_model.name\n",
    "voc_name = vocoder.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff35df8",
   "metadata": {},
   "source": [
    "Create output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4beba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not out_dir.exists():\n",
    "    out_dir.mkdir()\n",
    "if not rmir_dir.exists():\n",
    "    rmir_dir.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab4ef3",
   "metadata": {},
   "source": [
    "Stop already running docker file and run riva_servicemaker and run again with acoustic_model and vocoder paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72a1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run the riva servicemaker.\n",
    "!docker stop riva_rmir_gen &> /dev/null\n",
    "!set -x && docker run -td --gpus all --rm -v {str(riva_model_files)}:/riva_repo -v {str(synt_dir)}/:/synt -v {str(voc_dir)}:/voc \\\n",
    "            -v {str(rmir_dir.resolve())}:/data --name riva_rmir_gen --entrypoint=\"/bin/bash\" {riva_init_image}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58db5092",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "    Using <b>--force</b> tag in <b>riva-build</b> this will replace any existing RMIR.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9897d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.warn(\"Using --force in riva-build will replace any existing RMIR.\")\n",
    "riva_build=f\"\"\"riva-build speech_synthesis --force --voice_name={voice}  --language_code={lang} \\\n",
    "                --sample_rate={sample_rate} /data/FastPitch_HifiGan.rmir:{key} /synt/{synt_name}:{key} \\\n",
    "                /voc/{voc_name}:{key}  --abbreviations_file=/riva_repo/abbr.txt\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139fbda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_machine==\"arm\":\n",
    "    riva_build += \"\"\"--max_batch_size 1 --denoiser.max_batch_size 1 --preprocessor.max_batch_size 1 \\\n",
    "                --encoderFastPitch.max_batch_size 1 --chunkerFastPitch.max_batch_size 1 --hifigan.max_batch_size 1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a992d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ipa == \"Y\" or use_ipa==\"y\":\n",
    "    riva_build+=\" --phone_set=ipa --arpabet_file=/riva_repo/ipa_cmudict-0.7b_nv22.08.txt\"\n",
    "else:\n",
    "    riva_build+=\" --arpabet_file=/riva_repo/cmudict-0.7b_nv22.08\"\n",
    "print(riva_build)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7988d2",
   "metadata": {},
   "source": [
    "Execute the riva build command and stop the riva_servicemaker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec  riva_rmir_gen {riva_build}\n",
    "!docker stop riva_rmir_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e170f",
   "metadata": {},
   "source": [
    "## Deploy.\n",
    "\n",
    "So far in this tutorial, we have learned how to generate RMIR files from .riva files. We would see that a `FastPitch_HifiGan.rmir` has been generated in the `${out_dir}/rmir` location we defined earlier.  \n",
    "\n",
    "The RMIR file generated in this tutorial can be deployed using [riva_quickstart](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html).\n",
    "\n",
    "### Steps to deploy the RMIR\n",
    "- Download the riva_quickstart\n",
    "- Open `config.sh` and update the following params:  \n",
    "    - set `service_enabled_asr` to `false`.  \n",
    "    - set `service_enabled_nlp` to `false`.  \n",
    "    - set `service_enabled_tts` to `true`.  \n",
    "    - `riva_model_loc` to the location of your `out_dir`.  \n",
    "    - set `use_existing_rmirs` to `true`.  \n",
    "- run `riva_init.sh`.  \n",
    "- run `riva_start.sh`.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52f044f",
   "metadata": {},
   "source": [
    "From this step onwards you need to download the Riva QuickStart Resource from NGC. Set the path to the directory here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b31c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "RIVA_DIR = \"<Path to the uncompressed folder downloaded from quickstart(include the folder name)>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717e0c93",
   "metadata": {},
   "source": [
    "Next, we modify the `config.sh` file to enable the relevant Riva services (TTS in this case for FastPitch and HiFi-GAN), and provide the encryption key and path to the model repository (riva_model_loc) generated in the previous step.\n",
    "\n",
    "For example, if the above model repository is generated at `$MODEL_LOC/models`, then you can specify `riva_model_loc` as the same directory as `MODEL_LOC`\n",
    "\n",
    "\n",
    "### config.sh snippet  \n",
    "    # Enable or Disable Riva Services \n",
    "    service_enabled_asr=false                                                      ## MAKE CHANGES HERE  \n",
    "    service_enabled_nlp=false                                                      ## MAKE CHANGES HERE  \n",
    "    service_enabled_tts=true                                                     ## MAKE CHANGES HERE  \n",
    "\n",
    "    # Specify one or more GPUs to use\n",
    "    # specifying more than one GPU is currently an experimental feature, and may result in undefined behaviours.\n",
    "    gpus_to_use=\"device=0\"\n",
    "\n",
    "    # Specify the encryption key to use to deploy models\n",
    "    MODEL_DEPLOY_KEY=\"tlt_encode\"                                                  ## MAKE CHANGES HERE\n",
    "\n",
    "    # Locations to use for storing models artifacts\n",
    "    #\n",
    "    # If an absolute path is specified, the data will be written to that location\n",
    "    # Otherwise, a docker volume will be used (default).\n",
    "    #\n",
    "    # riva_init.sh will create a `rmir` and `models` directory in the volume or\n",
    "    # path specified. \n",
    "    #\n",
    "    # RMIR ($riva_model_loc/rmir)\n",
    "    # Riva uses an intermediate representation (RMIR) for models\n",
    "    # that are ready to deploy but not yet fully optimized for deployment. Pretrained\n",
    "    # versions can be obtained from NGC (by specifying NGC models below) and will be\n",
    "    # downloaded to $riva_model_loc/rmir by `riva_init.sh`\n",
    "    # \n",
    "    # Custom models produced by NeMo or TAO and prepared using riva-build\n",
    "    # may also be copied manually to this location $(riva_model_loc/rmir).\n",
    "    #\n",
    "    # Models ($riva_model_loc/models)\n",
    "    # During the riva_init process, the RMIR files in $riva_model_loc/rmir\n",
    "    # are inspected and optimized for deployment. The optimized versions are\n",
    "    # stored in $riva_model_loc/models. The riva server exclusively uses these\n",
    "    # optimized versions.\n",
    "    riva_model_loc=\"<add path>\"                              ## MAKE CHANGES HERE (Replace with MODEL_LOC)    \n",
    "\n",
    "    # The default RMIRs are downloaded from NGC by default in the above $riva_rmir_loc directory\n",
    "    # If you'd like to skip the download from NGC and use the existing RMIRs in the $riva_rmir_loc\n",
    "    # then set the below $use_existing_rmirs flag to true. You can also deploy your set of custom\n",
    "    # RMIRs by keeping them in the riva_rmir_loc dir and use this quickstart script with the\n",
    "    # below flag to deploy them all together.\n",
    "    use_existing_rmirs=false                                ## MAKE CHANGES HERE (Set to true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add4aa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have permission to execute these scripts\n",
    "! cd $RIVA_DIR && chmod +x ./riva_init.sh && chmod +x ./riva_start.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec024539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Riva Init. This will fetch the containers/models\n",
    "# YOU CAN SKIP THIS STEP IF YOU DID RIVA DEPLOY\n",
    "! cd $RIVA_DIR && ./riva_init.sh config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf577dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Riva Start. This will deploy your model(s).\n",
    "! cd $RIVA_DIR && ./riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a07a50",
   "metadata": {},
   "source": [
    "# Run Inference\n",
    "Once the Riva server is up and running with your models, you can send inference requests querying the server.\n",
    "\n",
    "To send gRPC requests, install the Riva Python API bindings for the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f1c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install client API bindings\n",
    "! pip install nvidia-riva-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d1f021",
   "metadata": {},
   "source": [
    "### Connect to the Riva server and run inference\n",
    "Now, we can query the Riva server; let’s get started. The following cell queries the Riva server (using gRPC) to yield a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0fc79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import soundfile\n",
    "import riva.client\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "\n",
    "server = \"localhost:50051\"                # location of riva server\n",
    "auth = riva.client.Auth(uri=server)\n",
    "tts_service = riva.client.SpeechSynthesisService(auth)\n",
    "\n",
    "\n",
    "text = \"Is it recognize speech or wreck a nice beach?\"\n",
    "language_code = \"en-US\"                   # currently required to be \"en-US\"\n",
    "sample_rate_hz = 22050                    # the desired sample rate\n",
    "voice_name = \"new_speaker.new_voice\"      # subvoice to generate the audio output.\n",
    "data_type = np.int16                      # For RIVA version < 1.10.0 please set this to np.float32\n",
    "\n",
    "resp = tts_service.synthesize(text, voice_name=voice_name, language_code=language_code, sample_rate_hz=sample_rate_hz)\n",
    "audio = resp.audio\n",
    "meta = resp.meta\n",
    "processed_text = meta.processed_text\n",
    "predicted_durations = meta.predicted_durations\n",
    "\n",
    "audio_samples = np.frombuffer(resp.audio, dtype=data_type)\n",
    "print(processed_text)\n",
    "ipd.Audio(audio_samples, rate=sample_rate_hz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_speech_ml0.6",
   "language": "python",
   "name": "py38_speech_ml0.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
