{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_asr_asr-python-advanced-customize-vocabulary-and-lexicon/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to Customize Riva ASR Vocabulary and Pronunciation with Lexicon Mapping\n",
    "\n",
    "This notebook walks you through the process of customizing Riva ASR NIM vocabulary and lexicon, in order to improve Riva vocabulary coverage and recognition of difficult words, such as acronyms.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Flashlight decoder, deployed by default in Riva Parakeet-CTC NIM, is a lexicon-based decoder and only emits words that are present in the provided lexicon file. That means, uncommon and new words, such as domain specific terminologies, that are not present in the lexicon file, will have no chance of being generated.\n",
    "\n",
    "On the other hand, the greedy decoder (available as an option during the `riva-build` process with the flag `--decoder_type=greedy`) is not lexicon-based and hence can virtually produce any word or character sequence.\n",
    "\n",
    "### Prerequisite\n",
    "\n",
    "This notebook assumes that the user is familiar with manually deploying a Riva ASR NIM using the Riva ServiceMaker tool, `riva-build` and `riva-deploy` commands. See Riva NIM [documentation](https://docs.nvidia.com/nim/riva/asr/latest/custom-deployment.html#deploying-custom-models-as-nim).\n",
    "\n",
    "### Terminologies\n",
    "- **Vocabulary file**: The vocabulary file is a flat text file containing a list of vocabulary words, each on its own line. For example:\n",
    "```\n",
    "the\n",
    "i\n",
    "to\n",
    "and\n",
    "a\n",
    "you\n",
    "of\n",
    "that\n",
    "...\n",
    "```\n",
    "\n",
    "This file is used by the `riva-build` process to generate the lexicon file. \n",
    "\n",
    "- **Lexicon file**: The lexicon file is a flat text file that contains the mapping of each vocabulary word to its tokenized form, e.g, sentencepiece tokens, separated by a `tab`. Below is an example:\n",
    "\n",
    "```\n",
    "with    ▁with\n",
    "not     ▁not\n",
    "this    ▁this\n",
    "just    ▁just\n",
    "my      ▁my\n",
    "as      ▁as\n",
    "don't   ▁don ' t\n",
    "```\n",
    "\n",
    "*Note: Ultimately, the Riva decoder makes use only of the lexicon file directly at run time (but not the vocabulary file).*\n",
    "\n",
    "Riva ServiceMaker automatically tokenizes the words in the vocabulary file to generate the lexicon file. It uses the correct tokenizer model that is packaged together with the acoustic model in the `.riva` file. By default, Riva generates 1 tokenized form for each word in the vocabulary file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can be customized?\n",
    "\n",
    "Both the vocabulary and the lexicon files can be customized.\n",
    "\n",
    "- Extending the vocabulary enriches the Riva default vocabulary, providing additional coverage for out-of-vocabulary words, terminologies, and abbreviations.\n",
    "\n",
    "- Customizing the lexicon file can further enrich the Riva knowledge base by providing one or more explicit pronunciations, in the form of tokenized sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending the vocabulary\n",
    "\n",
    "Extending the vocabulary must be done at Riva **build** time.\n",
    "\n",
    "When building a Riva ASR NIM pipeline, pass the [extended vocabulary file](#modify_vocab) to the `--decoding_vocab=<vocabulary_file>` parameter of the build command. For example, the build command for the Parakeet model:\n",
    "\n",
    "```\n",
    "    riva-build speech_recognition \\\n",
    "   <rmir_filename>:<key> <riva_filename>:<key> \\\n",
    "   --name=parakeet-english-asr-streaming \\\n",
    "   --ms_per_timestep=80 \\\n",
    "   --decoding_language_model_binary=<lm_binary> \\\n",
    "   --decoding_vocab=<vocabulary_file> \\\n",
    "   --language_code=en-US \\\n",
    "   <other_parameters>...\n",
    "```\n",
    "\n",
    "Refer to Riva NIM [documentation](https://docs.nvidia.com/nim/riva/asr/latest/pipeline-configuration.html) for build commands for supported models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modify_vocab'></a>\n",
    "### How to modify vocabulary file\n",
    "\n",
    "You can either provide your own vocabulary file, or extend Riva's default vocabulary file.\n",
    "\n",
    "- BYO vocabulary file: provide a flat text file containing a list of vocabulary words, each on its own line. Note that this file must not only contain a small list of \"difficult words\", but must contains all the words that you want the ASR pipeline to be able to generate, that is, including all common words.\n",
    "\n",
    "- Modifying an existing one: This is the recommended approach. Out-of-the-box vocabulary files  for Riva supported languages can be found on NGC, for example, for English, the vocabulary file named `flashlight_decoder_vocab.txt` can be found at this [link](https://catalog.ngc.nvidia.com/orgs/nim/teams/nvidia/models/parakeet-1-1b-ctc-en-us_finetune).\n",
    "   \n",
    "You can extend this default vocabulary file with the words of interest.\n",
    "\n",
    "Once modified, you'll have to deploy the Riva ASR NIM with `riva-build` while passing the flag `--decoding_vocab=<modified_vocabulary_file>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing pronunciation with lexicon mapping\n",
    "\n",
    "The lexicon file that is used by the Flashlight decoder can be found in the Riva models directory inside the NIM container. If you have deployed Parakeet 1.1B CTC en-US NIM, following command can be used to get the lexicon file.\n",
    "```bash\n",
    "        # Copy the lexicon file from the docker to the current directory\n",
    "        docker cp parakeet-1-1b-ctc-en-us:/data/models/parakeet-1.1b-en-US-asr-streaming-asr-bls-ensemble/1/lexicon.txt /dest\n",
    "```      \n",
    "\n",
    "### How to modify the lexicon file\n",
    "\n",
    "Modify the lexicon file to add the sentencepiece tokenizations for the words of interest. For example, one could add:\n",
    "```\n",
    "manu ▁ma n u\n",
    "manu ▁man n n ew\n",
    "manu ▁man n ew\n",
    "```\n",
    "which are 3 different pronunciations/tokenizations of the word `manu`.  If the acoustic model predicts those tokens, they will be decoded as `manu`.\n",
    "\n",
    "Once this is done, regenerate the model repository using that new decoding lexicon tokenization by passing `--decoding_lexicon=modified_lexicon.txt` to `riva-build` instead of `--decoding_vocab=decoding_vocab.txt`.\n",
    "\n",
    "### How to generate the correct tokenized form\n",
    "\n",
    "When modifying the lexicon file, ensure that:\n",
    "\n",
    "- The new lines follow the indentation/space pattern like the rest of the file and that the tokens used are part of the tokenizer model. \n",
    "\n",
    "- The tokens are valid tokens as determined by the tokenizer model (packaged with the Riva acoustic model).\n",
    "\n",
    "The latter ensures that you use only tokens that the acoustic model has been trained on. To do this, you’ll need the tokenizer model and the `sentencepiece` Python package (`pip install sentencepiece`). You can get the tokenizer model for the deployed pipeline from the model repository `...-asr-bls-ensemble` directory for your model. It will be named `<hash>_tokenizer.model`. For example:\n",
    "\n",
    "`/data/models/parakeet-1.1b-en-US-asr-streaming-asr-bls-ensemble/1/ef7c2379c9624a688ad233ab2d455e35_tokenizer.model`\n",
    "\n",
    "When using a docker volume to store Riva assets (by default), you can copy the tokenizer model to the local directory with a command such as:\n",
    "\n",
    "```bash\n",
    "        # Copy the tokenizer model file from the docker image to the current directory\n",
    "        docker cp parakeet-1-1b-ctc-en-us:/data/models/parakeet-1.1b-en-US-asr-streaming-asr-bls-ensemble/1/ef7c2379c9624a688ad233ab2d455e35_tokenizer.model /dest\n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then generate new lexicon entries, for example:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN=\"BRAF\"\n",
    "PRONUNCIATION=\"b raf\"\n",
    "\n",
    "import sentencepiece as spm\n",
    "s = spm.SentencePieceProcessor(model_file='tokenizer.model')\n",
    "for n in range(5):\n",
    "    print(TOKEN + '\\t' + ' '.join(s.encode(PRONUNCIATION, out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `TOKEN` represents the desired written form of the word, while `PRONUNCIATION` is what the word should sound like.\n",
    "\n",
    "Other examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN=\"WhatsApp\"\n",
    "PRONUNCIATION=\"what's app\"\n",
    "\n",
    "import sentencepiece as spm\n",
    "s = spm.SentencePieceProcessor(model_file='tokenizer.model')\n",
    "for n in range(5):\n",
    "    print(TOKEN + '\\t' + ' '.join(s.encode(PRONUNCIATION, out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN=\"Cya\"\n",
    "PRONUNCIATION=\"See ya\"\n",
    "\n",
    "import sentencepiece as spm\n",
    "s = spm.SentencePieceProcessor(model_file='tokenizer.model')\n",
    "for n in range(5):\n",
    "    print(TOKEN + '\\t' + ' '.join(s.encode(PRONUNCIATION, out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go deeper into Riva capabilities\n",
    "\n",
    "\n",
    "### Additional Riva Tutorials\n",
    "\n",
    "Checkout more Riva tutorials [here](https://github.com/nvidia-riva/tutorials) to understand how to use some of the advanced features of Riva ASR, including customizing ASR for your specific needs.\n",
    "\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "For more information about each of the Riva APIs and their functionalities, refer to the [documentation](https://docs.nvidia.com/nim/riva/asr/latest/protos.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
