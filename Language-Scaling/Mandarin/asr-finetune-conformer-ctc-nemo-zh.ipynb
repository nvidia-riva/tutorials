{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NpvEOMs_mKp"
   },
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/rivaasrasr-finetuning-conformer-ctc-nemo/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to Fine-Tune a Character Based Riva ASR Acoustic Model with NVIDIA NeMo\n",
    "# Take Mandarin (ZH) ASR as an example\n",
    "Characters are usually used as modeling untis for East-Asia Languages like Mandarin, Cantonese and Japanese in Speech and Language Processing tasks. In this tutorial, we will take Mandarin ASR as an example to walk you through how to fine-tune a Char-based ASR acoustic model with NVIDIA NeMo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6h7SXF6G_mKw"
   },
   "source": [
    "## 0. NVIDIA Riva Overview\n",
    "\n",
    "NVIDIA Riva is a GPU-accelerated SDK for building speech AI applications that are customized for your use case and deliver real-time performance. <br/>\n",
    "Riva offers a rich set of speech processing and natural language understanding (NLU) services such as:\n",
    "\n",
    "- Automated speech recognition (ASR). \n",
    "- Text-to-Speech synthesis (TTS). \n",
    "- A collection of natural language processing (NLP) services, such as named entity recognition (NER), punctuation, and intent classification.\n",
    "\n",
    "In this tutorial, we will fine-tune a Riva ASR acoustic model with NeMo. <br> \n",
    "To understand the basics of Riva ASR APIs, refer to [Getting started with Riva ASR in Python](https://github.com/nvidia-riva/tutorials/blob/stable/asr-basics.ipynb). <br>\n",
    "\n",
    "For more information about Riva, refer to the [Riva developer documentation](https://developer.nvidia.com/riva).\n",
    "\n",
    "**HINTS: We highly recommend you to run this Jupyter Notebook using pre-built NeMo docker image `nvcr.io/nvidia/nemo:23.04` to save your time!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rv47EmBp_mK2"
   },
   "source": [
    "## 1. NeMo (Neural Modules)\n",
    "[NVIDIA NeMo](https://developer.nvidia.com/nvidia-nemo) is an open-source framework for building, training, and fine-tuning GPU-accelerated speech AI and NLU models with a simple Python interface. For information about how to set up NeMo, refer to the [NeMo GitHub](https://github.com/NVIDIA/NeMo) instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1O2bGCMAQDs8",
    "outputId": "a8731f17-b61c-4457-ae39-7079b8c4edd3"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can run either this tutorial locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Perform the following steps to setup in Google Colab:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub.\n",
    "   a. Click **File** > **Upload Notebook** > **GITHUB** tab > copy/paste the GitHub URL.\n",
    "3. Connect to an instance with a GPU.\n",
    "   a. Click **Runtime** > Change the runtime type > select **GPU** for the hardware accelerator.\n",
    "4. Run this cell to set up the dependencies.\n",
    "5. Restart the runtime.\n",
    "   a. Click **Runtime** > **Restart Runtime** for any upgraded packages to take effect.\n",
    "\"\"\"\n",
    "\n",
    "# Install Dependencies\n",
    "!pip install wget\n",
    "!apt-get update && apt-get install -y sox libsndfile1 ffmpeg libsox-fmt-mp3\n",
    "!pip install text-unidecode\n",
    "!pip install matplotlib>=3.3.2\n",
    "!pip install Cython\n",
    "\n",
    "# ## Install NeMo\n",
    "!pip3 install nemo_toolkit['all']==1.12.0\n",
    "\n",
    "\"\"\"\n",
    "Remember to restart the runtime for the kernel to pick up any upgraded packages (e.g. matplotlib)!\n",
    "Alternatively, in the case where you want to use the \"Run All Cells\" (or similar) option, \n",
    "uncomment `exit()` below to crash and restart the kernel.\n",
    "\"\"\"\n",
    "# exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vo27Unex_mLG"
   },
   "source": [
    "---\n",
    "## 2. Fine-Tuning an ASR model with NeMo\n",
    "### 2.1 Pre-trained models\n",
    "First of all, you can check all pre-trained Riva Mandarin ASR models in [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/speechtotext_zh_cn_conformer). \n",
    "\n",
    "You have to login in NGC with your account and download the desired models. You need to download the `trainable_*` models for finetune. You can download the .nemo file from [website](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/speechtotext_zh_cn_conformer/files?version=trainable_v5.0) or using [NGC CLI](https://ngc.nvidia.com/setup/installers/cli), for example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngc registry model download-version \"nvidia/riva/speechtotext_zh_cn_conformer:trainable_v5.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you will get the pre-trained model in `speechtotext_zh_cn_conformer_vtrainable_v5.0/Conformer-CTC-L_char_zh-CN_5.0.nemo`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfRpENJJ_mLy"
   },
   "source": [
    "### 2.2 Download Mandarin Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKrmQQuK_mLz"
   },
   "source": [
    "In this tutorial, we will use open-sourced `AISHELL-1` dataset as an example. Now let's download the AISHELL-1 data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0eFF0r7z_mLz",
    "outputId": "c9f3772d-76ea-4e9d-b61e-64e32a6281df"
   },
   "outputs": [],
   "source": [
    "!mkdir -p datasets/aishell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the `get_aishell_data.py` script located in the nemo/scripts/dataset_processing dir if you cloned NeMo repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"get_aishell_data.py\"):\n",
    "    !wget https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/dataset_processing/get_aishell_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAsvlh53_mL0"
   },
   "source": [
    "Then just run the following command, you will finish downloading and data processing for AISHELL-1. It will take some time for downloading which depends on your network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OipV7YVq_mL1",
    "outputId": "eaeacde1-a279-444a-af72-31d244b78162"
   },
   "outputs": [],
   "source": [
    "!python get_aishell_data.py --data_root \"datasets/aishell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls datasets/aishell/data_aishell/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will get `train.json`, `dev.json` and `test.json` in the directory. To save time, you can create smaller subsets of the Mandarin dataset for training and validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -1000 datasets/aishell/data_aishell/train.json > datasets/aishell/data_aishell/train_1000.json\n",
    "!head -1000 datasets/aishell/data_aishell/dev.json > datasets/aishell/data_aishell/dev_1000.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 3 datasets/aishell/data_aishell/train_1000.json\n",
    "!head -n 3 datasets/aishell/data_aishell/dev_1000.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line of the manifest has 3 required fields including `audio_filepath` which means the file path of each audio, `duration` indicating the duration of the audio and `text` giving the transcription of the audio file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RAtW4wg_mL4"
   },
   "source": [
    "Let's listen to a sample audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGH3btfn_mL6"
   },
   "outputs": [],
   "source": [
    "# change path of the file here\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "path = 'datasets/aishell/data_aishell/wav/train/S0062/BAC009S0062W0157.wav'\n",
    "ipd.Audio(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Use your own data\n",
    "NeMo provides several data processing scripts for Mandarin including AISHELL-1 and AISHELL-2. You can check all supported datasets [here](https://github.com/NVIDIA/NeMo/tree/main/scripts/dataset_processing). \n",
    "\n",
    "If you want to train the model using your own data, you have to prepare training manifest similar as AISHELL-1 shown above. The `audio_filepath`, `duration` and `text` must be filled with your own data, like:  \n",
    "\n",
    " * {\"audio_filepath\": \"datasets/aishell/data_aishell/wav/dev/S0728/BAC009S0728W0126.wav\", \"duration\": 3.758, \"text\": \"必然先行抛售二三线城市的房产\"}\n",
    " \n",
    "You can use AISHELL-1 dev set for evaluation and also other testsets can be used. The data format should be the same as above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FubCSAin_mMC"
   },
   "source": [
    "### 2.4 Training \n",
    "After data preparation, now we can go through the training section! \n",
    "\n",
    "#### 2.4.0 Load a checkpoint\n",
    "First of all, you have to load the pre-trained checkpoint which you have just downloaded from NGC.\n",
    "\n",
    "You can use same vocabulary for your ASR model training and skip Section 2.4.1 since our model has covered most of the Chinese Characters. But if you custormize the vocaulary, next section will show the detailed steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.collections.asr.models import ASRModel\n",
    "from nemo.utils import model_utils\n",
    "\n",
    "# Locate the .nemo file which you downloaded from NGC\n",
    "model_path = 'Conformer-CTC-L_char_zh-CN_5.0.nemo'\n",
    "\n",
    "model_cfg = ASRModel.restore_from(restore_path=model_path, return_config=True)\n",
    "classpath = model_cfg.target  # original class path\n",
    "imported_class = model_utils.import_class_by_path(classpath)  # type: ASRModel\n",
    "\n",
    "asr_model = imported_class.restore_from(restore_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjHrIfDj_mMD"
   },
   "source": [
    "#### 2.4.1 Change vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zBKAO36_mME"
   },
   "source": [
    "Character based models don't need the tokenizer creation as only single characters are regarded as elements in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "maoasPkL_mMF",
    "outputId": "f468df62-ce9f-4828-ef6a-06e890de8b0e"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Get frequency of each character and get their orders\n",
    "def get_occ(filename):\n",
    "    char_set = Counter()\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            txt = data['text']\n",
    "            for c in txt:\n",
    "                char_set[c] += 1\n",
    "    char_set = dict(sorted(char_set.items(), key=lambda x: x[1], reverse=True))\n",
    "    return char_set\n",
    "\n",
    "train_charset = get_occ('datasets/aishell/data_aishell/train_1000.json')\n",
    "print(len(train_charset))\n",
    "\n",
    "vocab_train = train_charset.keys()\n",
    "print(vocab_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the following function to change the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model.change_vocabulary(new_vocabulary=list(vocab_train))\n",
    "\n",
    "print(len(asr_model.cfg.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(asr_model.cfg.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model.cfg.labels = asr_model.cfg.train_ds.labels\n",
    "asr_model.save_to(\"asr_customize_vocab.nemo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model = nemo_asr.models.EncDecCTCModel.restore_from(restore_path=\"asr_customize_vocab.nemo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-yDMTMO_mMJ"
   },
   "source": [
    "#### 2.4.2 Change configurations\n",
    "NeMo uses `.yaml` files to configure the training parameters. There's an [example yaml](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/conf/conformer/conformer_ctc_char.yaml) file in NeMo for the Char-based ASR model. You may update them directly by editing the configuration file or from the command-line interface. For example, if the number of epochs needs to be modified, along with a change in the learning rate, you can add `trainer.max_epochs=100` and `optim.lr=0.02` and train the model. \n",
    "\n",
    "You can also use `model.cfg` to modify the configurations since each NeMo model has a config embedded in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(asr_model.cfg.labels))\n",
    "print(asr_model.cfg.use_cer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Mandarin ASR model, we use `Character Error Rate (CER)` to evaluate the performaces so we must set `use_cer=True` in NeMo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model.cfg.use_cer = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3 Initialize a trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-bkkgxxv_mMK",
    "outputId": "9aeca99e-c3e0-4784-97f4-13cf997c2247"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as ptl\n",
    "\n",
    "GRAD_ACCUM=1\n",
    "MAX_EPOCHS=5\n",
    "GPUS=[0]\n",
    "LOG_EVERY_N_STEPS=10\n",
    "\n",
    "trainer = ptl.Trainer(devices=GPUS, \n",
    "                      accelerator=\"gpu\",\n",
    "                      max_epochs=MAX_EPOCHS, \n",
    "                      accumulate_grad_batches=GRAD_ACCUM,\n",
    "                      precision=32,\n",
    "                      enable_checkpointing=False,\n",
    "                      logger=False,\n",
    "                      log_every_n_steps=LOG_EVERY_N_STEPS,\n",
    "                      enable_progress_bar=True,\n",
    "                      check_val_every_n_epoch=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhGF0z_-rl8o",
    "outputId": "060b0b4c-224d-4242-bdef-72e0e346e137"
   },
   "outputs": [],
   "source": [
    "asr_model.set_trainer(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.4 Specify the training and validation manifests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = {}\n",
    "train_ds['manifest_filepath'] = ['datasets/aishell/data_aishell/train_1000.json']\n",
    "train_ds['sample_rate'] = 16000\n",
    "train_ds['labels'] = asr_model.cfg.labels\n",
    "train_ds['batch_size'] = 16\n",
    "train_ds['fused_batch_size'] = 16\n",
    "train_ds['shuffle'] = True\n",
    "train_ds['max_duration'] = 20.0\n",
    "train_ds['pin_memory'] = True\n",
    "train_ds['is_tarred'] = False\n",
    "train_ds['num_workers'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model.setup_training_data(train_data_config=train_ds)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_ds = {}\n",
    "validation_ds['sample_rate'] = 16000\n",
    "validation_ds['manifest_filepath'] = ['datasets/aishell/data_aishell/dev_1000.json']\n",
    "validation_ds['labels'] = asr_model.cfg.labels\n",
    "validation_ds['batch_size'] = 32\n",
    "validation_ds['shuffle'] = False\n",
    "validation_ds['num_workers'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model.setup_multiple_validation_data(val_data_config=validation_ds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.5 Set Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_conf = {}\n",
    "\n",
    "optimizer_conf['name'] = 'adamw'\n",
    "optimizer_conf['lr'] = 0.01\n",
    "optimizer_conf['betas'] =  [0.9, 0.98]\n",
    "optimizer_conf['weight_decay'] = 0\n",
    "\n",
    "sched = {}\n",
    "sched['name'] = 'CosineAnnealing'\n",
    "sched['warmup_steps'] = None\n",
    "sched['warmup_ratio'] = 0.10\n",
    "sched['min_lr'] = 1e-6\n",
    "optimizer_conf['sched'] = sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model.setup_optimization(optimizer_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.6 Set exp manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.utils import exp_manager\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "\n",
    "config = exp_manager.ExpManagerConfig(\n",
    "    exp_dir=\"experiments/zh/\"\",\n",
    "    name=\"Conformer-CTC\",\n",
    "    checkpoint_callback_params=exp_manager.CallbackParams(\n",
    "        monitor=\"val_wer\",\n",
    "        mode=\"min\",\n",
    "        always_save_nemo=True,\n",
    "        save_best_model=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "config = OmegaConf.structured(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model.log_predictions = False\n",
    "# set to True if you would like to track the evaluation loss\n",
    "asr_model.compute_eval_loss = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us freeze the encoder for easier initial convergence and faster training. On a smaller dataset when retraining the decoder, this is often a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def enable_bn_se(m):\n",
    "    if type(m) == nn.BatchNorm1d:\n",
    "        m.train()\n",
    "        for param in m.parameters():\n",
    "            param.requires_grad_(True)\n",
    "asr_model.encoder.freeze()\n",
    "asr_model.encoder.apply(enable_bn_se)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.7 Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asr_model.cfg.labels = asr_model.cfg.train_ds.labels\n",
    "trainer.fit(asr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.8 Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model.save_to(\"train_asr_customize_vocab.nemo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Use Training Scripts\n",
    "You can directly use the [speech_to_text_ctc.py](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/asr_ctc/speech_to_text_ctc.py) script to start the training. The above parameters are supported in the scripts. For example, you can start a training with:\n",
    "```bash\n",
    "    python speech_to_text_ctc.py \\\n",
    "        model.train_ds.manifest_filepath=\"datasets/aishell/data_aishell/train_1000.json\" \\\n",
    "        model.validation_ds.manifest_filepath=\"datasets/aishell/data_aishell/dev_1000.json\" \\\n",
    "        trainer.devices=1 \\\n",
    "        trainer.accelerator='gpu' \\\n",
    "        trainer.max_epochs=5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKDipfZF_mMK"
   },
   "source": [
    "## 3. ASR Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3svjWpR_mMN"
   },
   "source": [
    "Now that we have a model trained, we need to check how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D4ATNCSe_mMN",
    "outputId": "b9039c6c-8840-4ce2-9089-689c29a8a162"
   },
   "outputs": [],
   "source": [
    "test_ds = {}\n",
    "test_ds['sample_rate'] = 16000\n",
    "test_ds['manifest_filepath'] = ['datasets/aishell/data_aishell/dev_1000.json']\n",
    "test_ds['batch_size'] = 32\n",
    "test_ds['num_workers'] = 4\n",
    "test_ds['labels'] = asr_model.cfg.labels\n",
    "test_ds['use_cer'] = True\n",
    "\n",
    "asr_model.setup_test_data(test_data_config=test_ds)\n",
    "trainer.test(asr_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9ygVAEC_mMS"
   },
   "source": [
    "## 4. ASR Model Export "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ap5CjiV4_mMT"
   },
   "source": [
    "With NeMo, you can also export your model in a format that can be deployed using NVIDIA Riva: a highly performant application framework for multi-modal conversational AI services using GPUs. The same command for exporting to ONNX can be used here. The only small variation is the configuration for `export_format` in the spec file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myI104s2pY13"
   },
   "source": [
    "### 4.1 Install the Packages\n",
    "\n",
    "We will now install the NeMo and `nemo2riva` packages. `nemo2riva` is available on [NVIDIA NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/resources/riva_quickstart/files?version=2.8.1). Make sure you install `NGC CLI` first before running the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pd0Y3Gys_mMU",
    "outputId": "31cb1a3a-e539-4d5a-e395-862525f3cb99"
   },
   "outputs": [],
   "source": [
    "!pip install nvidia-pyindex\n",
    "!pip install nemo2riva\n",
    "!pip install protobuf==3.20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XNolXIPptAr"
   },
   "source": [
    "### 4.2 Convert to Riva\n",
    "\n",
    "Convert the downloaded model to the `.riva` format. We will set the encryption key with `--key=nemotoriva`. Choose a different encryption key value when generating `.riva` models for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PlUNmVHKp1ft",
    "outputId": "f612c5e8-604e-4eee-8806-4eb463d1e810"
   },
   "outputs": [],
   "source": [
    "!nemo2riva --out \"train_asr_customize_vocab.riva\" \"train_asr_customize_vocab.nemo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAtoNe9krC2z"
   },
   "source": [
    "## More Resources\n",
    "You can find more information about working with NeMo's ASR models in the [ASR section](https://github.com/NVIDIA/NeMo/tree/main/tutorials/asr) of the NeMo tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lv7ZRPoc_mMa"
   },
   "source": [
    "## What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJ7hWpRT_mMb"
   },
   "source": [
    "You can use NeMo to build custom models for your own applications, and deploy them with NVIDIA Riva! Refer to the [Conformer-CTC deployment tutorial](https://github.com/nvidia-riva/tutorials/blob/main/asr-deployment-conformer-ctc.ipynb)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
