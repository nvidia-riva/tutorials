{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc054b6",
   "metadata": {},
   "source": [
    "# VoxPopuli Dataset\n",
    "\n",
    "[VoxPopuli](https://aclanthology.org/2021.acl-long.80) is a large-scale multilingual speech corpus for representation learning, semi-supervised learning, and interpretation. VoxPopuli provides:\n",
    "\n",
    "- 400K hours of unlabeled speech data for 23 languages\n",
    "- 1.8K hours of transcribed speech data for 16 languages\n",
    "- 17.3K hours of speech-to-speech interpretation data for 15x15 directions\n",
    "- 29 hours of transcribed speech data of non-native English intended for research in ASR for accented speech (15 L2 accents)\n",
    "\n",
    "The raw data is collected from 2009-2020 European Parliament event recordings. \n",
    "\n",
    "In this tutorial, we will only use the German portion of the dataset.\n",
    "\n",
    "## Download\n",
    "\n",
    "First, we install the necessary packages and download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914870b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "git clone https://github.com/facebookresearch/voxpopuli.git\n",
    "cd voxpopuli\n",
    "pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cd58ec",
   "metadata": {},
   "source": [
    "Next, we prepare a folder to store the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./data/raw/voxpopuli\n",
    "!cd voxpopuli && python3 -m voxpopuli.download_audios --root ../data/raw/voxpopuli --subset asr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e5fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd voxpopuli && python3 -m voxpopuli.get_asr_data --root ../data/raw/voxpopuli --lang de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e3cde2",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Next, we standardize the audio data and convert the raw format to a NeMo manifest format.\n",
    "\n",
    "**Audio data**: Audio data acquired from various sources are inherently heterogeneous (file format, sample rate, bit depth, number of audio channels, and so on). Therefore, as a preprocessing step, we build a separate data ingestion pipeline for each source and convert the audio data to a common format with the following characteristics:\n",
    "- Wav format\n",
    "- Bit depth: 16 bits\n",
    "- Sample rate of 16 Khz\n",
    "- Single audio channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f65978",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./data/processed/voxpopuli\n",
    "!python3 ./data_ingestion/process_voxpopuli.py --data_root=./data/raw/voxpopuli/transcribed_data --out_dir=./data/processed/voxpopuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6713c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally: to remove the raw dataset to preserve disk space, uncomment the bash command bellow. \n",
    "\n",
    "#! rm -rf ./data/processed/voxpopuli"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
