{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_asr_asr-python-advanced-finetune-am-conformer-ctc-for-noisy-audio-withtao/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to improve accuracy on noisy speech by fine-tuning the Acoustic Model (Conformer-CTC) in the Riva ASR pipeline \n",
    "\n",
    "This tutorial walks you through some of the advanced customization features of the Riva ASR pipeline by fine-tuning the Acoustic Model (Conformer-CTC). These customization features improve accuracy on specific speech scenarios, like background noise and different acoustic environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVIDIA Riva Overview\n",
    "\n",
    "NVIDIA Riva is a GPU-accelerated SDK for building Speech AI applications that are customized for your use case and deliver real-time performance. <br/>\n",
    "Riva offers a rich set of speech and natural language understanding services such as:\n",
    "\n",
    "- Automated speech recognition (ASR)\n",
    "- Text-to-Speech synthesis (TTS)\n",
    "- A collection of natural language processing (NLP) services, such as named entity recognition (NER), punctuation, and intent classification.\n",
    "\n",
    "In this tutorial, we will show how to augment your training data with background noise data for fine-tuning the Acoustic Model (Conformer-CTC) to improve accuracy on audio with background noise.  \n",
    "To understand the basics of Riva ASR APIs, refer to [Getting started with Riva ASR in Python](https://github.com/nvidia-riva/tutorials/blob/stable/asr-python-basics.ipynb). <br>\n",
    "\n",
    "For more information about Riva, refer to the Riva [product page](https://developer.nvidia.com/riva) and [documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/overview.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "For fine-tuning we need audio data with background noise. If you already have such data, then you can use it directly.  \n",
    "In this tutorial, we will take the AN4 dataset and augment it with noise data from the Room Impulse Response and Noise Database from the [openslr database](https://www.openslr.org/28/).\n",
    "\n",
    "In this tutorial, we will be using NVIDIA NeMo for the data preprocessing step.\n",
    "\n",
    "#### NVIDIA NeMo Overview\n",
    "\n",
    "NVIDIA NeMo is a toolkit for building new state-of-the-art conversational AI models. NeMo has separate collections for Automatic Speech Recognition (ASR), Natural Language Processing (NLP), and Text-to-Speech (TTS) models. Each collection consists of prebuilt modules that include everything needed to train on your data. Every module can easily be customized, extended, and composed to create new conversational AI model architectures.\n",
    "For more information about NeMo, refer to the [NeMo product page](https://developer.nvidia.com/nvidia-nemo) and [documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/starthere/intro.html). The open-source NeMo repository can be found [here](https://github.com/NVIDIA/NeMo).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements and setup for data preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Requirements:\n",
    "\n",
    "We will be using [NVIDIA NeMo](https://github.com/NVIDIA/NeMo) for this data preprocessing step - Easiest way to install and run NeMo is through NVIDIA's PyTorch docker container. If you are not already running this notebook in the NVIDIA PyTorch docker container, please follow instructions [here](./README.md#running-the-nvidia-riva-tutorial-how-to-improve-accuracy-on-specific-speech-patterns-by-fine-tuning-the-acoustic-model-citrinet-in-the-riva-asr-pipeline) to re-run this tutorial from the PyTorch docker container\n",
    "\n",
    "We will be using the NVIDIA NeMo Docker container, so access to NGC is a must. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Download and process the AN4 dataset\n",
    "AN4 is a small dataset recorded and distributed by Carnegie Mellon University (CMU). It consists of recordings of people spelling out addresses, names, etc. Information about this dataset can be found on the official CMU site.\n",
    "\n",
    "Let's download the AN4 dataset tar file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the working directory for this part of the tutorial. \n",
    "working_dir = 'am_finetuning/'\n",
    "!mkdir -p $working_dir\n",
    "\n",
    "# Import the necessary dependencies.\n",
    "import wget\n",
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "import tarfile\n",
    "\n",
    "# The AN4 directory will be created in `data_dir`. It is currently set to the `working_dir`.\n",
    "data_dir = os.path.abspath(working_dir)\n",
    "\n",
    "# Download the AN4 dataset if it doesn't already exist in `data_dir`. \n",
    "# This will take a few moments...\n",
    "# We also set `an4_path` which points to the downloaded an4 dataset\n",
    "if not os.path.exists(data_dir + '/an4_sphere.tar.gz'):\n",
    "    an4_url = 'https://dldata-public.s3.us-east-2.amazonaws.com/an4_sphere.tar.gz'\n",
    "    an4_path = wget.download(an4_url, data_dir)\n",
    "    print(f\"AN4 dataset downloaded at: {an4_path}\")\n",
    "else:\n",
    "    print(\"AN4 dataset tarfile already exists. Proceed to the next step.\")\n",
    "    an4_path = data_dir + '/an4_sphere.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's untar the tar file to give us the dataset audio files in `.sph` format. Then, we'll convert the `.sph` files to 16kHz `.wav` files using the SoX library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir + '/an4/'):\n",
    "    # Untar\n",
    "    tar = tarfile.open(an4_path)\n",
    "    tar.extractall(path=data_dir)\n",
    "    print(\"Completed untarring the an4 tarfile\")\n",
    "    # Convert .sph to .wav (using sox)\n",
    "    print(\"Converting .sph to .wav...\")\n",
    "    sph_list = glob.glob(data_dir + '/an4/**/*.sph', recursive=True)\n",
    "    for sph_path in sph_list:\n",
    "        wav_path = sph_path[:-4] + '.wav'\n",
    "        #converting to 16kHz wav\n",
    "        cmd = f\"sox {sph_path} -r 16000 {wav_path}\"\n",
    "        subprocess.call(cmd, shell=True)\n",
    "    print(\"Finished converting the .sph files to .wav files\")\n",
    "else:\n",
    "    print(\"an4 dataset directory already exists. Proceed to the next step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's build the manifest files for the AN4 dataset. The manifest file is a `.json` file that maps the `.wav` clip to its corresponding text.\n",
    "\n",
    "Each entry in the AN4 dataset's manifest `.json` file follows the template:  \n",
    "`{\"audio_filepath\": \"<.wav file location>\", \"duration\": <duration of the .wav file>, \"text\": \"<text from the .wav file>\"}`  \n",
    "Example: `{\"audio_filepath\": \"/tutorials/am_finetuning/an4/wav/an4_clstk/fash/an251-fash-b.wav\", \"duration\": 1.0, \"text\": \"yes\"}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries.\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "# Method to build a manifest.\n",
    "def build_manifest(transcripts_path, manifest_path, wav_path):\n",
    "    with open(transcripts_path, 'r') as fin:\n",
    "        with open(manifest_path, 'w') as fout:\n",
    "            for line in fin:\n",
    "                # Lines look like this:\n",
    "                # <s> transcript </s> (fileID)\n",
    "                transcript = line[: line.find('(')-1].lower()\n",
    "                transcript = transcript.replace('<s>', '').replace('</s>', '')\n",
    "                transcript = transcript.strip()\n",
    "\n",
    "                file_id = line[line.find('(')+1 : -2]  # e.g. \"cen4-fash-b\"\n",
    "                audio_path = os.path.join(\n",
    "                    data_dir, wav_path,\n",
    "                    file_id[file_id.find('-')+1 : file_id.rfind('-')],\n",
    "                    file_id + '.wav')\n",
    "\n",
    "                duration = float(subprocess.check_output(\n",
    "                      \"soxi -D {0}\".format(audio_path), shell=True))\n",
    "                #duration = WAVE(filename=audio_path).info.length\n",
    "\n",
    "                # Write the metadata to the manifest\n",
    "                metadata = {\n",
    "                    \"audio_filepath\": audio_path,\n",
    "                    \"duration\": duration,\n",
    "                    \"text\": transcript\n",
    "                }\n",
    "                \n",
    "                fout.write(json.dumps(metadata) + '\\n')\n",
    "                \n",
    "                \n",
    "# Building the manifest files.\n",
    "print(\"***Building manifest files***s\")\n",
    "\n",
    "# Building manifest files for the training data\n",
    "train_transcripts = data_dir + '/an4/etc/an4_train.transcription'\n",
    "train_manifest = data_dir + '/an4/train_manifest.json'\n",
    "if not os.path.isfile(train_manifest):\n",
    "    build_manifest(train_transcripts, train_manifest, 'an4/wav/an4_clstk')\n",
    "    print(\"Training manifest created at\", train_manifest)\n",
    "else:\n",
    "    print(\"Training manifest already exists at\", train_manifest)\n",
    "\n",
    "# Building manifest files for the test data\n",
    "test_transcripts = data_dir + '/an4/etc/an4_test.transcription'\n",
    "test_manifest = data_dir + '/an4/test_manifest.json'\n",
    "if not os.path.isfile(test_manifest):\n",
    "    build_manifest(test_transcripts, test_manifest, 'an4/wav/an4test_clstk')\n",
    "    print(\"Test manifest created at\", test_manifest)\n",
    "else:\n",
    "    print(\"Test manifest already exists at\", test_manifest)\n",
    "\n",
    "print(\"***Done***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and process the background noise dataset\n",
    "\n",
    "For background noise, we will use the background noise samples from the Room Impulse Response and Noise database from the openslr database. For each 30 second isotropic noise sample in the dataset we use the first 15 seconds for training and the last 15 seconds for evaluation.\n",
    "\n",
    "Let's first download this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the background noise dataset if it doesn't already exist in `data_dir`. \n",
    "# This will take a few moments...\n",
    "# We also set `noise_path` which points to the downloaded background noise dataset.\n",
    "\n",
    "if not os.path.exists(data_dir + '/rirs_noises.zip'):\n",
    "    slr28_url = 'https://www.openslr.org/resources/28/rirs_noises.zip'\n",
    "    noise_path = wget.download(slr28_url, data_dir)\n",
    "    print(\"Background noise dataset download complete.\")\n",
    "else:\n",
    "    print(\"Background noise dataset already exists. Proceed to the next step.\")\n",
    "    noise_path = data_dir + '/rirs_noises.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to unzip the `.zip` file, which gives us the dataset audio files as 8-channel `.wav` files, sampled at 16kHz. The format and sample rate suit our purposes, but we need to convert these files to mono-channel to match the files in the AN4 dataset. Fortunately, the SoX library provides tools for that as well. \n",
    "\n",
    "Note: The conversion will take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract noise data\n",
    "from zipfile import ZipFile\n",
    "if not os.path.exists(data_dir + '/RIRS_NOISES'):\n",
    "    try:\n",
    "        with ZipFile(noise_path, \"r\") as zipObj:\n",
    "            zipObj.extractall(data_dir)\n",
    "            print(\"Extracting noise data complete\")\n",
    "        # Convert 8-channel audio files to mono-channel\n",
    "        wav_list = glob.glob(data_dir + '/RIRS_NOISES/**/*.wav', recursive=True)\n",
    "        for wav_path in wav_list:\n",
    "            mono_wav_path = wav_path[:-4] + '_mono.wav'\n",
    "            cmd = f\"sox {wav_path} {mono_wav_path} remix 1\"\n",
    "            subprocess.call(cmd, shell=True)\n",
    "        print(\"Finished converting the 8-channel noise data .wav files to mono-channel\")\n",
    "    except Exception:\n",
    "        print(\"Not extracting. Extracted noise data might already exist.\")\n",
    "else: \n",
    "    print(\"Extracted noise data already exists. Proceed to the next step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's build the manifest files for the noise data. The manifest file is a `.json` file that maps the `.wav` clip to its corresponding text.\n",
    "\n",
    "Each entry in the noise data's manifest `.json` file follows the template:  \n",
    "`{\"audio_filepath\": \"<.wav file location>\", \"duration\": <duration of the .wav file>, \"offset\": <offset value>, \"text\": \"-\"}`  \n",
    "Example: `{\"audio_filepath\": \"/tutorials/am_finetuning/RIRS_NOISES/real_rirs_isotropic_noises/RVB2014_type1_noise_largeroom1_1_mono.wav\", \"duration\": 30.0, \"offset\": 0, \"text\": \"-\"}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "iso_path = os.path.join(data_dir,\"RIRS_NOISES/real_rirs_isotropic_noises\")\n",
    "iso_noise_list = os.path.join(iso_path, \"noise_list\")\n",
    "\n",
    "# Edit the noise_list file so that it lists the *_mono.wav files instead of the original *.wav files\n",
    "with open(iso_noise_list) as f:\n",
    "    if '_mono.wav' in f.read():\n",
    "        print(f\"{iso_noise_list} has already been processed\")\n",
    "    else:\n",
    "        cmd = f\"sed -i 's|.wav|_mono.wav|g' {iso_noise_list}\"\n",
    "        subprocess.call(cmd, shell=True)\n",
    "        print(f\"Finished processing {iso_noise_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the manifest files from noise files\n",
    "def process_row(row, offset, duration):\n",
    "  try:\n",
    "    entry = {}\n",
    "    wav_f = row['wav_filename']\n",
    "    newfile = wav_f\n",
    "    duration = subprocess.check_output('soxi -D {0}'.format(newfile), shell=True)\n",
    "    entry['audio_filepath'] = newfile\n",
    "    entry['duration'] = float(duration)\n",
    "    entry['offset'] = offset\n",
    "    entry['text'] = row['transcript']\n",
    "    return entry\n",
    "  except Exception as e:\n",
    "    wav_f = row['wav_filename']\n",
    "    newfile = wav_f\n",
    "    print(f\"Error processing {newfile} file!!!\")\n",
    "    \n",
    "train_rows = []\n",
    "test_rows = []\n",
    "\n",
    "with open(iso_noise_list,\"r\") as in_f:\n",
    "    for line in in_f:\n",
    "        row = {}\n",
    "        data = line.rstrip().split()\n",
    "        row['wav_filename'] = os.path.join(data_dir,data[-1])\n",
    "        row['transcript'] = \"-\"\n",
    "        train_rows.append(process_row(row, 0 , 15))\n",
    "        test_rows.append(process_row(row, 15 , 15))\n",
    "\n",
    "# Writing manifest files\n",
    "def write_manifest(manifest_file, manifest_lines):\n",
    "    with open(manifest_file, 'w') as fout:\n",
    "      for m in manifest_lines:\n",
    "        fout.write(json.dumps(m) + '\\n')\n",
    "      print(\"Writing manifest file to\", manifest_file, \"complete\")\n",
    "\n",
    "# Writing training and test manifest files\n",
    "test_noise_manifest = os.path.join(data_dir, \"test_noise.json\")\n",
    "train_noise_manifest = os.path.join(data_dir, \"train_noise.json\")\n",
    "if not os.path.exists(test_noise_manifest):\n",
    "    write_manifest(test_noise_manifest, test_rows)\n",
    "else:\n",
    "    print('Test noise manifest file already exists. Proceed to the next step.')\n",
    "if not os.path.exists(train_noise_manifest):\n",
    "    write_manifest(train_noise_manifest, train_rows)\n",
    "else:\n",
    "    print('Train noise manifest file already exists. Please proceed to the next step.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the noise-augmented dataset\n",
    "\n",
    "Finally, let's create a noise-augmented dataset by adding noise to the the AN4 dataset with the `add_noise.py` NeMo script. This script generates the noise-augmented audio clips as well as the manifest files. \n",
    "\n",
    "Each entry in the noise-augmented data's manifest file follows the template:  \n",
    "`{\"audio_filepath\": \"<.wav file location>\", \"duration\": <duration of the .wav file>, \"text\": \"<text from the .wav file>\"}`\n",
    "Example: `{\"audio_filepath\": \"/tutorials/am_finetuning/noise_data/train_manifest/train_noise_0db/an251-fash-b.wav\", \"duration\": 1.0, \"text\": \"yes\"}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup:\n",
    "\n",
    "Configure and run the NeMo Docker container. We'll keep it active for the remainder of the data preprocessing section, so as to avoid having to update it with the latest version of the NeMo repo every time we need the container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install NeMo\n",
    "BRANCH = 'main'\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n",
    "! git clone https://github.com/NVIDIA/NeMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training dataset\n",
    "Let's create a noise-augmented training dataset using the AN4 training dataset. We'll add noise at different SNRs (Signal-to-Noise Ratios) ranging from 0 to 15 dB SNR using a NeMo script. Note that a 0 dB SNR means that the noise and signal in the given audio file are of equal volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_dir = data_dir + '/noise_data'\n",
    "test_noise_manifest = os.path.join(data_dir, \"test_noise.json\")\n",
    "train_noise_manifest = os.path.join(data_dir, \"train_noise.json\")\n",
    "train_manifest = data_dir + '/an4/train_manifest.json'\n",
    "\n",
    "test_manifest = data_dir + '/an4/test_manifest.json'\n",
    "\n",
    "run = f\"python NeMo/scripts/dataset_processing/add_noise.py \\\n",
    "    --input_manifest={train_manifest} \\\n",
    "    --noise_manifest={train_noise_manifest} \\\n",
    "    --snrs 0 5 10 15 \\\n",
    "    --out_dir={final_data_dir}\"\n",
    "\n",
    "! $run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above script generates a `.json` manifest file each for every SNR value, i.e., one manifest file each for 0, 5, 10 and 15db SNR.  \n",
    "Let's combine all the manifests into a single manifest for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = f\"cat {final_data_dir}/manifests/train* >{final_data_dir}/manifests/noisy_train.json\"\n",
    "!{run}\n",
    "\n",
    "print(\"Noise-augmented training dataset created at\", final_data_dir + \"/manifests/noisy_train.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test dataset\n",
    "\n",
    "Let's create a noise-augmented evaluation dataset using the an4 test dataset, by adding noise at 5 dB, using a NeMo script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmention - Add noise to test set.\n",
    "run = f\"python NeMo/scripts/dataset_processing/add_noise.py \\\n",
    "    --input_manifest={test_manifest} \\\n",
    "    --noise_manifest={test_noise_manifest} \\\n",
    "    --snrs=5 \\\n",
    "    --out_dir={final_data_dir}\"\n",
    "\n",
    "# For some reason, adding --user=$(id -u):$(id -g) breaks the script\n",
    "!$run\n",
    "\n",
    "print(\"Noise-augmented testing dataset created at\", final_data_dir+\"/test_manifest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Noise-augmented training manifest and data are created at `{working_dir}/noise_data/noisy_train.json` and `{working_dir}/noise_data/train_manifest` respectively.**  \n",
    "**Noise-augmented testing manifest and data are created at `{working_dir}/noise_data/manifests/test_manifest_test_noise_5db.json` and `{working_dir}/noise_data/test_manifest` respectively.**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We don't need the NeMo container anymore, so let's get rid of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-tuning the ASR model\n",
    "To finetune the ASR model with the augmented datasets that we just created, you can \n",
    "proceed to [this tutorial](https://github.com/nvidia-riva/tutorials/blob/stable/asr-python-advanced-finetune-am-conformer-ctc-tao-finetuning.ipynb)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ASR_with_NeMo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:main12032022]",
   "language": "python",
   "name": "conda-env-main12032022-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
