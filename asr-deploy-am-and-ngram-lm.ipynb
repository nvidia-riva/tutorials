{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/rivaasrasr-deploy-am-and-ngram-lm/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to Deploy a Custom Language Model (n-gram) Trained with NeMo as Riva ASR NIM\n",
    "This tutorial walks you through the deployment of a custom language model (n-gram) trained with NVIDIA NeMo on NVIDIA Riva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVIDIA Riva Overview\n",
    "\n",
    "NVIDIA Riva ASR NIM APIs provide easy access to state-of-the-art automatic speech recognition (ASR) models for multiple languages. Riva ASR NIM models are built on the NVIDIA software platform, incorporating CUDA, TensorRT, and Triton to offer out-of-the-box GPU acceleration.\n",
    "\n",
    "In this tutorial, we will interact with the automated speech recognition (ASR) APIs.\n",
    "\n",
    "For more information about Riva ASR NIM, refer to the [Riva NIM documentation](https://docs.nvidia.com/nim/riva/asr/latest/overview.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeMo (Neural Modules) and `nemo2riva`\n",
    "[NVIDIA NeMo](https://developer.nvidia.com/nvidia-nemo) is an open-source framework for building, training, and fine-tuning GPU-accelerated speech AI and natural language understanding (NLU) models with a simple Python interface. To fine-tune a Parakeet-CTC acoustic model with NeMo, refer to the [Parakeet-CTC fine-tuning tutorial](https://github.com/nvidia-riva/tutorials/blob/main/asr-finetune-parakeet-nemo.ipynb).\n",
    "\n",
    "The [`nemo2riva`]() command-line tool provides the capability to export your `.nemo` model in a format that can be deployed using [NVIDIA Riva](https://docs.nvidia.com/nim/riva/asr/latest/overview.html) ASR NIM. A Python `.whl` file for `nemo2riva` is available in [PyPi](https://pypi.org/project/nemo2riva/). You can install `nemo2riva` with `pip`, as shown in the [Parakeet-CTC fine-tuning tutorial](https://github.com/nvidia-riva/tutorials/blob/main/asr-finetune-parakeet-nemo.ipynb). \n",
    "\n",
    "This tutorial explores taking a `.riva` model &mdash; the result of invoking the `nemo2riva` CLI tool (refer to the [Parakeet-CTC fine-tuning tutorial](https://github.com/nvidia-riva/tutorials/blob/main/asr-finetune-parakeet-nemo.ipynb)) &mdash; and leveraging the Riva ServiceMaker framework to aggregate all the necessary artifacts for Riva deployment to a target environment. Once the model is deployed as a Riva NIM, you can issue inference requests to the server. We will demonstrate how quick and straightforward this whole process is.\n",
    "In this tutorial, you will learn how to:\n",
    "- Build an `.rmir` model pipeline from a `.riva` file with Riva ServiceMaker.\n",
    "- Deploy the model locally on the Riva server.\n",
    "- Send inference requests from a demo client using Riva API bindings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, ensure you have:\n",
    "- Access to NVIDIA NGC.\n",
    "-  A _language_ model file that you want to deploy.\n",
    "    - For more information on training and exporting an n-gram language model, refer to the [NeMo Language Modeling documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/asr_language_modeling.html).  \n",
    "    - The language model file can be in one of the two following formats: \n",
    "        - `.binary`. You can download a pre-trained version from the [Riva ASR LM NGC model page](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/speechtotext_en_us_lm).\n",
    "        - `.arpa`. You can download a pre-trained version from the [Riva ASR LM NGC model page](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/speechtotext_en_us_lm). \n",
    "- An _acoustic_ model file in the `.riva` format that you want to deploy. You can convert a `.nemo` model file to a `.riva` model file with the `nemo2riva` command.\n",
    "    - For more information on customizing a Parakeet-CTC acoustic model with NeMo and exporting the resulting model with `nemo2riva`, refer to the [Parakeet-CTC fine-tuning tutorial](https://github.com/nvidia-riva/tutorials/blob/main/asr-finetune-parakeet-nemo.ipynb). \n",
    "    - Alternatively, you can obtain a pre-trained Parakeet-CTC `.riva` model for English ASR [here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/speechtotext_en_us_conformer). \n",
    "    - For more information on training NeMo models, refer to the [Training](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/core/core.html#training) section in the [NeMo documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/index.html). \n",
    "    - For more information on Parakeet-CTC's architecture, refer to the [Parakeet](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/models.html#parakeet) section of the [NeMo ASR Models](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/models.html) page. \n",
    "    - For more information on the configuration files necessary for training Parakeet-CTC with NeMo, refer to the [Fastconformer configs](https://github.com/NVIDIA/NeMo/tree/main/examples/asr/conf/fastconformer/).\n",
    "- Weighted Finite State Transducer (WFST) tokenizer and verbalizer files for Inverse Text Normalization (ITN). \n",
    "    - For more information on WFST and ITN, refer to the [NeMo Inverse Text Normalization: From Development to Production](https://arxiv.org/pdf/2104.05055.pdf) paper.\n",
    "    - You can download pretrained WFST ITN model files from this [NVIDIA GPU Cloud (NGC)](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/inverse_normalization_en_us) model page. \n",
    "- A decoder vocabulary file. You can download one from the [Riva ASR LM NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/speechtotext_en_us_lm) model page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Riva ServiceMaker\n",
    "Riva ServiceMaker is a set of tools that aggregates all the necessary artifacts (models, files, configurations, and user settings) for Riva NIM deployment to a target environment. It has two main components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riva-Build\n",
    "\n",
    "This step helps build a Riva-ready version of the model. Its only output is an intermediate format (called an RMIR) of an end-to-end pipeline for the supported services within Riva. Let's consider an ASR n-gram language model. <br>\n",
    "\n",
    "`riva-build` is responsible for the combination of one or more exported models (`.riva` files) into a single file containing an intermediate format called Riva Model Intermediate Representation (`.rmir`). This file contains a deployment-agnostic specification of the whole end-to-end pipeline along with all the assets required for the final deployment and inference. For more information, refer to the [documentation](https://docs.nvidia.com/nim/riva/asr/latest/custom-deployment.html#deploying-custom-models-as-nim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: UPDATE THESE PATHS \n",
    "\n",
    "# Riva NIM Docker\n",
    "CONTAINER_ID = \"<add container name>\"\n",
    "# Refer to this table to get the CONTAINER_ID for the model architecture you want to deploy.\n",
    "# Example: CONTAINER_ID = \"parakeet-1-1b-ctc-en-us\", \n",
    "\n",
    "# Directory where model files are stored, \n",
    "# e.g. $MODEL_LOC/$ACOUSTIC_MODEL_NAME.riva\n",
    "MODEL_LOC = \"<add path to model location>\"\n",
    "\n",
    "\n",
    "# Name of the acoustic model .riva file\n",
    "ACOUSTIC_MODEL_NAME = \"<add model name>\"\n",
    "\n",
    "# Name of the language model .riva (or .arpa or .binary) file\n",
    "LANGUAGE_MODEL_NAME = \"<add model name>\"\n",
    "\n",
    "# Name of the decoder vocab file\n",
    "DECODER_VOCAB_NAME = \"<add decoder vocab file name>\"\n",
    "\n",
    "# Name of the WFST tokenizer\n",
    "WFST_TOKENIZER = \"<add WFST tokenizer model name>\"\n",
    "\n",
    "# Name of the WFST verbalizer\n",
    "WFST_VERBALIZER = \"<add WFST verbalizer model name>\"\n",
    "\n",
    "# Path to store NIM model repository, Make sure that this directory is empty\n",
    "NIM_EXPORT_PATH=\"~/nim_cache\" \n",
    "\n",
    "! mkdir -p $NIM_EXPORT_PATH\n",
    "! chmod 777 $NIM_EXPORT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the `.rmir` file\n",
    "\n",
    "**Notes** \n",
    "1. If you encrypted your acoustic model and/or language model by adding the `--key` flag when invoking `nemo2riva`, or you downloaded a pre-trained model from NGC, you'll need to append a colon and then the key's value to the model's name in the `riva-build` command, as shown below. You might find it convenient to set a string variable named `KEY` and pass it into the appropriate `riva-build` arguments as `$KEY`. The standard encryption key for the older pre-trained models is `tlt_encode`.\n",
    "2. If your language model is in the `.arpa` format, replace `/servicemaker-dev/$LANGUAGE_MODEL_NAME:$KEY` with `--decoding_language_model_arpa=/servicemaker-dev/$LANGUAGE_MODEL_NAME` when invoking `riva-build`.\n",
    "3. If your language model is in the `.binary` format, replace `/servicemaker-dev/$LANGUAGE_MODEL_NAME:$KEY` with `--decoding_language_model_binary=/servicemaker-dev/$LANGUAGE_MODEL_NAME` when invoking `riva-build`.\n",
    "4. Refer to the [Riva ASR NIM Pipeline Configuration documentation](https://docs.nvidia.com/nim/riva/asr/latest/pipeline-configuration.html) if you want to build an ASR NIM. To obtain the proper `riva-build` parameters for your particular application, select the acoustic model (the parameters below assume Parakeet-CTC), language, and pipeline type (offline for the purposes of this tutorial) from the interactive web menu at the bottom of the first section of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the appropriate value\n",
    "! docker run --gpus all --rm \\\n",
    "     -v $MODEL_LOC:/servicemaker-dev \\\n",
    "     --name riva-servicemaker \\\n",
    "     --entrypoint=\"\" \\\n",
    "     nvcr.io/nim/nvidia/$CONTAINER_ID \\\n",
    "     riva-build speech_recognition \\\n",
    "     /servicemaker-dev/asr_offline_riva_ngram_lm.rmir:tlt_encode \\\n",
    "     /servicemaker-dev/$ACOUSTIC_MODEL_NAME:tlt_encode \\\n",
    "      --offline \\\n",
    "      --name=Parakeet-en-US-asr-offline \\\n",
    "      --return_separate_utterances=True \\\n",
    "      --featurizer.use_utterance_norm_params=False \\\n",
    "      --featurizer.precalc_norm_time_steps=0 \\\n",
    "      --featurizer.precalc_norm_params=False \\\n",
    "      --ms_per_timestep=80 \\\n",
    "      --endpointing.start_history=200 \\\n",
    "      --nn.fp16_needs_obey_precision_pass \\\n",
    "      --endpointing.residue_blanks_at_start=-2 \\\n",
    "      --chunk_size=4.8 \\\n",
    "      --left_padding_size=1.6 \\\n",
    "      --right_padding_size=1.6 \\\n",
    "      --max_batch_size=16 \\\n",
    "      --featurizer.max_batch_size=512 \\\n",
    "      --featurizer.max_execution_batch_size=512 \\\n",
    "      --decoder_type=flashlight \\\n",
    "      --decoding_language_model_binary=/servicemaker-dev/$LANGUAGE_MODEL_NAME \\\n",
    "      --decoding_vocab=/servicemaker-dev/$DECODER_VOCAB_NAME \\\n",
    "      --flashlight_decoder.lm_weight=0.2 \\\n",
    "      --flashlight_decoder.word_insertion_score=0.2 \\\n",
    "      --flashlight_decoder.beam_threshold=20. \\\n",
    "      --language_code=en-US \\\n",
    "      --wfst_tokenizer_model=/servicemaker-dev/$WFST_TOKENIZER \\\n",
    "      --wfst_verbalizer_model=/servicemaker-dev/$WFST_VERBALIZER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riva-Deploy\n",
    "\n",
    "The deployment tool takes as input one or more RMIR files and a target model repository directory. It creates an ensemble configuration specifying the pipeline for the execution and finally writes all those assets to the output model repository directory.\n",
    "\n",
    "**Note:** If you added an encryption key to your `.rmir` file when building it with `riva-build`, make sure to append a colon and then the key's value to the model's name in the `riva-deploy` command, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax: riva-deploy -f dir-for-rmir/model.rmir[:key] output-dir-for-repository\n",
    "! docker run --gpus all --rm \\\n",
    "     -v $MODEL_LOC:/servicemaker-dev \\\n",
    "     -v $NIM_EXPORT_PATH:/model_tar \\\n",
    "     --name riva-servicemaker \\\n",
    "     --entrypoint=\"\" \\\n",
    "     nvcr.io/nim/nvidia/$CONTAINER_ID \\\n",
    "     bash -c \"riva-deploy -f /servicemaker-dev/asr_offline_riva_ngram_lm.rmir /data/models/ && tar -czf /model_tar/custom_models.tar.gz -C /data/models .\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Start the Riva ASR NIM\n",
    "After the model repository is generated, we are ready to start the Riva NIM server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run the container with the cache directory mounted in the appropriate location:\n",
    "! docker run -it --rm -d --name=$CONTAINER_ID \\\n",
    "   --runtime=nvidia \\\n",
    "   --gpus '\"device=0\"' \\\n",
    "   --shm-size=8GB \\\n",
    "   -e NGC_API_KEY \\\n",
    "   -e NIM_TAGS_SELECTOR \\\n",
    "   -e NIM_DISABLE_MODEL_DOWNLOAD=true \\\n",
    "   -e NIM_HTTP_API_PORT=9000 \\\n",
    "   -e NIM_GRPC_API_PORT=50051 \\\n",
    "   -p 9000:9000 \\\n",
    "   -p 50051:50051 \\\n",
    "   -v $NIM_EXPORT_PATH:/opt/nim/export \\\n",
    "   -e NIM_EXPORT_PATH=/opt/nim/export \\\n",
    "   nvcr.io/nim/nvidia/$CONTAINER_ID:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Inference\n",
    "After the Riva NIM server is up and running with your models, you can send inference requests querying the server. \n",
    "\n",
    "To send gRPC requests, you can install the Riva Python API bindings for the client. This is available as a [Python module on PyPI](https://pypi.org/project/nvidia-riva-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Client API Bindings\n",
    "! pip install nvidia-riva-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import riva.client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the Riva Server and Run Inference\n",
    "\n",
    "NIM server can take some time to load, wait till the server is ready to serve the requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time\n",
    "\n",
    "for i in range(30):\n",
    "    try:\n",
    "        print(f\"Waiting for NIM server to load, retrying in 5 seconds...\")\n",
    "        r = requests.get(\"http://0.0.0.0:9000/v1/health/live\", timeout=2)\n",
    "        if \"live\" in r.text:\n",
    "            print(\"NIM server is ready!\")\n",
    "            break\n",
    "    except requests.RequestException as e:\n",
    "        pass\n",
    "    time.sleep(5)\n",
    "else:\n",
    "    print(\"Server did not become ready after 30 attempts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once the server is ready, we can call this inference function to query the Riva NIM server (using gRPC) to transcribe an audio file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(audio_file, server='localhost:50051', print_full_response=False):\n",
    "    with open(audio_file, 'rb') as fh:\n",
    "        data = fh.read()\n",
    "    \n",
    "    auth = riva.client.Auth(uri=server)\n",
    "    client = riva.client.ASRService(auth)\n",
    "    config = riva.client.RecognitionConfig(\n",
    "        language_code=\"en-US\",\n",
    "        max_alternatives=1,\n",
    "        enable_automatic_punctuation=False,\n",
    "    )\n",
    "    \n",
    "    response = client.offline_recognize(data, config)\n",
    "    if print_full_response: \n",
    "        print(response)\n",
    "    else:\n",
    "        print(response.results[0].alternatives[0].transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"audio_samples/en-US_sample.wav\"\n",
    "run_inference(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can stop the Riva NIM server before shutting down the Jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker stop $CONTAINER_ID\n",
    "! docker rm $CONTAINER_ID"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
